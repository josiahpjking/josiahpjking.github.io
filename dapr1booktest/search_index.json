[
["index.html", "Data Analysis for Psychology in R (dapR1) - Labs Overview of the Course The team R Cheatsheets R Community", " Data Analysis for Psychology in R (dapR1) - Labs Department of Psychology, University of Edinburgh 2019-2020 Overview of the Course Data Analysis for Psychology in R 1 (dapR1) is your first step on the road to being a data, programming and applied statistics guru! This course provides a introduction to data, R and statistics. It is designed to work slowly through conceptual content that form the basis of understanding and working with data to perform statistical testing. At the same time, we will be introducing you to basic programming in R, covering the fundamentals of working with data, visualization and simple statistical tests. The overall aim of the course is to provide you with all the necessary skills to feel confident working with R and data, before we move on to discuss a broader array of statistical methods in year 2. On this page you will find the notes for your weekly labs including practice exercises and solutions. You can also read a gentle introduction to R and installation guide. As you progress through the course, the content will build up. Find the general overview of topics below so you know where we are heading: SEMESTER 1 Week Lecture Lab topic 1 Introduction lecture: research process, planning and design What is R and installation 2 Measurement: types of data RStudio environment, libraries, packages, scrips and notebooks. Introduction to Rmarkdown 3 Organising data: data sets, tables, plots Assignment: vectors, lists, data frames, data types 4 Describing data: central tendency Introduction to plots and geoms 5 Describing data: variability Central tendency &amp; variability Break 6 Functions &amp; data transformation Hand plotting functions. Introduction to data transformation 7 Statistical models, chance and probability R practice: read in data, merge 8 Fundamentals of probability Introduction to probability. Sampling in R 9 Probability &amp; probability distributions Introduction to probability distributions. Revision 10 Probability distributions: Binomial &amp; Normal Lab test 1 SEMESTER 2 Week Topic 11 Sampling 12 Bootstrapping and confidence intervals 13 Hypothesis testing 14 Normal distribution and probability 15 Revision Break 16 Test for one mean 17 Test for two means (independent samples) 18 Test for two means (paired samples) 19 Chi-squared 20 Covariance &amp; correlation The team Dr Tom Booth Tom.Booth@ed.ac.uk Lecturer and Course Organiser Dr Umberto Noe Umberto.Noe@ed.ac.uk Senior Teching Coordinator (Labs) Dr Josiah King Josiah.King@ed.ac.uk Senior Teching Coordinator (Labs) Ms Emma Waterston Emma.Waterston@ed.ac.uk PhD student in Psychology (Labs) (with special thanks to Dr Anastasia Ushakova) R Cheatsheets You can find a collection of cheatshets that summarise some of the most useful commands you will be using for tasks such as data transformation, visualisation, RMarkdown and many others here/ The key ones you will need to get for this year are: RMarkdown Data Visualisation (ggplot2) Data transformation with dplyr Data Import R Community R has great representation in Edinburgh. Check out these pages to find out more: R Ladies Edinburgh EdinburghR And worldwide you have: R Studio Community "],
["week-1.html", "Chapter 1 Week 1 1.1 Introduction to R and RStudio 1.2 Install the recent version of R and R Studio 1.3 Getting Started in RStudio 1.4 R as an interactive environment 1.5 Check your settings 1.6 Setting Up Your Working Directory 1.7 The Console 1.8 Spacing 1.9 Typos 1.10 Unfinishe…. d 1.11 Basic Arithmetic 1.12 Using Functions for Calculations 1.13 R Scripts 1.14 Short Example 1.15 Naming variables 1.16 Now over to you 1.17 Solutions", " Chapter 1 Week 1 1.1 Introduction to R and RStudio Welcome to Data Analysis for Psychology in R! This week we would like to focus on getting you started in R, get your software installation issues sorted and attempt some very quick and basic practice. There is a lot to cover as we will be moving through the year so if something may look overwhelming, just bear with us, we will make sure that we cover as much as possible so by the end of the year you will become a confident user of R. Everyone in our team really likes R and we hope that you will love it too :) R has a great community built over years and includes researchers, industry practitioners and curious minds across disciplines and fields. R also has been getting better and better over the years and now R has a large online community which shares solutions to any issues/encounters while you are learning R during your time at UoE and after the university. If unsure about something, don’t be afraid to simply Google it! If getting an error, sometimes the fastest you can do is to actually copy and paste it and let Google find you an explanation of what happened. But before doing that, come to labs regularly, ask questions, read the course materials carefully. R can be confusing at the start but we all have been there so stay with us ;) 1.2 Install the recent version of R and R Studio 1.2.1 First Step Depending on the operating system you may check the right version you need. You will need first to install the body of R. For MacOS press here - select R 3.6.1 For Windows press here 1.2.2 Second Step We then will install RStudio. Rsudio is a useful interface that allows us to interact with R and where can see our data, results from analyses and various plots/visualisations. Get a right version for your operating system here For Windows pick RStudio 1.2.1335 - Windows 7+ (64-bit) For MacOS pick RStudio 1.2.1335 - macOS 10.12+ (64-bit) Any issues? Let us know! Quick note: it is useful to update your R from now and then. As you progress in your degree, try to update you R every year you are back to dapR. 1.3 Getting Started in RStudio RStudio is what is called an Integrated Development Environment for R. It is dependent on R but separate from it. There are several ways of using R but R Studio is arguably the most popular and convenient. It’s time to open it up and get started! When you first open up RStudio, this is what you should see: There’s a whole lot of text that (1) isn’t very interesting, and (2) generally isn’t that important so you don’t need to pay much attention to it. 1.4 R as an interactive environment RStudio has four panels: Current file (Editor), Console, Environment, and Viewer. We will focus on using those more in the upcoming labs. 1 The Information area (all of the right-hand side of RStudio) shows you useful information about the state of your project. The top panel of the Information side of RStudio contains the Environment and History panes. History contains the history (unsurprising I know) of commands that have been typed into the console. The Environment is virtual storage of all objects you create in R, and contains your list of variables, data frames, strings, etc., which R has been told to save from previous commands. The bottom panel of the Information section contains a number of different panes - Files, Plots, Packages, Help, and Viewer. In other words, you have a file manager (where you can view which files are loaded into your project), a panel to show plots and graphs, a list of packages, help on the functions available within R, and viewer allows you to view local web content. On the left-hand area of RStudio, you have your Current File (Editor) and Console. The “heart of R” is the Console window. This is where instructions are sent to R, and its responses are given. It’s the place to try things out, but don’t want to save. Finally, the Editor is where you write more complicated scripts without having to run each command. When you run such a script file, it gets interpreted by R in a line by line fashion. This means that your data cleaning, processing, visualization, and analysis needs to be written up in sequence otherwise R won’t be able to make sense of your script. There is an important practical distinction between the Console and the Editor: In the Console, the Enter key runs the command. In the Editor, it just adds a new line. The hash (#) marks everything to the right of it as comment. Using comments can be very useful for annotating your code, and it improves the readability too. It will also help you to remember why/what you done months later, when you return to your code that you have inevitably forgotten! 1.5 Check your settings Before we get to some work in R, let us first make sure that everything works the way we want. Some of you may want to change the appearance of your R, adjust fonts or colors so you find it easier to use R on your computer. 1.5.1 Disable automatic saving of your workspace We will need this to make sure that we clear up our working space each time you close you RStudio so you can start afresh each week without piling up staff in your environment. You will see what we mean as we practice. For now, let’s do the following: 1.5.2 Adjust fonts/colours if you like Depending what works better for your vision and experience, there are number of choices to personalise the look of your R. Try these out here: 1.6 Setting Up Your Working Directory Step 1. Create a folder (sub-directory) named dapR1 on your “Desktop” folder Step 2. From RStudio, use the menu to change your working directory under Session &gt; Set Working Directory &gt; Choose Directory Step 3. Choose the directory you’ve just created in Step 1 Step 4: Select File -&gt; New File -&gt; R Script. In the window that appears, click on the disk icon just below Untitled1 and save the blank script as Lab1.R. Make sure that this saves in your DAPR1 folder. For the purpose of this course, we’re going to be using scripts and RMarkdown files (more on that next week), but first, lets have a play around with the console so you can familiarise yourself with it. 1.7 The Console Click on the console, type 1+1, and hit enter 1 + 1 ## [1] 2 It should have hopefully returned the answer 2. Next type 2*1, and hit enter. This should return the sum value. 2*1 ## [1] 2 Now, hit ctrl + uparrow (cmd + uparrow on mac). This is a useful shortcut, and allows you to quickly re-run or edit previous code used in the console. This handy little shortcut isn’t needed in the script, as you’ll be able to easily copy, change, or run code as needed. 1.8 Spacing You can use spaces to make code easier to read, and although R is pretty good at ignoring large gaps, it does have to guess and make the assumption that the spacing was unintended. Try to keep your code neat and tidy! 2 * 1 ## [1] 2 What you can’t do is insert spaces into the middle of a word, or R will get upset with you. 1.9 Typos You’ll also need to be careful to avoid typos, as R will not know that it is producing unintended output. Instead, it will assume that you meant exactly what you typed. For example, suppose you forget to press the shift key when typing the + sign. Your command would then be 20 = 10, as opposed to 20+10. Here is what would happen: 20 = 10 ## Error in 20 = 10: invalid (do_set) left-hand side to assignment And there you have it - your first error message! This happens when what you type doesn’t make any sense to R. It’s a pretty touchy programme, and can’t spot these kinds of simple human mistakes. Sometimes, R will produce the right answer, but to the wrong question, if you are not careful with what you type. Sticking with the same example, suppose your hand slipped and you pressed the “-” key next to the +. R has no way of knowing that you intended to add 10 to 20, not to subtract. This time you’d get: 20 - 10 ## [1] 10 This can be a little more dangerous, especially when you are working on more advanced stuff, as it can become more difficult to spot mistakes in output. The take home message is simple: You must be precise and accurate with what you say to R. It’s mindlessly obedient, and doesn’t have auto correct. Be careful with what you type! 1.10 Unfinishe…. d Sometimes, you might get a little too excited and hit enter when you haven’t actually finished a command. Because R is so obedient, it will just keep waiting. For example, if you type 20 + and then press enter, R will know that you probably want to put another number at the end of that command. and there in the console you should see a blinking cursor on the right of your + sign. This tells you that R is still waiting for you to finish your command, and the + sign is another command prompt. If you now type 10 and press enter, you will get: 1.11 Basic Arithmetic Table 1 lists the operators that correspond to the basic arithmetic: Operation Operator Example Input Example Output Addition + 20 + 10 30 Subtraction - 20 - 10 10 Multiplication * 20 * 10 200 Division / 20 / 10 2 Power ^ 20 ^ 10 1.024e+13 The one important thing to remember when using R to calculate sums is that brackets always come first. One easy way to remember this is to enclose what you want to happen first in brackets e.g. (20/10) * 2. In this example, R would have done the division first anyway, but its always important to make sure that R is doing exactly what you want! 1.12 Using Functions for Calculations As you will have seen above, there are lots of calculations you can do with very basic operators. If you wanted to do more advanced calculations, you need to use functions. Lets run through a few of the simple ones that are handy to use in R. Operation R code Example Input Example Output Square root sqrt( ) sqrt(100) 10 Absolute value abs( ) abs(-100) 100 Round round(x, digits = ) round(12.345, 2) 12.35 Minimum min(...) min(2.21, 2.22) 2.21 Max Maximum max(...) max(2.21, 2.22) 2.22 It’s also useful to note that you can use multiple functions together and combine them if desired. For example: sqrt(100 + abs(-44)) ## [1] 12 R has started out by calculating abs(44), and then simplifies the command to sqrt(100+44). To solve the sum, it then needs to add 100 and 44, before evaluating the sqrt(144), and returning the value of 12. Top Tip: Lets take an example from above, and say that we wanted to round a value. This time, in the console, start typing the name of the function, and then hit the ‘tab’ key. RStudio will display the below window. There are two panels: one gives a list of variables that starts with the letters I’ve typed, and the other tells you what it does. If you have the name of the function written, R will recognise this, and instead pop up with the arguments for that function instead. 1.13 R Scripts Now that you are hopefully familiar with the console, lets try out writing in a script! R will continue to provide the output in the console, and hitting enter won’t make R run any command. You now need to press Ctrl + Enter (Cmd + Enter). R operates on named data structures, and you can create very simple to very complex structures. # In your script, create an object a and assign it the value of 1 a &lt;- 1 # In the above, we would say &quot;the variable (object) a is assigned to 1&quot;, or &quot;a gets 1&quot; # increment a by 1 a + 1 ## [1] 2 # OK, now see what the value of a is a ## [1] 1 So, R returned us output as if it forgot we asked it to do a + 1 and didn’t change its value. That’s because we weren’t clear in what we wanted! The only way to keep this new value is to put it in an object. b &lt;- a + 1 # now let&#39;s see b ## [1] 2 # success! Quick Note: The &lt;- operator is used to ‘point’ to the object receiving the stated value. In most cases = can be used instead but we advise that you stick to &lt;-. You can also make assignments in the other direction too: b + 1 -&gt; c c ## [1] 3 Now, lets create a vector (or string) of numbers. This is a single entity that consists of a list of ordered numbers. If we wanted to create a vector called x, containing five numbers (2, 4, 6, 8, 10), we would use the R command: x &lt;- c(2, 4, 6, 8, 10) In simple words, we have now created a list of the five numbers, and assigned them to the object x. We made use of the c() function, as we were giving R a list of values. Let’s take a look at x. x ## [1] 2 4 6 8 10 You can now use these objects in other commands. For example, lets say you wanted to square each of the values in x, or multiply by b: x^2 ## [1] 4 16 36 64 100 x*b ## [1] 4 8 12 16 20 1.14 Short Example Now that you’ve created your first variables to store some numbers, lets try an example. Say that you have all found that this course was extremely helpful as an introduction to R, and that you wanted to recommend to your friend to buy the materials online. Firstly, I’d want to calculate how many copies I’d sell if we actually turned this into a proper book. Since there are (roughly) 170 students in the whole class, and you all loved it, I’m going to assume 170 sales, and create a variable called sales. So, how do I do this? sales &lt;- 170 Now to work out how much money I’m going to make per book. I need to create another variable called royalty, which we will use to indicate how much money I will get per copy sold. Let’s say I get £5 per copy and book get sold for £25. royalty &lt;- 5 The last thing I want to do is calculate how much money I’ll make from sales in this class. We now need to create our revenue variable, and ask R to print out the total value of revenue. revenue &lt;- sales * royalty revenue ## [1] 850 And there we have it - £850. As far as R thinks, the sales*royalty is the exact same as 170*5. What if at last minute a student decides to buy 10 copies for their friends too? That would mean that we need to update our revenue. The easiest way to do this is to overwrite the value. revenue_simple&lt;- revenue + 50 revenue_simple ## [1] 900 But we can also be smarter: revenue_smarter &lt;- revenue + 10*royalty revenue_smarter ## [1] 900 1.15 Naming variables You might have noticed so far that I’ve used very simple letters or names for my variables, and that they have all been written in lower case. R does have more flexibility in what you can name your variables, but it also has some rules… Variable names cannot include spaces: my revenue is not a valid name, but my_revenue is Variable names are case sensitive: Revenue and revenue are different names Variable names must be started with a letter or a full stop: You couldn’t use something like 1revenue or _revenue There are some reserved keywords that cannot be used to name variables with: These include, if, else, for, in, next, TRUE, FALSE, NULL, NA, NaN, repeat, function, NA_integer_, NA_real_, NA_complex_. Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these. There are a few other things you’d want to consider too, such as using meaningful names, using short names, and using a conventional style for multiword names when necessary. Consistency is key! 1.16 Now over to you You have seen a few basic operations you can do in R. Now it is time to try some yourself. These should be easy-peasy. 1.16.1 Exercise 1 Calculate the product of 155*12 Store the result in the ‘results’ object (check that it now appears in your environment) Now take a square root of the results Save the results in the object ‘final_results’ 1.16.2 Exercise 2 Go back to the revenues example Assuming that our book did not sell that great after all and we only sold 50 copies. Our publisher did not like it so they decrease the royalties that we get from sales so publisher can cover the cost of book production (new royalty is £3) What is the total revenue now? 1.17 Solutions #Exercise 1 155*2 #Store the results results&lt;-155*12 #sqrt sqrt(results) #New object final_results&lt;- sqrt(results) #Exercise 2 #New sales new_sales &lt;- 50 #New royalty new_royalty &lt;- 3 #New revenue new_revenue &lt;- new_sales * new_royalty new_revenue #Your new revenue should be 150 Jessica Ward (https://twitter.com/RLadiesNCL/status/1138812826917724160).↩ "],
["week-2.html", "Chapter 2 Week 2 2.1 Introduction to RMarkdown 2.2 What is R Markdown? 2.3 Getting things ready 2.4 R Markdown basics 2.5 Headings 2.6 Text 2.7 Tables 2.8 Lists 2.9 Rmd documents 2.10 Code chunks 2.11 Common Code Chunks Options 2.12 Including images or links 2.13 Generating documents 2.14 Extra: Open/Save files on Rstudio Cloud (Chrome users)", " Chapter 2 Week 2 2.1 Introduction to RMarkdown Welcome to the second lab of dapR 1. This week, we will introduce R Markdown (Rmd), an extremely versatile and powerful tool for writing reproducible documents of all sorts and formats. We promise you that once you get the hang of it, you will never again want to use your ordinary word processor/text editor for writing reports and notes that are based on data analysis. In fact, all the teaching materials in this course, from the lecture slides to these lab sheets, have been written in Rmd. Apart from Rmd, this week we will also have a bit of a closer look at the little bits of RStudio and walk you through environment, packages, libraries and editing spaces. Once again, do not worry if you do not remember everything at once, we will continue using various Rstudio panels each week and soon enough it will become a habit. 2.2 What is R Markdown? Well, it is a language - or a system - for telling computers how to process and format text. At this point you might be thinking “why on Earth should I be learning this when I can just use the text editor or Word on my computer?!”. That is a good question and the answer to it is that, when it comes to Rmd, its integration with R Studio makes it an incredibly useful tool for writing documents that include the results of a statistical analysis or data visualisations. You can easily write text and include code where you need to. Compared to R scripts that we showed you last week, we have more flexibility here. 2.3 Getting things ready Before you do anything else, open R Studio and install the rmarkdown package if you have not yet done so. This package will enable you to convert files written in Rmd to output of your choice. Task 1: Type exactly the following command into the console and press ↵ Enter: install.packages(&quot;rmarkdown&quot;)     Once you have done the step above, you do not need to repeat it again, unless you update your R or work on another computer. Task 2: Download and open the Week2_pratice.Rmd file (you can find it here and on Learn) and have a look what you can see in there. We will walk you through the first steps - there is some practice for you to try as well. 2.4 R Markdown basics Once you have done the step above, you do not need to repeat it again, unless you update your R or work on another computer. 2.5 Headings First, lets look at how you would make headings using #: # Section 1 ## Subsection 1 ### Sub subsection 1 2.6 Text You can also vary the format of your text: *italics* returns italics **bold** returns bold ~~strikethrough~~ returns strikethrough superscript^2 returns superscript^2 subscript^~2~ returns subscript^2 2.7 Tables An example of simple table that can be produced using RMarkdown is presented below. You will get an illustration on how to build one in your practice template. Operation R code Example Input Example Output Square root sqrt( ) sqrt(100) 10 Absolute value abs( ) abs(-100) 100 Round round(x, digits = ) round(12.345, 2) 12.35 Min min(...) min(2.21, 2.22) 2.21 Max max(...) max(2.21, 2.22) 2.22 2.8 Lists We can list items pretty easily. 2.8.1 Unordered Item Item Item 2.8.2 Ordered Item 1 Item 2 Sub-item 2.1 Item 3       And most importantly, we can include chunks of code which will produce the required output when we compile our document all together. Find a button `insert’ on the top right corner of your Rmd editor. Choose ‘R’. You will be able to see a chunk where you can put a comment and an example of an operation (say multiplication - try to vary those as you go). Press the green button on the right in the code chunk. # This is an R code chunk # Here you can write code and R will run it when you generate your document # and display the output below 6 * 7 ## [1] 42       For a quick reference guide to Rmd, see this cool cheat sheet. 2.9 Rmd documents OK, now that you understand simple editing, let’s look at the .Rmd file step-by-step. The first thing to realise is that an .Rmd file is just a plain text file (such as .txt). You could open it in Notepad, MS Word, or OpenOffice and you would basically see the same thing as in R Studio. The only reason for the special .Rmd extension is for R Studio to know to put all the nice colours in to aid readability and offer you options associated with R Markdown, such as the option to actually generate a document from the file. With that out of the way, but let’s scroll all the way up in the .Rmd file. There, you can see this header: --- pdf_document: default title: &quot;Introducing R Markdown&quot; author: &quot;dapR 1 -- Lab 2&quot; output: html_document: default pdf_document: default In our document, we set the title and author and define the output to be html and pdf (we will show you what we mean by that!) Setting your output file to HTML file is equivalent to the format of websites, which is why we can easily put it online like our book. You can also try out to create PDF of even Word from your Rmd file and save it in your folder. For this semester and perhaps, this year - lets stick to HTML output. 2.10 Code chunks Let’s talk a little more about code chunks, since they are the main reason why Rmd is so useful when it comes to reports of statistical analysis. For one, they are great for creating tables and figures. As a basic demonstration, we can create a simple histogram. Again, at this point, you don’t have to worry about understanding the code itself. The important bit is that, once you know how to create fancy plots and tables, you can create them directly in your .Rmd file to put them in your paper/report/presentation. We can first create some data. # create a made up sequence of numbers and pretend they are the ages of our participants age &lt;- c(34, 22, 26, 25, 43,19, 19, 20, 33, 27, 27, 26, 54) We can then try to plot it (say using histogram for now). We will teach you more about plots in week 4. hist(age) # basic quick histogram This feature has a very useful consequence: you can write a document in such a way that, if something about your data or analysis changes, you can simply edit the code in the appropriate chunks, re-generate the output file and all the values will get updated including your plots. Imagine having to redo a table of 40, 50 or 100 numbers – that’s an awfully tedious task and it’s prone to human error. With a proper use of R Markdown you will never have to do it! Imagine how many hours of work that will save you (trust us, it’s a lot). 2.11 Common Code Chunks Options For now, you can stick to simple code chunks without worrying too much about adding extra options. For future reference, the specifications below could be added to your code chunks. Important note: When provided with a template each week sometimes we will add some modifications to your chunks, please do not edit them or delete anything there as this may affect how your final document looks. If you are keen to learn more about what these settings do, see below: name - This allows you to name your code chunks, but is not necessary unless you want to reference them later echo - Whether to display the code chunk or just show the results. echo=FALSE will embed the code in the document, but the reader won’t be able to see it eval - Whether to run the code in the code chunk. eval=FALSE will display the code but not run it warning - Whether to display warning messages in the document message - Whether to display code messages in the document results - Whether and how to display the computation of the results 2.12 Including images or links 2.12.1 Adding links You can add links to your text quite easily, using square brackets and including the webpage link e.g. [here] (LINK). In practice, just remove the space. See our book [here] (https://bookdown.org/animestina/dapr1_labs/) 2.12.2 Adding figures &amp; pictures Include a picture from online or from your working directory (more on the latter later). With link it’s pretty simple: knitr::include_graphics(&quot;https://imgs.xkcd.com/comics/correlation.png&quot;) You can try to add another one in your practice Rmd file as well. 2.13 Generating documents Now that you have an understanding of the basics of Rmd along with some nifty tricks and can read the source file, let’s talk about how to generate output from the .Rmd files. The simplest way of turning the source file into output is by using the ‘Knit’ button at the top left of your Rmd file. The first time you generate a document like this, it can take a while for R to install and run all the tools necessary to produce your output. After a moment, the result should pop out in R Studio’s internal viewer. Take a minute to marvel at your creation! OK, that’s plenty for now! Close the viewer window and check your “Week_2” folder. Therein, you should find a file called “Week2_practice.html”. This is your actual output. Every time you adjust your Rmd file and compile it again, your output file will get updated as well. If you open it, it should appear in your default web browser because HTML files are the stuff websites are made from. That is all we have in store for you for this lab. We suggest you go over what you learnt today to help your newly acquired knowledge settle. We will be using an Rmd template each week and soon enough you will get used to compiling one at the end of each lab. 2.14 Extra: Open/Save files on Rstudio Cloud (Chrome users) If you are using Chrome Books you will find that there is slight difference with respect to how you would load/save files in RStudio. To provide you with an example on how to open and save the file for this week please use the guide below: "],
["week-3.html", "Chapter 3 Week 3 3.1 Vectors, lists, data frames and data types 3.2 Get the package first 3.3 Numeric Data 3.4 Text/Character Data 3.5 Logical Data 3.6 Variable Classes 3.7 Factors 3.8 Lists 3.9 Practice.Rmd Solutions", " Chapter 3 Week 3 3.1 Vectors, lists, data frames and data types In this section we will introduce you to various types of data you can store and create in R. In applied research and psychology in particular, you will often find different types of information in your data and these could be both numerical and text. We will work through a few examples below to give you an overview of various types of objects that store data and will talk about how you can access and work with the information within those. As you saw during your lecture last week there are few key types of variables we can enounter when it comes to storing information. There is a way to specify this in R: When working with continuous/numeric data you will be creating numeric variables. When working with categorical/nominal/ordinal data you will be creating factor variables. When working with variables which store information such as TRUE or FALSE you will be using logical variables. 3.2 Get the package first For today, the key package we will need is tidyverse. Let’s install it first. install.packages(&#39;tidyverse&#39;) Next, we will need to call it from the library: # Load from the library library(tidyverse) 3.3 Numeric Data Let’s imagine that someone wanted to provide summary data for the average temperature each month in Edinburgh. Imagine that they started updating their notes monthly. They started in June and by now they have the following records: The average temperature was 18 degrees in June, 22 in July, 19 in August, and 16 in September. Task: I want to create a variable, called monthly.temp that stores this data. The first number should be 18, the second 22, and so on. We want to use the combine function c() to help us do this. To create our vector, we should write: monthly.temp &lt;- c(18, 22, 19, 16) monthly.temp ## [1] 18 22 19 16 To summarise, we have created a single variable called monthly.temp, and this variable is a vector with four elements. So, now that we have our vector, how do we get information out of it? What if I wanted to know the average temperature for August, for example? Since we started in June, August is the third month, so let’s try: monthly.temp[3] ## [1] 19 Turns out that the numbers I put for August were wrong, and it was actually warmer this August (not 19 but 21!). How can I fix this in my monthly.temp variable? I could make the whole vector again, but that’s a lot of typing and it’s wasteful given that I only need to change one value. We can just tell R to change that one specific value: monthly.temp[3] &lt;- 21 monthly.temp ## [1] 18 22 21 16 You can also ask R to return multiple values at once by indexing. For example, say I wanted to know the temperature between July (the second element) and September (the fourth element). The first way to ask for an element is to simply provide the numeric position of the desired element in the structure (vector, list…) in a set of square brackets [ ] at the end of the object name. I would ask R: monthly.temp[2:4] ## [1] 22 21 16 # This is equivalent to: monthly.temp[c(2, 3, 4)] ## [1] 22 21 16 Notice that the order matters here. If I asked for it in the reverse order, then R would output the data in the reverse too. 3.4 Text/Character Data Although you will mostly be dealing with numeric data, this isn’t always the case. Sometimes, you’ll use text. Let’s create a simple variable to see how its done: greeting &lt;- &quot;hello&quot; greeting ## [1] &quot;hello&quot; It is important to note the use of quotation marks here. This is because R recognises this as a “character”. A character can be a single letter, 'g', but it can equally well be a sentence including punctuation, &quot;Descriptive statistics can be like online dating profiles: technically accurate and yet pretty darn misleading.&quot; Back to our temperature records example, I might want to create a variable that includes the names of the months. To do so, I could tell R: # Create months months &lt;- c(&quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;) In simple terms, you have now created a character vector containing four elements, each of which is the name of a month. Let’s say I wanted to know what the fourth month was. What would I type? # As before, access the fourth element of your vector months[4] ## [1] &quot;September&quot; 3.5 Logical Data A logical element can take one of two values: TRUE or FALSE. Logicals are usually the output of logical operations (anything that can be phrased as a yes/no question e.g. Is x equal to y?). In formal logic, TRUE is represented as 1 and FALSE as 0. This is also the case in R. If we ask R to calculate 2 + 2, it will always give the same answer. 2+2 ## [1] 4 If we want R to judge whether something is a TRUE statement, we have to explicitly ask. For example: 2+2 == 4 ## [1] TRUE By using the equality operator ==, R is being forced to make a TRUE or FALSE judgement. 2+2 == 3 ## [1] FALSE What if we try to force R to believe some fake news (aka incorrect truths)? 2+2 = 3 ## Error in 2 + 2 = 3: target of assignment expands to non-language object R cannot be convinced that easily. It understands that the 2+2 is not a variable (“non-language object”), and it won’t let you change what 2+2 is. In other words, it won’t let you change the ‘definition’ of the value of 2. There are several other logical operators that you can use, some of which are detailed in the table below. Operation R code Example input Example output Less than &lt; 1 &lt; 2 TRUE Greater than &gt; 1 &gt; 2 FALSE Less than or equal to &lt;= 1 &lt;= 2 TRUE Greater than or equal to &gt;= 1 &gt;= 2 FALSE Equal to == 1 == 2 FALSE Not equal to != 1 != 2 TRUE Not ! !(1==1) FALSE Or | (1==1) (1==2) And &amp; (1==1) &amp; (1==2) FALSE Let’s apply some of these logical operators to our vectors. Let’s use our monthly.temp vector, and ask R whether there were any months when the temperature dropped below zero. monthly.temp &lt; 0 ## [1] FALSE FALSE FALSE FALSE I can then store this information in a vector: any.temp &lt;- monthly.temp &lt; 0 any.temp ## [1] FALSE FALSE FALSE FALSE To summarise, we have created a new logical vector called any.temp, whose elements are TRUE only if the corresponding sale is below zero. But this output isn’t very helpful, as a big list of TRUE and FALSE values don’t give much insight into which months the temperature was below zero. We can use logical indexing to ask for the names of the months where temperature was below zero. Ask R: months[ any.temp &lt; 0 ] ## character(0) 3.6 Variable Classes So far, you’ve encountered character, numeric and logical data. It is really important that you remember/know what kind of information each variable stores (and it is essential) that R remembers, because otherwise you could run into some problems. For example, let’s say you create the following variables: x &lt;- 1 y &lt;- 2 Given that we have assigned numbers, let’s check whether they are numeric: is.numeric(x) ## [1] TRUE is.numeric(y) ## [1] TRUE Great, that means that we could proceed with simple sums e.g. multiplication. However, if they contained character data, R would provide you with an error: x &lt;- &quot;blue&quot; y &lt;- &quot;yellow&quot; x*y ## Error in x * y: non-numeric argument to binary operator Yes, R is smart enough to know that you can’t multiply colours. It knows because you’ve used the quotation marks to indicate that the variable contains text. This might seem unhelpful, but it is actually quite useful, especially when working with data. For example, without quotation marks, R would treat 10 as a number and would allow you to do sums with it. With the quotation marks, “5”, it treats it as text. Above, we checked to specifically see whether our x and y variables were stored as numeric variables. But what if you can’t remember what you should be checking for? You could use the class( ) and mode( ) functions instead. The class( ) of the variable tells you the classification, and mode( ) relates to the format of the information. The former is the most useful in most cases. x &lt;- &quot;hello&quot; class(x) ## [1] &quot;character&quot; mode(x) ## [1] &quot;character&quot; y &lt;- TRUE class(y) ## [1] &quot;logical&quot; mode(y) ## [1] &quot;logical&quot; z &lt;- 10 class(z) ## [1] &quot;numeric&quot; mode(z) ## [1] &quot;numeric&quot; 3.7 Factors Let’s get into some more relevant examples for statistics. We have only referred to ‘numeric’ data so far but we commonly make the distinctions between nominal, ordinal, interval, and ratio numeric data. Imagine that we had conducted a study with different treatment conditions. Within our study, all twelve participants completed the same task, but each of the three groups were given different instructions. Let’s first create a variable that tracks which group people were in: group &lt;- c(1,1,1,1,2,2,2,2,3,3,3,3) Now, it wouldn’t make sense to add two to group 1, group 2, and group 3 but let’s try anyway: group + 2 ## [1] 3 3 3 3 4 4 4 4 5 5 5 5 R has now created groups 4 and 5, which don’t exist. But we allowed it to do so, as the values are currently just ordinary numbers. We need to tell R to treat “group” as a factor. We can do this using the as_factor function. group &lt;- as_factor(group) group ## [1] 1 1 1 1 2 2 2 2 3 3 3 3 ## Levels: 1 2 3 This output is a little different from the first lot, but let’s check that it is now a factor: is.factor(group) ## [1] TRUE class(group) ## [1] &quot;factor&quot; Now, let’s try to add two to the group again to see what happens. group + 2 ## Warning in Ops.factor(group, 2): &#39;+&#39; not meaningful for factors ## [1] NA NA NA NA NA NA NA NA NA NA NA NA Great! Now R knows that we’re the ones being stupid! But what if we wanted to assign meaningful labels to the different levels of the factor? Say, for example, we had low, high, and control conditions? We can do it like this: levels(group) &lt;- c(&quot;low&quot;, &quot;high&quot;, &quot;control&quot;) print(group) ## [1] low low low low high high high high control control control control ## Levels: low high control Factors are extremely helpful, and they are the main way to represent nominal scales. It is really important that you label them with meaningful names, as they can help when interpreting output. There are lots of other ways that you can assign labels to your levels. 3.8 Lists Lists arrange elements in a collection of vectors or other data structures. In other words, lists are just a collection of variables, that have no constraints on what types of variables can be included. Emma &lt;- list(age = 26, siblings = TRUE, parents = c(&quot;Mike&quot;, &quot;Donna&quot;) ) Here, R has created a list variable called Emma, which contains three different variables - age, siblings, and parents. Let’s have a look at how R stores this list: print(Emma) ## $age ## [1] 26 ## ## $siblings ## [1] TRUE ## ## $parents ## [1] &quot;Mike&quot; &quot;Donna&quot; If you wanted to extract one element of the list, you would use the $ operator: Emma$age ## [1] 26 You can also add new entries to the list, again using the $. For example: Emma$handedness &lt;- &quot;right&quot; print(Emma) ## $age ## [1] 26 ## ## $siblings ## [1] TRUE ## ## $parents ## [1] &quot;Mike&quot; &quot;Donna&quot; ## ## $handedness ## [1] &quot;right&quot; Nice! These are key things we want you to understand for today. Now, let’s load your Week3_Practice.Rmd file from (download here or from Learn) and you can try these for yourself. Try to attempt all exercises and where necessary, go back to the notes above. Note, that you will need to create the data that we will use next week so make sure to complete all the steps. The solutions are available below but do not look at those yet. 3.9 Practice.Rmd Solutions To do this practice, we will use tidyverse. Make sure that you first run the installation code for the package in your console. Remember once you have done it once, there is no need to do it ever again. install.packages(&#39;tidyverse&#39;) # Load packages library(&#39;tidyverse&#39;) 3.9.1 Exercise 1 Create a list with some information about yourself or play around and store something you think can be best described using lists. Check out the example you saw in the tutorial. # Exercise 1 (example answer) Anastasia&lt;- list(favourite_colour = &#39;blue&#39;, married = TRUE , speak_languages = c(&quot;English&quot;, &quot;Russian&quot;) ) 3.9.2 Exercise 2 Create a nominal variable called sex with three groups: male, female, and other, with four individuals in each group. Make sure to level and label your variable appropriately! I provided an example for this one, try to figure out the second one by yourself. # Exercise 2 (example answer) # Create the variable sex &lt;- c(1,1,1,1,2,2,2,2,3,3,3,3) # Transform into factor sex &lt;- as_factor(sex) # Label the levels levels(sex) &lt;- c(&quot;male&quot;, &quot;female&quot;, &quot;other&quot;) print(sex) Create a variable called “group” that you saw in the tutorial. Remember that one to three represent low, high, and control conditions respectively. # Create group (example answer) group &lt;- c(1,2,3,1,2,2,3,3,1,2,3,3) # Tranform into factor group &lt;- as_factor(group) # Label the levels levels(group) &lt;- c(&quot;low&quot;, &quot;high&quot;, &quot;control&quot;) print(group) 3.9.3 Exercise 3 Earlier we created two variables called group and sex. We also have the test scores and ages of these individuals, so let’s record those as well so we can create a full data set that has everything together. My records for age were: 20, 22, 49, 41, 35, 47, 18, 33, 21, 24, 22, 28 My records were scores were: 70, 89, 56, 60, 68, 62, 93, 63, 71, 65, 54, 67 # Exercise 3 (example answer) age &lt;- c(20, 22, 49, 41, 35, 47, 18, 33, 21, 24, 22, 28) score &lt;- c(70, 89, 56, 60, 68, 62, 93, 63, 71, 65, 54, 67) We now have four variables of the same size in our environment - age, sex, group, and score. Each of them are the same size (i.e. vectors with four elements) and the first entry for age (i.e. age[1]) corresponds to the same person for sex[1]. All of these four variables correspond to the same data set, but R doesn’t know this yet, we need to tell it! Now, let’s put everything side by side: # Now let&#39;s put everything together mydata &lt;- tibble(age, sex, group, score) mydata Note that data is now completely self-contained, and if you were to make changes to say, your original age variable stored in a vector, it will not make any changes to age stored in your data frame. When you have large data frames, you might want to check what variables you have stored in there. To do this, you can ask R to return the names of each of the variables using the names() function. # Check for the variable names names(mydata) # Glimpse at your data glimpse(mydata) This gives you a very basic overview of your data, but is a very helpful tool in displaying the breakdown of what is contained in an object. You might want to get some specific data out of your data frame, as opposed to all four columns. You need to be specific in asking R to return you this information. For example, let’s say you want to extract the scores. # Select specific column select(mydata, score) # Note how we specify data first, then the variable we want # Same fo age select(mydata, age) Quick note: Remember the steps you used to create ‘mydata’ as we will use it again next week to build plots and visualisations. "],
["week-4.html", "Chapter 4 Week 4 4.1 Introduction to plots &amp; geoms (ggplot) 4.2 Visualisations 4.3 Get our data sorted 4.4 Numerical data 4.5 Simple one-variable plot using ggplot() 4.6 Categorical variables 4.7 Continuous variables grouped by a categorical one 4.8 Even more advanced 4.9 Practice.Rmd Solutions", " Chapter 4 Week 4 4.1 Introduction to plots &amp; geoms (ggplot) This week we will walk you through both simple and slightly advanced visualisations in R. Visualisations can in fact be a very creative task and you can let yourself go wild with colours and shapes to visualise your data. :) Good visualisations can also be very powerful when it comes to telling a story with your data. Watch out next time you are reading a news article that uses data. Check how they present the data, what types of graphics they use and how easy it is to read the message. Some cool examples of visualisations made in R for the BBC can be found here. Below we will show you quite a few examples of what you can do with R. This week we want to focus mainly on one-variable visualisations using ggplot(). We will leave some extra materials for you to work through in your own time on how to plot two or more variables. You will need those later in your courses so bookmark the page for the future. :) # Load tidyverse library(tidyverse) 4.2 Visualisations Remember that last week we created some data. But what does it look like? Visualising data is one of the most important tasks facing the data analyst or researcher. It’s important to be able to display your data clearly and coherently, as it makes it easier for the reader to understand. Plus, it helps you to understand your data. The best way to learn plots is by practicing building and modifying simple visualisations. This week we will focus on the key plots you will need to visualise a continuous and a categorical variable, plus a plot that allows you to combine the two. 4.3 Get our data sorted Before we get into using specialised graphics, let’s start by drawing a few very simple graphs to get a feel for what it’s like to draw pictures using R. We will need first to have data. Luckily, there are some in-built datasets that we can use for this illustration. We will use diamonds. 4.4 Numerical data # Get diamonds dataset data &lt;- diamonds We can use help to find out more about this dataset. Try to run the code below: ?diamonds You will see some information in your right-hand pane. It is handy to use ? ... for anything you may want clarification on. # Glimpse glimpse(diamonds) ## Observations: 53,940 ## Variables: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30, 0.23, 0.22, 0.31, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very Good, Fair, Very Good,… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, E, H, J, J, G, I, J, D,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, SI1, SI2, SI2, I1, SI2, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64.0, 62.8, 60.4, 62.2, 60… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58, 54, 54, 56, 59, 56, 55… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 342, 344, 345, 345, 348, 3… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.25, 3.93, 3.88, 4.35, 3.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.28, 3.90, 3.84, 4.37, 3.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.73, 2.46, 2.33, 2.71, 2.… We have quite detailed information about each diamond. Take note of the number of observations and the number of variables. Let’s try to visualise some of the key variables. We will focus on price and cut. 4.5 Simple one-variable plot using ggplot() We can start by creating a space in R where we will map our data. Let’s start by specifying the dataset we want and the variable we are interested in plotting. 4.5.1 Continuous variable # Simple histogram plot for variable &#39;price&#39; - baseline ggplot(data = data, aes(x = price)) Note the key components of the specification above: data: where we provide the name of the dataset. aes: where we provide the aesthetics, i.e. the ‘x-scale’ or more precisely, what we are mapping. We will now need to add geometry - this will be the way our data will be mapped to the space we have just defined: geometry: specifies which type of plot we want to use (i.e geom_bar(), geom_histogram()). Some of the key plot specifications we may need are: For one variable: + geom_bar() adds a bar plot geometry to the graph. + geom_histogram() adds a histogram geometry to the graph. + geom_boxplot() adds a boxplot geometry to the graph. + geom_violin() adds a violin plot geometry to the graph. For two variables: + geom_point() adds a point (scatter plot) geometry to the graph (use with two continuous variables). + geom_boxplot() adds a boxplot geometry to the graph (great for plotting a continuous variable which is grouped). # Simple histogram for variable &#39;price&#39; ggplot(data = data,aes(x = price)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Check carefully what you are seeing. We have an x axis (price) and a y axis (count). We can see that we have more diamonds of the price below $10,000 and fewer very expensive ones. From the first look, I feel the plot could be nicer. Perhaps we could edit our axes, provide a title or change the background? Well, we can do all of these things! Let’s start with the axes and the title. # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram() + labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39; ) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can now modify what our histogram looks like. We will work within geom_histogram(). Note how we leave brackets empty when we are going with the default option. By adding specifications we can modify the look of our graph. I will use color and fill arguments. Try varying these to see how they change: The key colours that are available are (there are more if you need!): Let’s pick some which are good for the reader. If you find that certain colours create certain associations with the data you are plotting that could be helpful or confusing - so please be careful about which ones you choose. We are going with classic: # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(color = &quot;black&quot;, fill = &quot;white&quot;) + # I would like to change the fill to white labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39;) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Try to vary the colours (you could make it quite bright): # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(color = &quot;red&quot;, fill = &quot;blue&quot;) + # I would like to change the fill to blue labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39;) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Whilst we are here, we can also add density to our plot to better visualise the underlying distribution of our variable: # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(aes(y = ..density..), color = &quot;brown&quot;, fill = &quot;grey&quot;) + geom_density() + # Note that I have now added aes(y=..density..) labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39;) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can also adjust the line type around our bins in the histogram, note how we can add linetype= argument inside of geom_histogram(). # Simple histogram for variable &#39;price&#39; ggplot(data = data,aes(x = price)) + geom_histogram(aes(y = ..density..), color = &quot;brown&quot;, fill = &quot;grey&quot;, linetype = &quot;dashed&quot;) + geom_density() + # Note that I have now added aes(y = ..density..) labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39; ) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Lastly, to reflect on some new knowledge, we can also add a line to indicate where the mean of the variable is. We will use geom_vline. # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(aes(y = ..density..), color = &quot;brown&quot;, fill = &quot;grey&quot;, linetype = &quot;dashed&quot;) + geom_density() + # Note that I have now added aes(y = ..density..) and geom_density() geom_vline(aes(xintercept = mean(price)), # Note how we provide x intercept using mean(x) linetype = &quot;dashed&quot;)+ # You can modify the line here as well (change size, colour) labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39;) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. I want to make the line a bit bigger and maybe red so it’s easier to see: # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(aes(y = ..density..), color = &quot;brown&quot;, fill = &quot;grey&quot;, linetype = &quot;dashed&quot;) + geom_density() + # Note that I have now added aes(y = ..density..) geom_vline(aes(xintercept = mean(price)), # Note how we provide x intercept using mean(x) linetype = &quot;dashed&quot;, color = &#39;red&#39;, size = 2) + # You can modify the line here as well (change size, colour) labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39; ) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Nice! Now let’s see how we can work with categorical data. 4.6 Categorical variables In the dataset we are working with we have one interesting categorical variable, the cut of the diamond. It has five levels (Fair, Good, Very Good, Premium, Ideal). Check what it looks like: # Check &#39;cut&#39; select(data, cut) ## # A tibble: 53,940 x 1 ## cut ## &lt;ord&gt; ## 1 Ideal ## 2 Premium ## 3 Good ## 4 Premium ## 5 Good ## 6 Very Good ## 7 Very Good ## 8 Very Good ## 9 Fair ## 10 Very Good ## # … with 53,930 more rows One way to visualise the data would be to start with a simple bar plot. Note how I again specified the data, the x axis and the type of plot I am interested in. # Simple bar plot for variable &#39;cut&#39; ggplot(data = data, aes(x = cut)) + geom_bar() Nice! Let’s do some edits as before. I will do them now in one go. Try to breakdown each modification that was added. I added something new here, namely theme_minimal() so take a note how it changes the plot. # Simple bar plot for variable &#39;cut&#39; ggplot(data = data, aes(x = cut)) + geom_bar(color = &quot;grey&quot;, fill = &quot;cornsilk&quot;) + theme_minimal() + #use different theme labs(x = &quot;Quality of the diamond cut&quot;, # Add x axis label title = &#39;Bar plot for the quality of diamonds cut&#39;) # Add title There are a few others you can try as well (try them out): theme_classic() theme_dark() theme_void() 4.7 Continuous variables grouped by a categorical one Sometimes you may want to provide visualisation of continuous variables using groups. In our case, we could check how the price varies by the cut of the diamond. Note, now I will add aes(x = cut, y = price). I have reversed the axis because I am now interested to show how price varies by cut. Note: This is a sneak peak into what you will be working on in Semester 2 - you will have an actual test to compare these groups for statistical differences but for now, let’s carry on with our plots. ;) I will now build a simple boxplot for price only, note that price is now in the argument as y. # Simple boxplot for variable price ggplot(data = data, aes (y = price)) + geom_boxplot() Now let’s provide visualisations where we also include cut: # Simple boxplot for variable price by cut ggplot(data = data, aes(x = cut, y = price)) + geom_boxplot() Quite cool, don’t you think? We can see that the average price is not that different and some diamonds are just very expensive regardless of the quality of the cut. This could be due to many reasons: carat, shape, clarity, etc. Let’s focus on editing the plot above. Here we can do even more editing - adjusting the colours to differentiate the type of diamond when visualising the price. Let’s first add fill to our aes(): # Simple boxplot for variable price by cut ggplot(data = data, aes(x = cut, y = price, fill = cut)) + geom_boxplot() + theme_minimal() + labs(x = &quot;Quality of the diamond cut&quot;, y = &quot;Price of the diamond ($)&quot;, # Add x and y axes labels title = &#39;Box plot of diamonds price by the cut&#39; ) # Add title We can also play with the colours using a palette, some of the common ones are below: You can now see that we can really edit the colours to our liking. Let’s try using the palette ‘Blues’: # Simple boxplot for variable price by cut ggplot(data = data, aes(x = cut, y = price, fill = cut)) + geom_boxplot() + theme_minimal() + scale_fill_brewer(palette = &quot;Blues&quot;) + #note the palette labs(x = &quot;Quality of the diamond cut&quot;, y = &quot;Price of the diamond ($)&quot;, # Add x and y axes label title = &#39;Box plot of diamonds price by the cut&#39; ) # Add title 4.8 Even more advanced We can also provide visualisations of price distribution by cut all in one go. Check this one out, you will need an extra package so make sure to run install.packages('ggridges'): # Load the package after installation library(ggridges) # Now create fancy plot ggplot(data, aes(x = price, y = cut)) + geom_density_ridges(aes(fill = cut)) + # Note how we add mutiple densities scale_fill_brewer(palette = &quot;Blues&quot;) + # We can assign colours using pallette again labs(x = &quot;Price of the diamond ($)&quot;, y = &quot;Quality of the diamond cut&quot;, # Add x and y axes label title = &#39;Distributions of diamonds price by the cut&#39;) #Add title Nice! Now you have seen some of the basics, we want you to try to build your own plots. Download the Week4_practice.Rmd file (download here or from Learn) and use the notes above to help you where needed. 4.9 Practice.Rmd Solutions Before the start, make sure that you have tidyverse loaded. # Load tidyverse library(tidyverse) 4.9.1 Exercise 1 For your first practice we want you to build a plot based on the example you have seen in the tutorial. We will work with diamonds again. # Get diamonds dataset data &lt;- diamonds Can you check which variables we have? # Glimpse at your data glimpse(data) ## Observations: 53,940 ## Variables: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30, 0.23, 0.22, 0.31, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very Good, Fair, Very Good,… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, E, H, J, J, G, I, J, D,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, SI1, SI2, SI2, I1, SI2, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64.0, 62.8, 60.4, 62.2, 60… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58, 54, 54, 56, 59, 56, 55… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 342, 344, 345, 345, 348, 3… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.25, 3.93, 3.88, 4.35, 3.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.28, 3.90, 3.84, 4.37, 3.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.73, 2.46, 2.33, 2.71, 2.… Select variable carat. # Check carat select(data, carat) ## # A tibble: 53,940 x 1 ## carat ## &lt;dbl&gt; ## 1 0.23 ## 2 0.21 ## 3 0.23 ## 4 0.290 ## 5 0.31 ## 6 0.24 ## 7 0.24 ## 8 0.26 ## 9 0.22 ## 10 0.23 ## # … with 53,930 more rows Produce a simple histogram (finish the expression below). # ggplot of carat ggplot(data = data, aes(x = carat)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Once you are happy with what you see above, try to add labels and change colours. # Complete ggplot for carat ggplot(data = data,aes(x = carat)) + geom_histogram(aes(y =..density..), color = &quot;cadetblue&quot;, fill = &quot;bisque&quot;) + geom_density() + labs(x = &quot;Weight of the diamond (carat)&quot;, title = &#39;Histogram of diamond weight (carat)&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If you want to put both plots side by side: # Assign your plots to specific objects # plot1 plot1 &lt;- ggplot(data = data,aes(x = carat)) + geom_histogram() # plot2 plot2 &lt;- ggplot(data = data,aes(x = carat)) + geom_histogram(aes(y = ..density..), color = &quot;cadetblue&quot;, fill = &quot;bisque&quot;) + geom_density() + labs(x = &quot;Weight of the diamond (carat)&quot;, title = &#39;Histogram of diamond weight (carat)&#39;) You can then install.packages('cowplot') and use the following code: # Load package library(cowplot) # Set theme theme_set(theme_grey()) # Put plots side by side plot_grid(plot1, plot2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.9.2 Exercise 2 Now I am keen again to look at the categorical variable cut and ideally, I want to plot the differences in weight of the diamond by cut. Let’s first produce a plot for cut, you need to use geom_bar here: # ggplot for cut ggplot(data = data, aes(x = cut)) + geom_bar() Try to make it a little nicer. # ggplot for cut (with labels, title, colours) # Simple bar plot for variable &#39;cut&#39; ggplot(data = data, aes(x = cut)) + geom_bar(color = &quot;grey&quot;, fill = &quot;blue&quot;) + labs(x = &quot;Quality of the diamond cut&quot;, title = &#39;Bar plot for the quality of diamonds cut&#39;) + theme_minimal() Finally, get the visualisation of carat by cut: # ggplot for carat by cut (add labels, title, colours) - I started this one for you: ggplot(data = data, aes(x = cut, y = carat, fill = cut)) + geom_boxplot() + theme_minimal() + scale_fill_brewer(palette=&quot;Pastel2&quot;) + labs(x = &quot;Quality of the diamond cut&quot;, y = &quot;Weight of the diamond (carat)&quot;, title = &#39;Box plot of diamond weight (carat) by the cut&#39;) 4.9.3 Exercise 3 - Advanced Here is the plot which is based on slighly different data. For this task you will need to get the data and explore it yourself. You will then want to work with key variables that are visible on the plot below. Your task is to recreate the plots. # Load data data2 &lt;- iris Check what each variable means: ?iris # Simple plot of sepal length ggplot(data = data2, aes(y = Sepal.Length)) + geom_boxplot() # Sepal length by species ggplot(data = data2, aes(x = Species, y = Sepal.Length)) + geom_boxplot() Note that here we also adjusted the position of the legend and the theme. # Advanced plot ggplot(data = data2, aes(x = Species, y = Sepal.Length)) + geom_boxplot(aes(fill = Species)) + ylab(&quot;Sepal Length&quot;) + ggtitle(&quot;Iris Data Boxplot: Sepal Length by Species&quot;) + scale_fill_brewer(palette=&quot;YlGn&quot;) + theme_dark() # Even more advanced plot library(ggridges) ggplot(data2, aes(x = Sepal.Length, y = Species)) + geom_density_ridges(aes(fill = Species)) + # Note how we add mutiple densities scale_fill_brewer(palette = &quot;YlGn&quot;) + # We can assign colours using pallette again labs(x = &quot;Sepal Length&quot;, y = &quot;Species&quot;, title = &#39;Distributions of sepal length by species&#39;) # Add title ## Picking joint bandwidth of 0.181 "],
["week-5.html", "Chapter 5 Week 5 5.1 Central Tendency and Variability 5.2 Load all the necessary packages 5.3 Descriptive Statistics 5.4 Central tendency &amp; variability 5.5 Mean and median 5.6 Using piping operator for descriptive statistics 5.7 Standard deviation 5.8 Visualise your data 5.9 Practice.Rmd Solutions", " Chapter 5 Week 5 5.1 Central Tendency and Variability The recommended additional reading can be found in Chapter 5 of Navarro Textbook. We have largely summarised the key information from the reading and the lecture notes in this week’s materials so that you can get a grasp of how to quickly produce descriptions of your data in R. By the end of this week, hopefully you will get some intuition about reading data in R and providing visualisations to accompany your descriptive data analysis. 5.2 Load all the necessary packages library(tidyverse) 5.3 Descriptive Statistics When statisticians are asked to describe what their data looks like, they will often start with descriptive statistics as a quick way to inform people about their data or variable distributions. It may sound weird now but as you move through the years of doing statistics you will find that just reporting the mean, median and standard deviation can easily allow you to visualise the data in your mind without the need for any graphics. Descriptive statistics are an essential component of any research paper or report and you will often find them in those of your readings that use empirical evidence. Interestingly, when it comes to news articles, often only the mean or median are reported (e.g. “Median wage in the UK is £29,588”). Without additional information on sample size or other measures of central tendency, these statistics can be quite meaningless and can trick the reader. We will show you why in the practice exercises below. 5.4 Central tendency &amp; variability To illustrate measures of central tendency we will be using quite a common but easy to understand example of flight delays. Knowing the average flight delay is handy when you are making plans for travelling, but without knowing the actual distribution of the delays for a given dataset, we cannot really know exactly what to expect. Here is some data I found online about average flight delays across some of the UK’s biggest airports. 2 5.5 Mean and median We will work with flight delays in Edinburgh, since we are based here! You will know from experience that you rarely have a delay of exactly 15.8 minutes and in reality, delays could easily range from a minute to a few hours. This may depend on the day of the year, the airline, the time of day and many other factors. Consequently, taking the mean at face value is not always the best strategy! I have collected the data for the delay times of flights on Christmas Eve and a randomly selected midweek day during autumn. Let’s see what we have. The data comes in minutes. Read the data in using the command below. We will then explore the data using some descriptive measures. We will try to see if we can tell anything about the distribution of the data from the central tendency measures - we will then visualise it to check how good our intuition was. 5.5.1 Reading data in We will need to read in the data after saving it in our folder. We will do the steps together but if you forgotten here is the quick guide: For RCloud it is the same process really, just make sure you have uploaded your file, then select working directory and read in the file. There is another way to set your working directory. Go to Session -&gt; Set Working Directory -&gt;Choose Directory -&gt;Locate your folder . Then run the following: # Read data in edinburgh_delays &lt;- read.csv(&#39;edinburgh_delays.csv&#39;) Note that data now appears in your Environment. # Check what is inside head(edinburgh_delays) ## delay_time day ## 1 10 christmas eve ## 2 26 christmas eve ## 3 35 christmas eve ## 4 12 christmas eve ## 5 120 christmas eve ## 6 100 christmas eve 5.6 Using piping operator for descriptive statistics 5.6.1 Mean The mean is also known as the average across your observations and is achieved by summing all of the observations together and then dividing by the total number of obervations (N). The equation for the mean is: \\[\\frac{\\sum\\limits_{i = 1}^{N}x_i}{N}\\] Get the means: # Overall mean edinburgh_delays %&gt;% summarise(mean = mean(delay_time)) ## mean ## 1 27.775 # Mean (grouped by day) edinburgh_delays %&gt;% group_by(day) %&gt;% summarise(Mean = mean(delay_time)) ## # A tibble: 2 x 2 ## day Mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 average day 15.3 ## 2 christmas eve 40.2 It seems like the mean delay time on an ‘average day’ is much closer to what the article reported compared to the mean Christmas Eve delay time which is more than double. Let’s explore it a bit more. 5.6.2 Median The median suggests where the centre of the distribution is. If the data is skewed (e.g. remember our diamond prices from last week?), the median is certainly a better measure of central tendency than the mean. edinburgh_delays %&gt;% group_by(day) %&gt;% summarize(Median = median(delay_time)) ## # A tibble: 2 x 2 ## day Median ## &lt;fct&gt; &lt;dbl&gt; ## 1 average day 15.5 ## 2 christmas eve 26 Note the median for Christmas Eve. Or all in one go: edinburgh_delays %&gt;% group_by(day) %&gt;% summarise( median = median(delay_time), mean = mean(delay_time) ) ## # A tibble: 2 x 3 ## day median mean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 average day 15.5 15.3 ## 2 christmas eve 26 40.2 5.7 Standard deviation We can also calculate standard deviation to get an idea of the variability around our means. Note the formula for the overall variance around the mean first: \\[{\\sigma}^2=\\frac{1}{N-1}\\sum_{i=1}^{N} (X_{i} -\\bar{X})^2 \\] The standard deviation is by derivation: \\[{\\sigma}=\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N} (X_{i} -\\bar{X})^2} \\] Let’s see what it looks like numerically: # Describe the flight delays (mean, median, variance, sd) edinburgh_delays %&gt;% group_by(day) %&gt;% summarise( mean = mean(delay_time), median = median(delay_time), variance = var(delay_time), sd = sd(delay_time) ) ## # A tibble: 2 x 5 ## day mean median variance sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 average day 15.3 15.5 14.8 3.85 ## 2 christmas eve 40.2 26 1400. 37.4 We can make some preliminary conclusions about the data. Note the standard deviation for Christmas Eve. We already know there is a large difference between the two means, but standard deviation can tell us about the spread of our data around the mean. There is less variation on an average day than on Christmas Eve. Before we make some plots, note that you can save your summary statistics as an object. # Save descriptives as an object descriptives &lt;- edinburgh_delays %&gt;% group_by(day) %&gt;% summarise( median = median(delay_time), mean = mean(delay_time), variance = var(delay_time), sd = sd(delay_time) ) descriptives ## # A tibble: 2 x 5 ## day median mean variance sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 average day 15.5 15.3 14.8 3.85 ## 2 christmas eve 26 40.2 1400. 37.4 5.8 Visualise your data Now we have gathered all the numerical descriptions, let’s see how good our intuition was about what the data looks like. Let’s try visualising the delays regardless of the day: # Visualise the data using histogram ggplot(data = edinburgh_delays, aes(x = delay_time)) + geom_histogram(aes(y = ..density..), binwidth = 0.7, color = &quot;blue&quot;) + geom_density() + xlab(&quot;Delay time (min)&quot;) + ggtitle(&quot;Distribution of Delays&quot;) We have quite a variation. We also know that there were differences in the means and the standard deviations when we checked the variable by day. Let’s now visulise each day separately. We can do these with a mini %in% in ggplot(). Pretty cool, right? # Visualise the data using a histogram by day (average day) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;average day&quot;)), aes(x = delay_time)) + geom_histogram() + geom_vline(aes(xintercept = mean(delay_time)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color = &#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Average Day)&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Visualise the data using a histogram by day (average day) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;average day&quot;)), aes(x = delay_time)) + geom_histogram(color =&#39;grey&#39;, fill =&#39;cornsilk&#39;) + geom_vline(aes(xintercept = mean(delay_time)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color = &#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Average Day)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Visualise the data using a histogram by day (Christmas Eve) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = delay_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(delay_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Conclusions? Of course, when describing the average delay for the news article it is best to focus on an average day rather than Christmas Eve. From the example above, if we take an average autumn weekday our data matches what the article suggests pretty well. Nevertheless, it tell us little about what to expect during the weekend or a busy public holiday. To gauge the range of delays and the variation around the mean value, we would need statistics for the mean, median and standard deviation to get a better picture about the situation. Graphics will help us even more! As a final one, let’s also add a box plot or ggridges(): # Boxplot to visualise a comparison ggplot(data = edinburgh_delays, aes(y = delay_time, x = day, fill = day)) + geom_boxplot() + labs(x = &#39;Day&#39;, y = &quot;Delay time (min) &quot;, title = &quot;Delays at the Edinburgh Airport &quot;) + theme_minimal() # ggridges library(ggridges) ggplot(data = edinburgh_delays, aes(y = day, x = delay_time, fill = day)) + geom_density_ridges2() + labs(x = &#39;Delay time (min)&#39;, y = &quot;Density&quot;, title = &quot;Delays at the Edinburgh Airport&quot;) + theme_minimal() ## Picking joint bandwidth of 8.38 Note: before jumping to any conclusions, remember that we only have records for 40 flights for each of the days. Edinburgh Airport can see as many as 313 flights a day. Since we picked our data at random we can consider our samples representative (note: a rule of thumb is &gt; 25 but this can vary depending on the phenomena you are trying to describe). Let’s try to explore something else now. Load your practice file for today and work with the income data in a similar fashion to what we just did. 5.9 Practice.Rmd Solutions Make sure to load the packages first: # Load tidyverse library(tidyverse) 5.9.1 Income Distribution Example Let’s look at some data on income collected for the UK in 2017 by ONS. Note the mean and median values and the skew. What can you say about the income distribution in the UK? How representative is the data of the true population? You can read more about income calculations in the ONS report, if curious of course. I decided to collect my own sample of individuals so I collected the data for 115 residents aged 18+ in London. I further grouped them into North or South London. Use the data to study the variation in income. # Get the data income_london &lt;- read.csv(&quot;income_london.csv&quot;) Open your Rmd template (from here or from Learn) and the data (from here or Learn) for this week and attempt the following steps. Work with the data on income to provide information about the income distribution for Londoners. Save an RMarkdown file for this lab, make notes where necessary and replicate the steps you have seen in the worked example. Make sure to visualise the data, describe it and write a few notes with conclusions about what you found. Are there differences based on geography (i.e. North versus South)? Pay attention to the sample size and how representative it is of the true population (i.e. can we use the London mean to represent the UK?) Compile your file in the end to produce the final report. Polish off some chunks to make it all look nicer! Congratulate yourself on the amazing work you have done over past five weeks! # Check what&#39;s inside head(income_london) ## income region ## 1 64352 north ## 2 49667 north ## 3 57265 north ## 4 20106 north ## 5 54457 north ## 6 72379 north # Means income_london %&gt;% summarise(mean = mean(income)) ## mean ## 1 35504.47 # Means by region income_london %&gt;% group_by(region) %&gt;% summarise(mean = mean(income)) ## # A tibble: 2 x 2 ## region mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 north 37348. ## 2 south 33661. # Medians by region income_london %&gt;% group_by(region) %&gt;% summarise(median = median(income)) ## # A tibble: 2 x 2 ## region median ## &lt;fct&gt; &lt;dbl&gt; ## 1 north 35766 ## 2 south 34016. # Means, medians and SDs by region descriptives &lt;- income_london %&gt;% group_by(region) %&gt;% summarise( mean = mean(income), median = median(income), sd = sd(income) ) descriptives ## # A tibble: 2 x 4 ## region mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 north 37348. 35766 14937. ## 2 south 33661. 34016. 10719. 5.9.2 Visualisations 5.9.2.1 Visualise the distribution of income # Visualise the data using a histogram and/or a density plot ggplot(data = income_london, aes(x = income)) + geom_histogram(aes(y = ..density..), color = &#39;grey&#39;, fill = &#39;yellow&#39;) + geom_density() + geom_vline(aes(xintercept = mean(income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Income (£)&quot;, title = &quot;Distribution of Londoners Income&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 5.9.2.2 Visualise the distribution of income by region Now, focus on regions and provide a separate plot for ‘South’ and ‘North’. Feel free to add mean and median lines too. You will note that the central tendency measures may have changed a little but the shape of the data remains almost the same (i.e. approximately normal). # Visualise the data using a histogram by region (South) ggplot(data = subset(income_london, region %in% c(&quot;south&quot;)), aes(x = income)) + geom_histogram(aes(y = ..density..), color = &#39;grey&#39;, fill = &#39;orange&#39;) + geom_density() + geom_vline(aes(xintercept = mean(income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Income (£) &quot;, title = &quot;Distribution of Londoners Income (South)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Visualise the data using a histogram by region (North) ggplot(data = subset(income_london, region %in% c(&quot;north&quot;)), aes(x = income)) + geom_histogram(aes(y = ..density..), color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_density() + geom_vline(aes(xintercept = mean(income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Income (£)&quot;, title = &quot;Distribution of Londoners Income (North)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Boxplot to visualise a comparison ggplot(data = income_london, aes(y = income, x = region, fill = region)) + geom_boxplot() + labs(x = &#39;Region&#39;, y = &quot;Income (£) &quot;, title = &quot;Distribution of Londoners Income by Region&quot;) What can you conclude about regional differences from studying your data? We observed that there are slight differences in the central tendency measures between North and South London, however we cannot say anything more at this point. Next semester, we will show you an appropriate statistical test that you can use to provide a confident answer to such questions. Compare the results you have observed to the graphic we have from ONS. Are there any differences? Can we use the London data to describe the whole population? Finally, praise yourself for all the work so far :) library(praise) praise() ## [1] &quot;You are shining!&quot; Original Article ‘The average flight delay at each of the UK’s 25 busiest airports’↩ "],
["week-6.html", "Chapter 6 Week 6 6.1 Plotting Functions and Simple Data Transformation 6.2 Load all the necessary packages 6.3 Part 1: Hand plotting functions 6.4 Basics 6.5 Linear functions 6.6 Practice plotting 6.7 Part 2: Introduction to Data Transformation 6.8 Renaming 6.9 Adding new variables 6.10 Transformation and skewness 6.11 Mean Centering 6.12 Standardisation 6.13 Practice.Rmd Solutions", " Chapter 6 Week 6 Practice.Rmd here or on Learn. We’ll use the same data as last week (you can download it again from here or Learn). 6.1 Plotting Functions and Simple Data Transformation The key reading that you may find useful for this week: Chapter 5 on Data Transformation - R for Data Science by Garett Grolemund and Hadley Wikham 6.2 Load all the necessary packages library(tidyverse) 6.3 Part 1: Hand plotting functions This week we will be focusing on learning more about various functions and the nature of the data you may be dealing with as a psychology researcher. We will also introduce some hand calculations so make sure you have some paper and a pencil with you to practice these. The main aim is to build intution about functions and learn how these can be applied to the data. 6.4 Basics You have seen by now how to plot your data. Thanks to mathematics, each of the plots we presented to you earlier can be described mathematically by formulating the functions that describe the data best. This section will provide a foundation for understanding functions that can be used to describe relationships in your data. 6.5 Linear functions 6.5.1 Tree height example You will find that in statistics most analytical approaches are based on the assumption that a proccess in your data can be described using linear functions. The most common form would be : \\[y=a+bx\\] Please note that you always have two axes (x and y). Your \\(x\\) will always represent an independent process and \\(y\\) will represent the response in some variable due to change in the independent process. In the equation above \\(a\\) and \\(b\\) are constans. This may sound really complicated so here is an example. As the time passes, the tree height is increasing. Here Time will be our \\(x\\) - an independent variable and (Tree height) will be a dependent variable. In other words, Tree height will be our \\(y\\) - it depends on x (Time). ggplot(data = tree, aes(x = x, y = y)) + geom_line(color = &#39;blue&#39;) + geom_point() + xlab(&quot;Time (years)&quot;) + ylab(&quot;Tree Height (cm)&quot;) + ggtitle(&quot;Tree height&quot;) Note that the height is affected by Time but Time is not affected by Tree Height. Hence the relationship. Your choice of what goes where in your graph depends largely on the assumption about the process you are studying. If we were to fit a model to this data the function we would use is: \\[y=a+bx\\] Tree height (y): \\[y = 5+15x\\] Note that 5 is where we start our line or in other words the value of \\(y\\) when \\(x\\) is zero, and 15 is the slope of the line. Interpretation: The tree will grow in height by about 15 cm each year. 6.5.2 Non-linear functions (first order polynomials) Of course there are cases where the change in \\(y\\) will not always be the same for every unit change of \\(x\\). Such can also be described as non-constant change in y as a function of x. For those cases, polynomials are very helpful. Consider the case below: If we have a sequence of \\(x\\) represented by: \\[x=\\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\\\ 6 \\\\ 7 \\\\ 8\\\\ 9\\\\ 10\\\\ \\end{bmatrix} \\] Then for: \\[f(x)=x^2\\] We will have a corresponding \\(f(x)=y\\): \\[f(x)=y=\\begin{bmatrix} 0 \\\\ 1 \\\\ 4 \\\\ 9 \\\\ 16 \\\\ 25\\\\ 36 \\\\ 49 \\\\ 64\\\\ 81\\\\ 100\\\\ \\end{bmatrix} \\] Can you see how it works? Try now by yourself. 6.6 Practice plotting To play a bit with various functions and plotting, consider using both R and a piece of paper and a pencil. Imagine that you have following values for \\(x\\): \\[x=\\begin{bmatrix} 2 \\\\ 4 \\\\ 5 \\\\ 8 \\\\ 12 \\\\ 16\\\\ 18 \\\\ 22 \\\\ \\end{bmatrix} \\] We are keen to provide visualisations for various functions of x that can be expressed via \\(f(x) = y\\) To plot the following function \\[y=x\\] we can frist create a mini data that will have our \\(x\\) and the function we want to apply to the vector of \\(x\\). # Assign x and y data_1 &lt;- tibble(x = c(2,4,5,8,12,16,18,22), y = x) # Plot ggplot(data = data_1, aes(x = x, y = y)) + geom_point() + geom_line() + xlab(&quot;x&quot;) + ylab(&quot;y = x&quot;) + ggtitle(&quot;Plot of y = x&quot;) Note the response in \\(y\\) for each value of \\(x\\). You can check the result in R too by running data. data_1 ## # A tibble: 8 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 ## 2 4 4 ## 3 5 5 ## 4 8 8 ## 5 12 12 ## 6 16 16 ## 7 18 18 ## 8 22 22 Now try with: \\[y=x-3x\\] # Assign x and y data_2 &lt;- tibble(x = c(2,4,5,8,12,16,18,22), y = x-3*x) # Plot ggplot(data = data_2, aes(x = x, y = y)) + geom_point() + geom_line() + xlab(&quot;x&quot;) + ylab(&quot;y = x - 3x&quot;) + ggtitle(&quot;Plot of y = x - 3x&quot;) data_2 ## # A tibble: 8 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 -4 ## 2 4 -8 ## 3 5 -10 ## 4 8 -16 ## 5 12 -24 ## 6 16 -32 ## 7 18 -36 ## 8 22 -44 Now try with: \\[y=(x+3)^2\\] # Assign x and y data_3 &lt;- tibble(x = c(2,4,5,8,12,16,18,22), y = (x + 3)^2) # Plot ggplot(data = data_3, aes(x = x, y = y)) + geom_point() + geom_line() + xlab(&quot;x&quot;) + ylab(&quot;y = (x + 3)^2&quot;) + ggtitle(&quot;Plot of y = (x + 3)^2&quot;) data_3 ## # A tibble: 8 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 25 ## 2 4 49 ## 3 5 64 ## 4 8 121 ## 5 12 225 ## 6 16 361 ## 7 18 441 ## 8 22 625 Finally, try with: \\[y=log(x)\\] # Assign x and y data_4&lt;- tibble(x = c(2,4,5,8,12,16,18,22), y = log(x)) # Plot ggplot(data = data_4, aes(x = x, y = y)) + geom_point() + geom_line() + xlab(&quot;x&quot;) + ylab(&quot;y = log(x)&quot;) + ggtitle(&quot;Plot of y = log(x)&quot;) data_4 ## # A tibble: 8 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0.693 ## 2 4 1.39 ## 3 5 1.61 ## 4 8 2.08 ## 5 12 2.48 ## 6 16 2.77 ## 7 18 2.89 ## 8 22 3.09 6.7 Part 2: Introduction to Data Transformation This week we will walk you through examples of various data transformations you can do using tidyverse. When working with real data you often may be interested to apply various transformations to your variable to change or/and adjust the scale of your data or for many other purposes. We will use the datasets from Week 5 and will focus on understanding mutate() function. We will further discuss how variable transformations can address the skewness in your variable distributions - if this may seem a bit confusing at this stage, bear with us - we will get back to it soon enough. Let’s start by loading tidyverse. # Load tidyverse library(tidyverse) And the data from last week - edinburgh_delays. # Load the data edinburgh_delays &lt;- read.csv(&#39;edinburgh_delays.csv&#39;) Whilst we are here let us provide a quick data decscription: # Quick description edinburgh_delays %&gt;% group_by(day) %&gt;% summarise(mean = mean(delay_time), median = median(delay_time), sd = sd(delay_time)) ## # A tibble: 2 x 4 ## day mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 average day 15.3 15.5 3.85 ## 2 christmas eve 40.2 26 37.4 From last week remember that Christmas Eve delay times were slighly skewed as we had some very extreme delays which have affected the shape of our histogram. # Visualise the data using a histogram by day (average day) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;average day&quot;)), aes(x = delay_time)) + geom_histogram(color =&#39;grey&#39;, fill =&#39;cornsilk&#39;) + geom_vline(aes(xintercept = mean(delay_time)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color = &#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Average Day)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. And for Christmas Eve too: # Visualise the data using a histogram by day (christmas eve) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = delay_time)) + geom_histogram(color =&#39;grey&#39;, fill =&#39;cornsilk&#39;) + geom_vline(aes(xintercept = mean(delay_time)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color = &#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.8 Renaming Now we not only want to specify that delay time is measured in minutes but we also want to add an extra variable that will provide us with an idea of the delay time in hours. # Rename variable time as delay_min edinburgh_delays &lt;-rename(edinburgh_delays, delay_min = delay_time) 6.9 Adding new variables # Change time into hours edinburgh_delays &lt;- mutate(edinburgh_delays, # Note how we use mutate() delay_hours = delay_min / 60) # We want to divide minutes by 60 to get the number of hours 6.10 Transformation and skewness Now, look back at your histograms for both Christmas Eve and Average Day. Whilst the shape of the Average Day distribution may look quite symmetrical, we may want to address the skewness in the Christmas Eve shape. There are two common ways to do this: taking the log() of the variable or taking the sqrt(). Let us see what these will do in action: # Transform using log edinburgh_delays &lt;- mutate(edinburgh_delays, log_time = log(delay_min)) # Transform using sqrt edinburgh_delays &lt;- mutate(edinburgh_delays, sqrt_time = sqrt(delay_min)) # Visualise the data using a histogram by day (Christmas Eve) now with log transformation ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = log_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(log_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(log_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (log(min))&#39;,y = &#39;Frequency&#39;, title = &#39;Log of Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Visualise the data using a histogram by day (Christmas Eve) now with sqrt transformation ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = sqrt_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(sqrt_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(sqrt_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (sqrt(min))&#39;, y = &#39;Frequency&#39;, title = &#39;Sqrt of Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Log is much more effective in dealing with skewness but from these plots it might be hard to tell. Focus on how close the mean and median values are. Let us compare the real values with the log now - we can use a cowplot. Please note how we first assign our plots to objects which we then plot. # Put side-by-side to compare library(cowplot) # Original data plot plot_original &lt;- ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = delay_min)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(delay_min)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_min)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min) &#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Christmas Eve) &#39;) + theme_minimal() # Log transformed plot plot_log &lt;- ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = log_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(log_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(log_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (log(min))&#39;, y = &#39;Frequency&#39;) + theme_minimal() # To put side by side plot_grid(plot_original, plot_log) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.11 Mean Centering Mean centering is one of the most common transformation for a continious variable. To provide transformation in R we can use the following: # Mean centering edinburgh_delays &lt;- mutate(edinburgh_delays, mean_centered_time = delay_min - mean(delay_min)) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = mean_centered_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(mean_centered_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(mean_centered_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time - mean centered&#39;, y = &#39;Frequency&#39;, title = &#39; Mean Centered Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.12 Standardisation Lastly, we can also standardise our variables by calculating the distance between each observation and the mean in standard deviation units using scale(), note that the results are very smiliar to mean centering technique. # Transform using scale() edinburgh_delays &lt;- mutate(edinburgh_delays, stand_time = scale(delay_min)) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = stand_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(stand_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(stand_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time - standardized&#39;, y = &#39;Frequency&#39;, title = &#39; Standardised Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We will talk more about these as we progress through the course - for now, just take a note of the various transformations that you can apply to your numeric variables. 6.13 Practice.Rmd Solutions For this week practice we want you to work with simple functions and basic data transformations. Part 1: Work with the vector of \\(x\\) and apply the following functions. If we have \\(x_1\\) represented by: \\[x_1=\\begin{bmatrix} -10 \\\\ 1\\\\ 4 \\\\ 19 \\\\ - 6 \\\\ 0 \\\\ 14 \\\\ -8 \\\\ 0\\\\ 12\\\\ 9\\\\ \\end{bmatrix} \\] And \\(x_2\\) represented by: \\[x_2=\\begin{bmatrix} -3 \\\\ 2\\\\ 1 \\\\ 0 \\\\ 1 \\\\ 2\\\\ 3 \\\\ 5 \\\\ -1\\\\ -2\\\\ -3\\\\ \\end{bmatrix} \\] Provide the results for: \\(y_1=x_1^3\\) \\(y_2=3x_1-5x_1\\) \\(y_3=x_1*x_2\\) Note: you can use mutate() to add an extra variables to represent each of \\(y_i\\). Have a look at the solutions for the hints. # Create the data first with x online my_data &lt;- tibble(x1 = c(-10,1,4,19,-6,0,14,-8,0,12,9), x2 = c(-3,2,1,0,1,2,3,5,-1,-2,-3)) We can use mutate() to add extra variables for \\(y\\): my_data &lt;- mutate(my_data, y1 = x1^3, y2=3*x1-5*x1, y3=x1*x2) my_data ## # A tibble: 11 x 5 ## x1 x2 y1 y2 y3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -10 -3 -1000 20 30 ## 2 1 2 1 -2 2 ## 3 4 1 64 -8 4 ## 4 19 0 6859 -38 0 ## 5 -6 1 -216 12 -6 ## 6 0 2 0 0 0 ## 7 14 3 2744 -28 42 ## 8 -8 5 -512 16 -40 ## 9 0 -1 0 0 0 ## 10 12 -2 1728 -24 -24 ## 11 9 -3 729 -18 -27 Nice! Part 2: Let’s practice with the income data which we worked with last week. Load the data in and work in the template. Create an extra variable called income_thousands which will provide information on income in thousands of pounds instead of just pounds. Visualise the distribution of income. Do you think we need to transform the income variable? Try one of the transformations you have seen today and visualise your data post-transformation (hint: try with log() and mean centering) library(tidyverse) Load the data in and work in the template. Provide descriptives by region. # Read data in income_london &lt;- read.csv(&#39;income_london.csv&#39;) # Quick description income_london %&gt;% group_by(region) %&gt;% summarise(mean = mean(income), median = median(income), sd = sd(income)) ## # A tibble: 2 x 4 ## region mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 north 37348. 35766 14937. ## 2 south 33661. 34016. 10719. Create an extra variable called income_thousands which will provide information on income in thousands of pounds instead of pounds. # Create variable income_thousands using mutate() income_london &lt;- mutate(income_london, # Note how we use mutate() income_thousands = income / 1000) # We want to divide income by 1000 Visualise the distribution of income using thousands. Do you think we need to transform the income variable? Try one of the transformations you have seen today and visualise your data. # Distribution of income in thousands ggplot(data = income_london, aes(x = income_thousands)) + geom_histogram( color = &#39;grey&#39;, fill = &#39;yellow&#39;) + geom_vline(aes(xintercept = mean(income_thousands)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(income_thousands)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Income (thousands of pounds)&quot;, title = &quot;Distribution of London Incomes (thousands of pounds)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. In the context of the distribution, we may find that the shape is pretty symmetrical and our data can be described using the properties of normal distribution. To make the scale of income more comparative we can still try to transform. Let’s do the log(income). # Log transform income_london &lt;- mutate(income_london, log_income = log(income)) And quickly visualise: # Distribution of log(income) ggplot(data = income_london, aes(x = log_income)) + geom_histogram( color = &#39;grey&#39;, fill = &#39;brown&#39;) + geom_vline(aes(xintercept = mean(log_income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(log_income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;log(Income)&quot;, title = &quot;Distribution of Londoners log(Income)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Through rescaling we achieved a reverse - now, our distribution is negatively skewed. Try with the mean centering as well. # Mean Centered income_london &lt;- mutate(income_london, mean_centered_income = income-mean(income)) And quickly visualise: # Distribution of mean centered income ggplot(data = income_london, aes(x = mean_centered_income)) + geom_histogram( color = &#39;grey&#39;, fill = &#39;brown&#39;) + geom_vline(aes(xintercept = mean(mean_centered_income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(mean_centered_income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Mean Centered Income&quot;, title = &quot;Distribution of Londoners Income (Mean Centered)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Take a note of the transformation. Why do you think mean centered income here might be a better transformation of the data compared to that of log()? "],
["week-7.html", "Chapter 7 Week 7 7.1 Reading in Data, Merge and More R Practice 7.2 Dataset 1 (.csv) 7.3 Dataset 2 (.txt) 7.4 Dataset 3 (.sav and .dta) 7.5 Merging datasets together 7.6 Sorting and arrange() 7.7 Data description 7.8 Using filter() 7.9 Visualisations 7.10 Save the file in your folder 7.11 Practice.Rmd Solutions", " Chapter 7 Week 7 Practice.Rmd here or on Learn. A zipped folder of all the datasets can be downloaded here or from Learn. Individual datasets are also available: - data_students_1.csv - data_students_2.txt - data_students_3.csv - data_students_3.dta - data_students_3.sav 7.1 Reading in Data, Merge and More R Practice This week we will focus on reading various datasets and merging them together where necessary. This can be useful if you collected data at different times or might be merging datasets from different studies for your analysis. We will also work through a few code examples that can help you to navigate around the data and filter observations, using filter and arrange from tidyverse. You can find more examples in the books here: Chapter 5 on Data Transformation - R for Data Science by Garett Grolemund and Hadley Wikham Chapter on Descriptive Statistics and Data Manipuliation - Modern R with the Tidyverse by Bruno Rodrigues Reading in and merging data of different formats can be tricky if you don’t have R to help you. We will walk you through a fairly simple example today to build your intutition. When you get to Years 3 and 4 you will get to work with your own dissertation data - we hope you’ll find the notes here useful for then. First things first, let’s load tidyverse. library(tidyverse) This week we will work with the datasets that have information on students’ grades for different programmes. We have got three separate datasets which we want to put together for the analysis. The first one has data on programmes for 15 students. The second dataset provides grades for the same students. We then have a separate dataset that has information on another 15 students and their respective programmes and grades. On top of that, each dataset comes in a different format. We have .csv, .txt, .sav (SPSS) and also .dta (STATA) format. Confusing right? Our task for today will be to find an efficient way to create a single dataset that has information for these 30 students that you will then work with in your practice. 7.2 Dataset 1 (.csv) We have student IDs and grades. In terms of observations, we have 15 in total. ID (ID1, ID2, ID3… ID15) grades (1-100) The dataset comes in the format familiar to us, csv, so we know how to read that one in, please note that this week we introduce a tidyverse read_csv() function which can be useful for us when it comes to joining the data. data_students_1 &lt;- read.csv(&#39;data_students_1.csv&#39;) 7.3 Dataset 2 (.txt) We have student IDs and programme. In terms of observations, we have 15 in total. ID (ID1, ID2, ID3… ID15) programme (‘psych’, ‘lang’, and ‘phil’) Here, we have got a .txt format. Not a problem for R: data_students_2 &lt;- read.table(&quot;data_students_2.txt&quot;, header = TRUE) # Note that we add the header TRUE which will read the first line in the file as the column names. 7.4 Dataset 3 (.sav and .dta) We have student IDs, grades and programme. In terms of observations, we have 15 in total but these are different students so we will want to add those with the previous datasets later. ID (ID16… ID30) grades (1-100) programme (‘psych’, ‘lang’, and ‘phil’) We have .dta format here which may look foreign to you as it looks like the data was saved by a different software. To deal with those in R, we can install package foreign and then read directly from the format: library(foreign) data_students_3 &lt;- read.dta(&#39;data_students_3.dta&#39;) You may also note that we have data_students_3.sav. This format comes from very popular software that psychology researchers often use, SPSS. We can open it vis foreign as well: data_students_3 &lt;- read.spss(&quot;data_students_3.sav&quot;, to.data.frame=TRUE) # note the argument for data.frame - if you don&#39;t specify, the data will be loaded as the list 7.5 Merging datasets together Once the data is visible in the environment, we can start attempting to bring them together. There are number of ways to do this. Let us start with the most intuitive one. We can merge datasets 1 and 2 using the ID column. We are lucky to have a unique identifier which can allow us to bring datasets together so we can have our grades and programme all in one dataset. Exploring data first can help: head(data_students_1) ## X ID grades ## 1 1 ID_1 20 ## 2 2 ID_2 35 ## 3 3 ID_3 45 ## 4 4 ID_4 85 ## 5 5 ID_5 70 ## 6 6 ID_6 72 head(data_students_2) ## ID programme ## 1 ID_1 psych ## 2 ID_2 lang ## 3 ID_3 phil ## 4 ID_4 psych ## 5 ID_5 lang ## 6 ID_6 phil We can merge data by ID now and we will use full_join(). # Full join students_grades_prog &lt;- full_join(data_students_1, data_students_2, by = c(&#39;ID&#39;)) # We can specify the unqiue variable we use to match the datasets via the &#39;by =&#39; argument. Quickly check that you got what you wanted: head(students_grades_prog) ## X ID grades programme ## 1 1 ID_1 20 psych ## 2 2 ID_2 35 lang ## 3 3 ID_3 45 phil ## 4 4 ID_4 85 psych ## 5 5 ID_5 70 lang ## 6 6 ID_6 72 phil Nice! Now we can work with this data a bit using some extra functions from tidyverse. 7.6 Sorting and arrange() To check how the data looks when sorted we can use arrange():: # Sort in ascending order (default option) students_grades_prog %&gt;% arrange(grades) ## X ID grades programme ## 1 1 ID_1 20 psych ## 2 2 ID_2 35 lang ## 3 15 ID_15 40 phil ## 4 11 ID_11 44 lang ## 5 3 ID_3 45 phil ## 6 10 ID_10 56 psych ## 7 9 ID_9 58 phil ## 8 8 ID_8 60 lang ## 9 14 ID_14 64 lang ## 10 7 ID_7 65 psych ## 11 12 ID_12 68 phil ## 12 5 ID_5 70 lang ## 13 13 ID_13 70 psych ## 14 6 ID_6 72 phil ## 15 4 ID_4 85 psych # Sort in descending order (add &#39;desc&#39;) students_grades_prog %&gt;% arrange(desc(grades)) ## X ID grades programme ## 1 4 ID_4 85 psych ## 2 6 ID_6 72 phil ## 3 5 ID_5 70 lang ## 4 13 ID_13 70 psych ## 5 12 ID_12 68 phil ## 6 7 ID_7 65 psych ## 7 14 ID_14 64 lang ## 8 8 ID_8 60 lang ## 9 9 ID_9 58 phil ## 10 10 ID_10 56 psych ## 11 3 ID_3 45 phil ## 12 11 ID_11 44 lang ## 13 15 ID_15 40 phil ## 14 2 ID_2 35 lang ## 15 1 ID_1 20 psych We can also sort by other variables: # Sort in ascending order by programme (just remove desc()) students_grades_prog %&gt;% arrange(programme) ## X ID grades programme ## 1 2 ID_2 35 lang ## 2 5 ID_5 70 lang ## 3 8 ID_8 60 lang ## 4 11 ID_11 44 lang ## 5 14 ID_14 64 lang ## 6 3 ID_3 45 phil ## 7 6 ID_6 72 phil ## 8 9 ID_9 58 phil ## 9 12 ID_12 68 phil ## 10 15 ID_15 40 phil ## 11 1 ID_1 20 psych ## 12 4 ID_4 85 psych ## 13 7 ID_7 65 psych ## 14 10 ID_10 56 psych ## 15 13 ID_13 70 psych 7.7 Data description Now, we have a full dataset we can try to do some simple analysis on the data and study the variation in grades across the cohort and by programme. # All grades students_grades_prog %&gt;% summarise(mean = mean(grades), median = median(grades), sd = sd(grades)) ## mean median sd ## 1 56.8 60 17.00084 # Group grades by programme students_grades_prog %&gt;% group_by(programme) %&gt;% summarise(mean = mean(grades), median = median(grades), sd = sd(grades)) ## # A tibble: 3 x 4 ## programme mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 lang 54.6 60 14.6 ## 2 phil 56.6 58 14.0 ## 3 psych 59.2 65 24.3 7.8 Using filter() We can use a handy option filter() to do descriptives only for certain observations in the data. Let’s group descriptives by programme but only look at ‘psych’. # Check programme &#39;psych&#39; students_grades_prog %&gt;% filter(programme == &#39;psych&#39;) %&gt;% summarise(mean_psych = mean(grades), median_psych = median(grades), sd_psych = sd(grades)) ## mean_psych median_psych sd_psych ## 1 59.2 65 24.30432 Try with ‘lang’ and ‘phil’ too. # Check programme &#39;lang&#39; students_grades_prog %&gt;% filter(programme == &#39;lang&#39;) %&gt;% summarise(mean_lang = mean(grades), median_lang = median(grades), sd_lang = sd(grades)) ## mean_lang median_lang sd_lang ## 1 54.6 60 14.58767 # Check programme &#39;phil&#39; students_grades_prog %&gt;% filter(programme == &#39;phil&#39;) %&gt;% summarise(mean_phil = mean(grades), median_phil = median(grades), sd_phil = sd(grades)) ## mean_phil median_phil sd_phil ## 1 56.6 58 13.95708 Where necessary, we can also focus on studying only specific values in our data. For instance, imagine you just wanted to study students who have received grades of 50 and above in ‘psych’. # Check programme &#39;psych&#39;, grades &gt; 50 students_grades_prog %&gt;% filter(programme == &#39;phil&#39;, grades &gt; 50) %&gt;% summarise(mean_phil = mean(grades), median_phil = median(grades), sd_phil = sd(grades)) ## mean_phil median_phil sd_phil ## 1 66 68 7.211103 You will get much higher means now as you removed the grades which were lower. We can also use filter() to check just the for occurence of specific values. For example, what if we focused on only very high grades or very low grades in psych? # Check programme &#39;psych&#39; grades above 70 or below 40 students_grades_prog %&gt;% filter(programme == &#39;psych&#39;) %&gt;% filter(grades &gt; 70 | grades &lt; 40) # Take a note of how we use &#39;|&#39; to specify OR. ## X ID grades programme ## 1 1 ID_1 20 psych ## 2 4 ID_4 85 psych There are two extreme values in our data for our specification but let’s look at all the programmes together as well. # Group programmes and check for extreme values students_grades_prog %&gt;% group_by(programme) %&gt;% filter(grades &gt; 80 | grades &lt; 40) ## # A tibble: 3 x 4 ## # Groups: programme [2] ## X ID grades programme ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; ## 1 1 ID_1 20 psych ## 2 2 ID_2 35 lang ## 3 4 ID_4 85 psych 7.9 Visualisations Lastly, as usual, always visualise your data to gauge what the distribution looks like. # Visualise ggplot(data = students_grades_prog, aes(x = grades)) + geom_histogram(bins = 15, color = &#39;grey&#39;, fill = &#39;blue&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades&#39;) + theme_minimal() What about by programme? Try plotting with subsets. Bear in mind we don’t have that many observations. # Example for psych only ggplot(data = subset(students_grades_prog, programme %in% c(&#39;psych&#39;)), aes(x = grades)) + geom_histogram(bins = 20, color = &#39;grey&#39;, fill = &#39;blue&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades (Psych)&#39;) + theme_minimal() 7.10 Save the file in your folder Before finishing off, we can also write the merged dataset into our folder so it can be saved for the future. Check your folder after you run below. write.csv(students_grades_prog, &#39;student_grades_prog.csv&#39;) # Note how we first specify the object we want to save and then the name of the file including the extension &#39;.csv&#39;. 7.11 Practice.Rmd Solutions First, make sure that all the necessary packages are loaded: library(tidyverse) library(foreign) For your practice work with the same data as in the tutorial. You will need to use filter and arrange and also mutate to answer the questions below. You are also required to provide some simple visualisations for your data to show what is happening in student grades by programme. Here is the breakdown of the tasks we want you to do and the solutions: Read all three datasets in (data_students_1, data_students_2, data_students_3). Since they come in different formats make sure to check your notes from the tutorial. Note that you have data_students_3 in different formats so you can choose which one you want to read in. After you’ve read them in, check what’s inside of each dataset. # Dataset 1 data_students_1 &lt;- read.csv(&#39;data_students_1.csv&#39;) head(data_students_1) ## X ID grades ## 1 1 ID_1 20 ## 2 2 ID_2 35 ## 3 3 ID_3 45 ## 4 4 ID_4 85 ## 5 5 ID_5 70 ## 6 6 ID_6 72 # Dataset 2 data_students_2 &lt;- read.table(&quot;data_students_2.txt&quot;, header = TRUE) head(data_students_2) ## ID programme ## 1 ID_1 psych ## 2 ID_2 lang ## 3 ID_3 phil ## 4 ID_4 psych ## 5 ID_5 lang ## 6 ID_6 phil # Dataset 3 data_students_3 &lt;- read.dta(&#39;data_students_3.dta&#39;) head(data_students_3) ## ID grades programme ## 1 ID_16 40 psych ## 2 ID_17 38 lang ## 3 ID_18 50 phil ## 4 ID_19 80 psych ## 5 ID_20 69 lang ## 6 ID_21 70 phil # Or data_students_3 &lt;- read.csv(&#39;data_students_3.csv&#39;) head(data_students_3) ## X ID grades programme ## 1 1 ID_16 40 psych ## 2 2 ID_17 38 lang ## 3 3 ID_18 50 phil ## 4 4 ID_19 80 psych ## 5 5 ID_20 69 lang ## 6 6 ID_21 70 phil Note that we have IDs 16-30 which means that we have got data for an extra 15 students. We can now add these to the other dataset we have. Let us first merge grades and programme for data_students_1 and data_students_2. Merge datasets together. First merge data_students_1 and data_students_2, then merge the resulting data with data_students_3. Hint: you will need to use full_join(). # Dataset 1 + Dataset 2 students_grades_prog &lt;- full_join(data_students_1, data_students_2, by = c(&#39;ID&#39;)) # We can specify the unqiue variable we use to match the datasets via the &#39;by =&#39; argument. # + Dataset 3 students_grades_prog_all &lt;- full_join(students_grades_prog, data_students_3) # Please note that we do not need to specify a unique identifier here as we just want to match data by columns and R is clever enough to know what to do. head(students_grades_prog_all) ## X ID grades programme ## 1 1 ID_1 20 psych ## 2 2 ID_2 35 lang ## 3 3 ID_3 45 phil ## 4 4 ID_4 85 psych ## 5 5 ID_5 70 lang ## 6 6 ID_6 72 phil Work with the final dataset that has information on all students (30 observations). Provide means, medians and standard deviations for grades in each programme. students_grades_prog_all %&gt;% group_by(programme) %&gt;% summarise(mean = mean(grades), median = median(grades), sd = sd(grades)) ## # A tibble: 3 x 4 ## programme mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lang 54.7 61 13.8 ## 2 phil 56.9 58.5 12.0 ## 3 psych 61 65.5 19.1 Provide a simple visualisaiton for each programme. # Psych ggplot(data = subset(students_grades_prog_all, programme %in% c(&#39;psych&#39;)), aes(x = grades)) + geom_histogram(bins = 20, color = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades (Psych)&#39;) + theme_minimal() # Lang ggplot(data = subset(students_grades_prog_all, programme %in% c(&#39;lang&#39;)), aes(x = grades)) + geom_histogram(bins = 20,color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades (Lang)&#39;) + theme_minimal() # Phil ggplot(data = subset(students_grades_prog_all, programme %in% c(&#39;phil&#39;)), aes(x = grades)) + geom_histogram(bins = 20, color = &#39;grey&#39;, fill = &#39;coral&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades (Phil)&#39;) + theme_minimal() Now, try to answer the following questions: How many students in the dataset received the grades above 70? # Filter for grades above 70 students_grades_prog_all %&gt;% filter(grades &gt; 70) ## X ID grades programme ## 1 4 ID_4 85 psych ## 2 6 ID_6 72 phil ## 3 4 ID_19 80 psych The answer is three. What is the mean and the median grade for those who got more than 65? # Mean and median for grades above 70 students_grades_prog_all %&gt;% filter(grades &gt; 65) %&gt;% summarise(mean_above_65 = mean(grades), median_above_65 = median(grades)) ## mean_above_65 median_above_65 ## 1 72 70 The answer is 72 and 70. How many students received grades that were between 40 and 50 in philosophy programme? # Phil grades between 40 and 50 students_grades_prog_all %&gt;% filter(programme == &#39;phil&#39;) %&gt;% filter(grades &gt; 40 &amp; grades &lt; 50) # Note that we use &#39;&amp;&#39; to specify that we want grades both less than 50 and more than 40. ## X ID grades programme ## 1 3 ID_3 45 phil ## 2 15 ID_30 42 phil There are two students. Considering only philosophy programme, what were the top three grades in the cohort? # Only phil arranged students_grades_prog_all %&gt;% filter(programme == &#39;phil&#39;) %&gt;% arrange(desc(grades)) ## X ID grades programme ## 1 6 ID_6 72 phil ## 2 6 ID_21 70 phil ## 3 12 ID_12 68 phil ## 4 12 ID_27 65 phil ## 5 9 ID_24 59 phil ## 6 9 ID_9 58 phil ## 7 3 ID_18 50 phil ## 8 3 ID_3 45 phil ## 9 15 ID_30 42 phil ## 10 15 ID_15 40 phil The answer is 72, 70 and 68. Now, for language, what were the three lowest grades in the cohort? # Only lang arranged students_grades_prog_all %&gt;% filter(programme == &#39;lang&#39;) %&gt;% arrange(grades) ## X ID grades programme ## 1 2 ID_2 35 lang ## 2 2 ID_17 38 lang ## 3 11 ID_26 40 lang ## 4 11 ID_11 44 lang ## 5 8 ID_8 60 lang ## 6 8 ID_23 62 lang ## 7 14 ID_14 64 lang ## 8 14 ID_29 65 lang ## 9 5 ID_20 69 lang ## 10 5 ID_5 70 lang It should be 35, 38 and 40. Well done. It may have taken a while to build all of these code chunks but it is an essential part of the practice to keep playing with the code we are showing you. Try to arrange things differently and see what happens. "],
["week-8.html", "Chapter 8 Week 8 8.1 Introduction to Probability 8.2 Example 1 (hot hand) 8.3 Example 2 (coin toss) 8.4 Example 3 (Independent shooter) 8.5 Practice Rmd. Solutions 8.6 Pen and Paper Exercises", " Chapter 8 Week 8 Practice.Rmd here or on Learn. 8.1 Introduction to Probability The essential reading for this week can be found in Navarro’s Chapter 9 This week we will combine learning R with doing some basic probability calculations. We will toss a few coins and we will also try to work out what the probability of certain events happening is, given that we draw the data at random. This could be fun but might look tricky too. The goal is to build an intutition, not to overwhelm you with probability theory. There are few probability rules that you may want to refer to when working with basic probability examples for this week. The probability of an event lies in the interval [0,1] The probability of two events that are mutually exclusive (i.e. cannot happen at the same time) happening together can be found by adding their individual probabilities The probability of two independent events that are happening together can be found by muiltiplying their individual probabilities The sum of the probabilities of all outcomes must equal 1 The key functions we will need for today wil be sample(), tibble(), count(), table() and select(). Make sure to load tidyverse before your start. library(tidyverse) 8.2 Example 1 (hot hand) The example is adapted from the Open Intro Statistics textbook that, by the way, has an excellent overview of basic statistics and R. Check this out here. This example is based on the data of every shot taken by a basketball player, Kobe Bryant, pictured above. His performance against the Orlando Magic in the 2009 NBA Finals earned him quite a reputation - this lead to him being associated with a phenomenon know as ‘hot hand’. If we are keen to study this in more detail one approach would be to construct a simple distribution of his shots and study the data - we then can check that indeed Kobe does have a ‘hot hand’ meaning that he is more likely to score after a successful shot already occured. The data comes from the textbook website and we can easily load it by using the link below: load(url(&quot;http://www.openintro.org/stat/data/kobe.RData&quot;)) head(kobe) ## vs game quarter time description basket ## 1 ORL 1 1 9:47 Kobe Bryant makes 4-foot two point shot H ## 2 ORL 1 1 9:07 Kobe Bryant misses jumper M ## 3 ORL 1 1 8:11 Kobe Bryant misses 7-foot jumper M ## 4 ORL 1 1 7:41 Kobe Bryant makes 16-foot jumper (Derek Fisher assists) H ## 5 ORL 1 1 7:03 Kobe Bryant makes driving layup H ## 6 ORL 1 1 6:01 Kobe Bryant misses jumper M There are 133 observations of Kobe Bryant’s hits and misses. We will focus on the variable basket - it has values H for hit and M for miss. #Count hits/misses kobe %&gt;% count(basket) ## # A tibble: 2 x 2 ## basket n ## &lt;chr&gt; &lt;int&gt; ## 1 H 58 ## 2 M 75 The first sequence looks like this: \\[H M | M | H H M | M | M | M\\] The streak length is the number of consecutive HITs until a MISS e.g. \\(|H M |\\) represents a streak of length 1 whilst \\(|H H M |\\) represents a streak of length 2. A miss on its own will represent a streak of length 0. #Calculate streaks using the function that came with the data (calc=streak) kobe_streak&lt;-tibble(streak=calc_streak(pull(kobe,basket))) #note that we use pull() here to select data, variable #Count streaks kobe_streak %&gt;% count(streak) ## # A tibble: 5 x 2 ## streak n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 39 ## 2 1 24 ## 3 2 6 ## 4 3 6 ## 5 4 1 #Barplot ggplot(data=kobe_streak,aes(x=streak))+geom_bar(fill=&#39;lightblue&#39;) + labs(title=&#39;Distribution of streaks by Kobe Bryant&#39;) +theme_minimal() Now we can see easily what has been happening. How many hits and misses are in a streak of one? What about a streak length of 0? A streak of 0 is actually the most common. Then we have streak of length 1 - a hit (H) followed by a miss (M), almost as common as having |M M|. We have few occurences of streaks of l. Imagine that we actually had the probability assigned. If Kobe had 58 hits out of 133 then: \\[P(shot 1 = H)=0.45\\] If Kobe indeed had a ‘hot hand’ then the probability of getting a hit after a hit, two in a row, should be even higher than getting a single hit: \\[P(shot 2 = H|shot 1 = H)=0.60\\] But if he doesn’t really have a ‘hot hand’ then the probability could also be: \\[P(shot 2 = H|shot 1 = H)=0.45\\] That means that getting a hit then a miss or two hits in a row is totally random. Later, you will study the streaks of Kobe and then compare them to an independent shooter. We will show you an example below of how to generate a simulation of a random event and you can then try to construct your own independent shooter example. Let’s look at coin tossing as an example of a random process. 8.3 Example 2 (coin toss) We can easily generate a toin coss in R. Why coins? Traditionally, we use coin tossing in statistics because the flip of a coin describes events with an equal chance of happening, they are random, and random processes are a great playground for probability learning. #Lets assign some outcome variable outcomes&lt;-c(&quot;heads&quot;, &quot;tails&quot;) #We then will sample the outcomes 100 time with replacement (note: events are independent) sample(outcomes, size = 100, replace = TRUE) ## [1] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; ## [13] &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; ## [25] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; ## [37] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; ## [49] &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; ## [61] &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; ## [73] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; ## [85] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; ## [97] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; #We can also assign the simulation to an object and use count() to see the resulting distribution and the proportion outcomes&lt;-c(&quot;heads&quot;, &quot;tails&quot;) simulation_coin&lt;-tibble(coin_side = sample(outcomes, size = 100, replace = TRUE)) simulation_coin %&gt;% count(coin_side) ## # A tibble: 2 x 2 ## coin_side n ## &lt;chr&gt; &lt;int&gt; ## 1 heads 48 ## 2 tails 52 You can also do the same via table() and use the results to construct a propotions table: simulation_coin %&gt;% select(coin_side) %&gt;% #select the variable to provide counts table() %&gt;% #table how many values are in each category print() %&gt;% #output the results prop.table() # use the output to calculation the proportion ## . ## heads tails ## 48 52 ## . ## heads tails ## 0.48 0.52 Try to vary the size and see how table changes. You’ll note that as your sample size (N) approaches infinity (\\(N -&gt; +∞\\)) you get closer to the true probability of the event. # Try with the small simulation first simulation_coin &lt;- tibble(coin_side = sample(outcomes, size = 10, replace = TRUE)) simulation_coin %&gt;% count(coin_side) ## # A tibble: 2 x 2 ## coin_side n ## &lt;chr&gt; &lt;int&gt; ## 1 heads 7 ## 2 tails 3 # Or increase to a 1000 simulation_coin&lt;-tibble(coin_side=sample(outcomes, size = 1000, replace = TRUE)) simulation_coin %&gt;% count(coin_side) ## # A tibble: 2 x 2 ## coin_side n ## &lt;chr&gt; &lt;int&gt; ## 1 heads 509 ## 2 tails 491 What if you have an unfair coin? By default the previous simulation assumes that we have 50/50 chance to observe ‘heads’ or ‘tails’. You can assign different probability via simulations: # Simulate an unfair coin simulation_coin_unfair &lt;- tibble(coin_side = sample(outcomes, size = 100, replace = TRUE, prob = c(0.2, 0.8))) # Note that the first outcome has probability 0.2 and the second 0.8 simulation_coin_unfair %&gt;% count(coin_side) ## # A tibble: 2 x 2 ## coin_side n ## &lt;chr&gt; &lt;int&gt; ## 1 heads 13 ## 2 tails 87 Lastly, plot them both to compare: # Barplot (fair coin) ggplot(data = simulation_coin, aes(x = coin_side)) + geom_bar(fill = &#39;grey&#39;) + labs(x = &#39;Coin Side&#39;, title = &#39;Distribution of head/tails (fair coin&#39;) + theme_minimal() # Barplot ggplot(data = simulation_coin_unfair, aes(x = coin_side)) + geom_bar(fill = &#39;lightgreen&#39;) + labs(x = &#39;Coin Side&#39;, title = &#39;Distribution of head/tails (unfair coin)&#39;) + theme_minimal() 8.4 Example 3 (Independent shooter) We can now easily simulate a very similar draw for hits and misses we worked with earlier. We can create an independent shooter draw and compare it with the Kobe example. # Assign some outcome variable outcomes_h_m &lt;- c(&quot;H&quot;, &quot;M&quot;) # We then will sample the outcomes 100 times with replacement sample(outcomes_h_m, size = 133, replace = TRUE, prob = c(0.45, 0.55)) ## [1] &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; ## [26] &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ## [51] &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; ## [76] &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; ## [101] &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; ## [126] &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; #Lets save to a tibble #Or increase to a 1000 simulation_h_m&lt;-tibble(h_m=sample(outcomes_h_m, size = 1000, replace = TRUE)) simulation_h_m %&gt;% count(h_m) ## # A tibble: 2 x 2 ## h_m n ## &lt;chr&gt; &lt;int&gt; ## 1 H 483 ## 2 M 517 Also, a proportion: simulation_h_m %&gt;% select(h_m) %&gt;% table() %&gt;% print() %&gt;% prop.table() ## . ## H M ## 483 517 ## . ## H M ## 0.483 0.517 To provide a comparison, let’s plot the distribution of streaks: # Calculate streaks using the function that came with the data (calc = streak) indep_streak&lt;-tibble(h_m_ind=calc_streak(pull(simulation_h_m,h_m))) # Barplot ggplot(data = indep_streak, aes(x = h_m_ind)) + geom_bar(fill = &#39;lightblue&#39;) + labs(title = &#39;Distribution of streaks by an Independent Shooter&#39;) + theme_minimal() 8.5 Practice Rmd. Solutions 8.5.1 Sex of the babies born in the UK Work with the following example to study the probability of males and females being born in the UK. You can read more about the actual report [here] (https://www.gov.uk/government/statistics/sex-ratios-at-birth-in-great-britain-2013-to-2017) and check the methodologies on how people determine whether there is or isn’t evidence for sex selection at birth. There were 3.7 million births registered in Great Britain in this period with a ratio of males to females being 105.4. This also can be interpreted as the odds being 105.4 to 100 for males to females. \\[ \\frac{male}{female} = \\frac{105.4}{100} \\] To convert from odds to probability, divide the odds by one plus the odds. So to convert odds of 105.4/100 to a probability we can use the following: \\[ {\\frac{105.4}{100}}/({\\frac{105.4}{100} +{\\frac{100}{100}}}) = 0.51\\] From the information we were given we can then conclude that there might be a pretty random chance for baby male and baby female being registered and there is no evidence for sex selection at birth. Can you generate a sample of 100 babies that follow this distribution and then provide us with the plot? # Assign some outcome variable outcomes_babies &lt;- c(&quot;M&quot;, &quot;F&quot;) # We then will sample the outcomes 100 times with replacement sample(outcomes_babies, size = 100, replace = TRUE, prob = c(0.51, 0.49)) ## [1] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; ## [26] &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ## [51] &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; ## [76] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; # Or increase to 1000 and put into a tibble simulation_babies&lt;-tibble(baby=sample(outcomes_babies, size = 1000, replace = TRUE, prob = c(0.51,0.49))) simulation_babies%&gt;% count(baby) ## # A tibble: 2 x 2 ## baby n ## &lt;chr&gt; &lt;int&gt; ## 1 F 466 ## 2 M 534 To provide a comparison, let’s plot as well: # Barplot ggplot(data = simulation_babies, aes(x = baby)) + geom_bar(fill = &#39;grey&#39;) + labs(title = &#39;Distribution of sex at birth based on the UK estimates&#39;) + theme_minimal() What if there were differences? Assign the probabilities as 0.3 and 0.7 and see what happens: # Change the probabilities as if not random simulation_babies &lt;- tibble(baby = sample(outcomes_babies, size = 300, replace = TRUE, prob = c(0.30, 0.70))) simulation_babies%&gt;% select(baby) %&gt;% count(baby) ## # A tibble: 2 x 2 ## baby n ## &lt;chr&gt; &lt;int&gt; ## 1 F 209 ## 2 M 91 To provide a comparison, let’s plot as well: # Barplot ggplot(data = simulation_babies, aes(x = baby)) + geom_bar(fill = &#39;grey&#39;) + labs(title = &#39;Distribution sex at birth based on the UK estimates (if not random)&#39;) + theme_minimal() Try to play with it by resimulating your data and plot again. You will note that there is likely to be a switch between the two bars when we have a random event but a stronger tendency towards differences when the probability was pre-determined. 8.6 Pen and Paper Exercises Work with pen and paper on these ones. Try to calculate a few probabilities given the information below - go to your lab notes where necessary. 8.6.1 Boys and girls paradox (Mr. Jones and Mr. Smith) The Boy or Girl paradox, also known as The Two Child Problem, comes from a problem set up in the 1950s that asks two questions about the children of Mr. Jones and Mr. Smith. It was first featured by Martin Gardner in October 1959 in the “Mathematical Games column” in Scientific American. The paradox comes as follows: Mr. Jones has two children. The older child is a girl. What is the probability that both children are girls? Mr. Smith has two children. At least one of them is a boy. What is the probability that both children are boys? How would you calculate these given the information you have just been given? It is a bit tricker then one may think. The answer could easily be 50/50 but it can also vary given how much extra information we know. Whenever we are calculating probabilities for real world events we do need to make certain assumptions. If it is a random event, then: Each child is either male or female. Each child has the same chance of being male as being female (50/50). The sex of each child is independent of the sex of the other - there is no conditional effect. Given this, there are four possible options, and for Mr.Jones we can immediately eliminate the last two: Older Child = Girl, Younger Child = Boy Older Child = Girl, Younger Child = Girl Older Child = Boy, Younger Child = Boy Older Child = Boy, Younger Child = Girl But what about Mr.Smith? We can’t really know the combination so perhaps there could be three options, which will lead to the probability of the second child being a boy being \\(1/3\\) or \\(0.33\\) - can you tell which ones we would keep to calculate these? Older Child = Girl, Younger Child = Boy Older Child = Girl, Younger Child = Girl Older Child = Boy, Younger Child = Boy Older Child = Boy, Younger Child = Girl A very nice article that discussed the paradox was written by Peter Lynch in 2011, titled ’The Two-Child Paradox: Dichotomy and Ambiguity’: quite worth having a look! :) 8.6.2 Rugby captain A rugby team contains 6 Englishmen, 4 Welshmen, 8 Irishmen and 2 Scotsmen. If the captain is chosen at random, find the probability that he is: Scottish Welsh Irish or Scottish Not Scottish 8.6.3 Answers Calculate the size of the team first by adding all up: \\[ 6 + 4 + 8 + 2 = 20 \\] Then you will have: \\(Welsh = \\frac{4}{20}\\) \\(Scottish = \\frac{2}{20}= \\frac{1}{10}\\) \\(Irish | Scottish = (\\frac{8 + 2)}{20} = \\frac{10}{20} = \\frac{1}{2}\\) \\(Not Scottish = \\frac{(6 + 4 + 8)}{20} = \\frac{18}{20}\\) or also (total probability =1 , then \\(1 - \\frac{2}{20} = \\frac{20}{20} - \\frac{2}{20} = \\frac{18}{20}\\) 8.6.4 Defective A batch of ten electronic components contains three that are defective. If two components are selected from the batch, find the probability that: Both are defective Neither is defective At least one is defective 8.6.5 Answers Both are defective The probability that the first component we pick is defective is \\(\\frac{3}{10}\\). We set that defective component, once we determined it, aside and look at the box again. We now have \\(\\frac{2}{9}\\) probability of getting a defective one (since we picked a component, the total has gone down from 10 to 9, and since we picked a defective one, there are now only 2 defective components in the box). To combine probability we multiply: \\[(\\frac{3}{10})*(\\frac{2}{9}) = (\\frac{6}{90}) = \\frac{1}{15}\\] Neither is defective The same process, but we are now looking at the 7 out of 10 non-defective parts. Probability of picking the first working part: \\(\\frac{7}{10}\\). Which leaves \\(\\frac{6}{9}\\) working parts in the box. Then: \\[ (\\frac{7}{10})*(\\frac{6}{9})=(\\frac{7}{10})*(\\frac{2}{3})=(\\frac{14}{30})=\\frac{7}{15}\\] At least one is defective The easiest here is to profit from the rule where \\(P(E) + P(E&#39;) = 1\\) where: \\(P(E)\\) is the probability of event to occur \\(1 - P(E) = P(E&#39;)\\) is the probability of the event not to occur In words: 1 - Probability(No part defective) = Probability(Some part defective) So we can take the total probability of everything (1) and take away the probability that neither is defective. That leaves us with any case of having one or two defective parts: \\[1 - (\\frac{7}{15}) = (\\frac{15}{15}) - (\\frac{7}{15}) = \\frac{8}{15} \\] "],
["week-9.html", "Chapter 9 Week 9 9.1 Introduction to Probability Distributions and Revision 9.2 Discrete example (guessing homework answers) 9.3 Generate a homework attempt 9.4 Changing the probability (TRUE/FALSE) 9.5 Studying the distribution 9.6 Cumulative Probability (Advanced) 9.7 Revision Practice Rmd. Solutions 9.8 Extra Probability Practice", " Chapter 9 Week 9 An .Rmd with extra probability practice can be downloaded here or from Learn. A revision practice .Rmd can be downloaded here or from Learn. Data for this can be found here or on Learn. 9.1 Introduction to Probability Distributions and Revision The essential reading for this week can be found in Navarro’s Chapter 9 This week we will extend the probability content to probability distributions. We have already had a glance at coin flips last week. We will extend on the concept this week by talking more about sequences of the events. Last week we were working towards building your intuition about random and independent events. The beauty of the independent variable is that we can construct distributions of the values the variable can take (e.g. outcomes of the coin toss) which we can then study to evaluate the likelihood of certain events happening (e.g. Kobe and his ‘hot hand’). We will continue today by working with discrete distributions and the task is to sample from the outcomes, visualise, and attempt to determine the likelihood of an event from the distributions. 9.2 Discrete example (guessing homework answers) Imagine that you are working on your weekly homework quiz on Learn which has 10 multiple choice questions with 4 answers and you want to see what is the likelihood of getting the answer right if you randomly pick the answer each time with your eyes closed. Assume that there is only one correct option. We can study the probability of getting all the questions right or, say, of getting only half of questions right, by generating the distribution of the likely outcomes. If you pick an answer at random then the probability of getting it right should be \\(\\frac{1}{4}=0.25\\). Such would be true also for question two, assuming that your answer for question one is independent of question two. We can generate samples for each trial (n = 10 questions) and will check what is the probability of getting one, two, three, and so on questions out of 10 right. We will use the rbinom() function in R to generate the distribution of a number of attempts. We can work with it directly in R using the extensions: rbinom() to generate the distribution (‘r’ for random) for discrete outcomes dbinom() to study the probability of the outcome pbinom() to study the cumulative probability (also can be described as the area under the curve) The key arguments we will use are: x is a vector of numbers. p is a vector of probabilities. n is number of observations. size is the number of trials. prob is the probability of success of each trial. 9.3 Generate a homework attempt # We can generate an attempt for 10 questions rbinom(n = 10, # 10 attempts size = 10, # 10 questions) prob = 0.25) # Probability that one is correct (1/4) ## [1] 0 4 2 3 0 1 1 4 2 1 # We can generate an attempt for 100 questions/trials rbinom(n = 100, # 100 times of 10 size = 10, prob = 0.25) ## [1] 3 0 4 2 2 3 1 1 4 2 2 2 3 3 0 2 1 5 3 2 1 4 1 4 2 4 3 1 2 0 2 1 2 2 4 4 2 1 1 1 3 4 4 2 1 4 3 0 2 1 ## [51] 4 4 4 2 2 3 3 2 3 3 5 4 2 3 2 2 2 1 1 2 0 2 5 2 3 3 2 3 4 2 3 2 5 1 2 2 3 3 2 1 6 1 3 4 3 0 2 2 3 3 Let’s assign it to a tibble so we can calculate the proportions and make a plot. library(tidyverse) homework_guess &lt;- tibble(right_guess = rbinom(n = 100, size = 10, prob = 0.25)) homework_guess %&gt;% count(right_guess) ## # A tibble: 7 x 2 ## right_guess n ## &lt;int&gt; &lt;int&gt; ## 1 0 2 ## 2 1 22 ## 3 2 27 ## 4 3 26 ## 5 4 12 ## 6 5 10 ## 7 6 1 ggplot(data = homework_guess, aes(x = right_guess)) + geom_bar(fill = &#39;lightblue&#39;) + labs(x = &#39;Right Guess&#39;, y = &#39;Count&#39;) + theme_minimal() We can change the scale on the graph to see all the options: ggplot(data = homework_guess, aes(x = right_guess)) + geom_bar(fill = &#39;lightblue&#39;) + xlim(0,10) + #note how we add xlim() labs(x = &#39;Right Guess&#39;, y = &#39;Count&#39;) + theme_minimal() ## Warning: Removed 1 rows containing missing values (geom_bar). What’s more, we can also change y units to probability. ggplot(data = homework_guess, aes(x = right_guess)) + geom_bar(aes(y = (..count..)/sum(..count..)), # we will work with the function of count, hence we use&#39;..&#39; fill = &#39;lightblue&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;Probability&#39;) + theme_minimal() ## Warning: Removed 1 rows containing missing values (geom_bar). 9.4 Changing the probability (TRUE/FALSE) What if the multiple choice had only two choices for the answer (i.e. TRUE or FALSE questions) - the right answer will now have a probability of 0.5 instead. Let’s reflect on that and create a new tibble(): #Homework guess distribution with TRUE/FALSE homework_guess_true_false &lt;- tibble(right_guess = rbinom(n = 100, size = 10, prob = 0.5)) # Note that the probability of getting the right answer has gone up homework_guess_true_false %&gt;% count(right_guess) ## # A tibble: 9 x 2 ## right_guess n ## &lt;int&gt; &lt;int&gt; ## 1 1 1 ## 2 2 4 ## 3 3 13 ## 4 4 17 ## 5 5 21 ## 6 6 23 ## 7 7 16 ## 8 8 3 ## 9 9 2 Your chances are higher! Can you see why? ggplot(data = homework_guess_true_false, aes(x = right_guess)) + geom_bar(fill = &#39;lightgreen&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;Count&#39;) + theme_minimal() We have less options now, hence a higher chance of picking the correct ones. Let’s also plot with probabilities: ggplot(data = homework_guess_true_false, aes(x = right_guess)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = &#39;lightgreen&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;Probability&#39;) + theme_minimal() 9.5 Studying the distribution What’s great about learning probability disitributions is that we can use the distribution above to find exact probability of guessing different numbers of questions correctly. We will use dbinom(). Check what the probability is of getting at least one question right when guessing at random: dbinom(1, # Number of outcomes that we got right size=10, # 10 trials (we have 10 questions) prob=0.25) # Probability of getting the right answer (1/4) ## [1] 0.1877117 Or for two correct answers: dbinom(2, # Number of outcomes that we got right size = 10, # 10 trials (we have 10 questions) prob = 0.25) # Probability of getting the right answer (1/4) ## [1] 0.2815676 Or three: dbinom(3, # Number of outcomes that we got right size = 10, # 10 trials (we have 10 questions) prob = 0.25) # Probability of getting the right answer (1/4) ## [1] 0.2502823 And so on…. What if we want to find the probability of guessing all 10 questions correctly? That’s a magical result! dbinom(10, size = 10, prob = 0.25) ## [1] 9.536743e-07 #Round to 2 d.p. round(dbinom(10, size = 10, prob = 0.25), 2) ## [1] 0 So rare that it is almost zero, so never try to do the quiz just relying on your luck! :) 9.6 Cumulative Probability (Advanced) By adding the above we can also get a cumulative probability, meaning that we can study what would be the chance to get four or less right, meaning that you want to include the chances of getting zero, one, two, and three right as well. Graphically, we really want to analyse the probability mass here, given that all should sum up to one. To know the chances of getting one or two questions right we can sum the probabilities: dbinom(1, size = 10, prob = 0.25) + dbinom(1, size = 10, prob = 0.25) + dbinom(2, size = 10, prob = 0.25) ## [1] 0.656991 A faster way to see all the probabilities at once would be: all_probs &lt;- round(dbinom(x = c(0,1,2,3,4,5,6,7,8,9,10), prob = 0.25, size = 10),2) # Note that we use round to see the values to two decimal places all_probs ## [1] 0.06 0.19 0.28 0.25 0.15 0.06 0.02 0.00 0.00 0.00 0.00 You can then find out what the probability is of getting five or less answers right versus five or more answers right: # Five or less pbinom(q = 5, prob = 0.25, size = 10) ## [1] 0.9802723 Quite high chances :) # Five or more 1 - pbinom(q = 5, prob = 0.25, size = 10 ) ## [1] 0.01972771 Not that much! Can you see what we did? We found the probability of outcomes that are equal or less than five and then substracted it from total. Graphically we can show this as following: knitr::include_graphics(&#39;images/prob.jpg&#39;) There is an exercise for you to try this yourself with the distribution for TRUE/FALSE questions. Make sure that you revise this before coming back to the course in January as it will remind you of where we ended. 9.7 Revision Practice Rmd. Solutions This week’s practice is built around the key material we covered during the past eight weeks. You will need to load the data from Learn and then work with the key variables to provide descriptive statistics and visualisations. There is an extra practice at the end for you to work on the discrete probability distribution example as well. The dataset has information on participants that took part in the memory experiment: ’ IDs, Age, Memory score on three different tasks (Task A, Task B, and Placebo), and data on whether the participant received saw information/text twice. We are trying to explore whether treament (i.e. task) and seeing information twice may affect the memory scores. ID: 1 to 143 Age: 18-51 Memory Score: 1-100 (100 when remembered everything) Task: Task A, Task B, Placebo Saw_twice: Yes/No (if participant saw the text twice) # Load tidyverse library(tidyverse) # Read data in data &lt;- read.csv(&#39;week_9.csv&#39;) # Check what&#39;s inside head(data) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 54 Task A No ## 2 ID2 27 23 Task A Yes ## 3 ID3 25 32 Placebo Yes ## 4 ID4 25 38 Task B No ## 5 ID5 49 43 Task B Yes ## 6 ID6 47 32 Placebo No 9.7.1 Provide descriptive statistics for age and memory score variables There are different ways to do so using what we have learned so far: Look at each variable separately: # First age data %&gt;% summarise(mean = mean(Age), median = median(Age), sd = sd(Age)) ## mean median sd ## 1 34.39583 33.5 9.219331 # Then memory data %&gt;% summarise(mean = mean(Memory_score), median = median(Memory_score), sd = sd(Memory_score)) ## mean median sd ## 1 44.03472 41 20.54516 Or do it all in one go: data %&gt;% summarise(mean_age = mean(Age), mean_memory = mean(Memory_score), median_age = median(Age), median_memory = median (Memory_score), sd_age = sd(Age), sd_memory = sd(Memory_score)) ## mean_age mean_memory median_age median_memory sd_age sd_memory ## 1 34.39583 44.03472 33.5 41 9.219331 20.54516 9.7.2 Descriptives by groups Provide descriptives of the memory scores by task, and by whether someone saw the information on the task twice: # By treatment/task data %&gt;% group_by(Task) %&gt;% #note that we are adding group_by() to differentiate by a variable summarise(mean = mean(Memory_score), median = median(Memory_score), sd = sd(Memory_score)) ## # A tibble: 3 x 4 ## Task mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Placebo 43.2 41 21.9 ## 2 Task A 45.7 41.5 19.8 ## 3 Task B 43.4 42 20.2 # By whether someone saw information on the task twice data %&gt;% group_by(Saw_twice) %&gt;% summarise(mean = mean(Memory_score), median = median(Memory_score), sd = sd(Memory_score)) ## # A tibble: 2 x 4 ## Saw_twice mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 44.1 42.5 20.6 ## 2 Yes 44.0 41 20.6 9.7.3 Visualise Provide distributions of age and memory scores. # Age ggplot(data = data, aes(x = Age)) + geom_histogram(colour = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Age (Years)&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Age&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Memory Scores ggplot(data = data, aes(x = Memory_score)) + geom_histogram(colour = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Memory Score (1-100)&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Memory Scores&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 9.7.4 Visualise a subset What about memory scores only for Task A? # Memory Scores for Task A ggplot(data = subset(data, Task %in% c(&#39;Task A&#39;)), aes(x = Memory_score)) + geom_histogram(colour = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Memory Score (1-100)&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Memory Scores (Task A)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can also use %&gt;% to do the same. Check this out: # Memory Scores for Task A data %&gt;% filter (Task == &#39;Task A&#39;) %&gt;% ggplot(data = ., #note how we replace the data with `.` which will allow us to use the specification above as our input aes(x = Memory_score)) + geom_histogram(colour = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Memory Score (1-100)&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Memory Scores (Task A)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 9.7.5 Variable by a group What about a plot of Memory Score by Task? ggplot(data = data, aes(x = Task, y = Memory_score, fill = Task)) + geom_boxplot() Anything more advanced? Maybe you could try ggridges? library(ggridges) ggplot(data = data, aes(x = Memory_score, y = Task, fill=Task)) + geom_density_ridges() ## Picking joint bandwidth of 8.01 9.7.6 Create a new variable using mutate() Let’s change memory scores into percentages. How would you add an extra variable that converts scores (1-100) into percentages?? # We can use mutate (try first adding the new variable via piping) data %&gt;% mutate(Memory_score=Memory_score/100) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 0.54 Task A No ## 2 ID2 27 0.23 Task A Yes ## 3 ID3 25 0.32 Placebo Yes ## 4 ID4 25 0.38 Task B No ## 5 ID5 49 0.43 Task B Yes ## 6 ID6 47 0.32 Placebo No ## 7 ID7 28 0.51 Task A No ## 8 ID8 26 0.26 Placebo No ## 9 ID9 28 0.56 Placebo No ## 10 ID10 35 0.39 Task A No ## 11 ID11 48 0.32 Task B Yes ## 12 ID12 26 0.19 Placebo No ## 13 ID13 50 0.46 Task B No ## 14 ID14 22 0.26 Task B Yes ## 15 ID15 21 0.28 Task A No ## 16 ID16 44 0.48 Task A Yes ## 17 ID17 45 0.28 Task B Yes ## 18 ID18 40 0.76 Task A No ## 19 ID19 23 0.62 Placebo No ## 20 ID20 42 0.44 Placebo No ## 21 ID21 20 0.26 Task B Yes ## 22 ID22 34 0.30 Task A Yes ## 23 ID23 48 0.24 Task A No ## 24 ID24 48 0.82 Task B No ## 25 ID25 44 0.29 Task A Yes ## 26 ID26 44 0.47 Task B No ## 27 ID27 25 0.55 Task A No ## 28 ID28 48 0.56 Placebo No ## 29 ID29 31 0.30 Placebo No ## 30 ID30 22 0.44 Task B Yes ## 31 ID31 32 0.66 Task A Yes ## 32 ID32 22 0.12 Placebo No ## 33 ID33 29 1.24 Placebo Yes ## 34 ID34 33 0.59 Task A No ## 35 ID35 38 0.34 Task A No ## 36 ID36 31 0.81 Task A Yes ## 37 ID37 49 0.36 Task B Yes ## 38 ID38 25 0.08 Task B No ## 39 ID39 28 0.66 Task B No ## 40 ID40 28 0.62 Placebo No ## 41 ID41 42 0.13 Task A Yes ## 42 ID42 43 0.84 Task B No ## 43 ID43 47 0.60 Placebo Yes ## 44 ID44 28 0.05 Task B No ## 45 ID45 34 0.46 Task B Yes ## 46 ID46 33 0.72 Placebo No ## 47 ID47 24 0.41 Task A Yes ## 48 ID48 39 0.87 Placebo Yes ## 49 ID49 44 0.56 Task A No ## 50 ID50 49 0.22 Placebo No ## 51 ID51 40 0.94 Task B No ## 52 ID52 22 0.42 Task B Yes ## 53 ID53 32 0.33 Task A No ## 54 ID54 47 0.78 Task B Yes ## 55 ID55 37 0.49 Placebo No ## 56 ID56 51 0.19 Task A Yes ## 57 ID57 36 0.67 Task B No ## 58 ID58 22 0.28 Task B No ## 59 ID59 37 0.31 Task A Yes ## 60 ID60 42 0.38 Task B Yes ## 61 ID61 20 0.53 Task B Yes ## 62 ID62 25 0.29 Task B No ## 63 ID63 29 0.38 Task B Yes ## 64 ID64 44 0.53 Task B No ## 65 ID65 28 0.37 Task B No ## 66 ID66 19 0.77 Placebo No ## 67 ID67 42 0.46 Placebo Yes ## 68 ID68 32 0.14 Task B No ## 69 ID69 42 0.89 Placebo No ## 70 ID70 23 0.53 Placebo Yes ## 71 ID71 46 0.36 Task B Yes ## 72 ID72 35 0.55 Placebo Yes ## 73 ID73 23 0.18 Placebo Yes ## 74 ID74 25 0.31 Placebo No ## 75 ID75 38 0.41 Placebo Yes ## 76 ID76 33 0.42 Placebo Yes ## 77 ID77 24 0.34 Placebo No ## 78 ID78 47 0.80 Task A No ## 79 ID79 37 0.20 Task A Yes ## 80 ID80 40 0.25 Task B Yes ## 81 ID81 19 0.75 Task B Yes ## 82 ID82 39 0.42 Task A No ## 83 ID83 24 0.31 Task B Yes ## 84 ID84 27 0.53 Task B No ## 85 ID85 31 0.35 Task B No ## 86 ID86 41 0.28 Task B No ## 87 ID87 29 0.75 Task A Yes ## 88 ID88 43 0.51 Task B No ## 89 ID89 42 0.24 Task A No ## 90 ID90 31 0.31 Task A No ## 91 ID91 22 0.32 Placebo No ## 92 ID92 47 0.43 Placebo No ## 93 ID93 24 0.28 Placebo Yes ## 94 ID94 33 0.39 Task A Yes ## 95 ID95 47 0.34 Task A No ## 96 ID96 33 0.81 Task A No ## 97 ID97 28 0.41 Task B Yes ## 98 ID98 36 0.15 Task B No ## 99 ID99 44 0.71 Task A No ## 100 ID100 40 0.34 Task A Yes ## 101 ID101 21 0.20 Task A No ## 102 ID102 21 0.54 Task A No ## 103 ID103 40 0.67 Task B No ## 104 ID104 38 0.26 Placebo No ## 105 ID105 28 0.57 Placebo No ## 106 ID106 41 0.31 Placebo No ## 107 ID107 23 0.14 Task B No ## 108 ID108 50 0.65 Task A Yes ## 109 ID109 38 0.54 Task A No ## 110 ID110 40 0.34 Placebo No ## 111 ID111 42 0.50 Task B Yes ## 112 ID112 34 0.54 Task B No ## 113 ID113 32 0.22 Placebo Yes ## 114 ID114 46 0.82 Task A No ## 115 ID115 35 0.58 Task B Yes ## 116 ID116 31 0.30 Task B No ## 117 ID117 39 0.63 Task A No ## 118 ID118 29 0.55 Placebo No ## 119 ID119 20 0.26 Placebo Yes ## 120 ID120 26 0.31 Task A Yes ## 121 ID121 22 0.25 Task B Yes ## 122 ID122 50 0.37 Placebo No ## 123 ID123 34 0.52 Placebo No ## 124 ID124 44 0.46 Task B No ## 125 ID125 45 0.33 Task A No ## 126 ID126 22 0.33 Placebo Yes ## 127 ID127 33 0.37 Placebo No ## 128 ID128 43 0.27 Placebo No ## 129 ID129 26 0.45 Placebo No ## 130 ID130 47 0.59 Task B No ## 131 ID131 32 0.19 Task B No ## 132 ID132 42 0.53 Task A Yes ## 133 ID133 44 0.25 Task A No ## 134 ID134 42 0.03 Placebo No ## 135 ID135 28 0.70 Task B Yes ## 136 ID136 33 0.43 Task B No ## 137 ID137 20 0.19 Placebo No ## 138 ID138 46 0.29 Placebo No ## 139 ID139 41 0.45 Task A Yes ## 140 ID140 23 0.46 Placebo Yes ## 141 ID141 45 0.61 Placebo No ## 142 ID142 25 0.62 Task B No ## 143 ID143 28 0.41 Placebo No ## 144 ID144 32 0.65 Task A Yes We can see a new variable above but if you head your data it won’t appear: head(data) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 54 Task A No ## 2 ID2 27 23 Task A Yes ## 3 ID3 25 32 Placebo Yes ## 4 ID4 25 38 Task B No ## 5 ID5 49 43 Task B Yes ## 6 ID6 47 32 Placebo No That’s because we have not assigned it to a dataset. To do so, we will need to use &lt;- in the following way: new_data &lt;- data %&gt;% mutate(Memory_score=Memory_score/100) Check now: head(new_data) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 0.54 Task A No ## 2 ID2 27 0.23 Task A Yes ## 3 ID3 25 0.32 Placebo Yes ## 4 ID4 25 0.38 Task B No ## 5 ID5 49 0.43 Task B Yes ## 6 ID6 47 0.32 Placebo No 9.7.7 Subset observations using filter() Can you subset only Task A and Placebo from the data? We can use filter and then assign the filtered observations to an object too: data %&gt;% filter(Task == &#39;Task A&#39; | Task== &#39;Placebo&#39;) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 54 Task A No ## 2 ID2 27 23 Task A Yes ## 3 ID3 25 32 Placebo Yes ## 4 ID6 47 32 Placebo No ## 5 ID7 28 51 Task A No ## 6 ID8 26 26 Placebo No ## 7 ID9 28 56 Placebo No ## 8 ID10 35 39 Task A No ## 9 ID12 26 19 Placebo No ## 10 ID15 21 28 Task A No ## 11 ID16 44 48 Task A Yes ## 12 ID18 40 76 Task A No ## 13 ID19 23 62 Placebo No ## 14 ID20 42 44 Placebo No ## 15 ID22 34 30 Task A Yes ## 16 ID23 48 24 Task A No ## 17 ID25 44 29 Task A Yes ## 18 ID27 25 55 Task A No ## 19 ID28 48 56 Placebo No ## 20 ID29 31 30 Placebo No ## 21 ID31 32 66 Task A Yes ## 22 ID32 22 12 Placebo No ## 23 ID33 29 124 Placebo Yes ## 24 ID34 33 59 Task A No ## 25 ID35 38 34 Task A No ## 26 ID36 31 81 Task A Yes ## 27 ID40 28 62 Placebo No ## 28 ID41 42 13 Task A Yes ## 29 ID43 47 60 Placebo Yes ## 30 ID46 33 72 Placebo No ## 31 ID47 24 41 Task A Yes ## 32 ID48 39 87 Placebo Yes ## 33 ID49 44 56 Task A No ## 34 ID50 49 22 Placebo No ## 35 ID53 32 33 Task A No ## 36 ID55 37 49 Placebo No ## 37 ID56 51 19 Task A Yes ## 38 ID59 37 31 Task A Yes ## 39 ID66 19 77 Placebo No ## 40 ID67 42 46 Placebo Yes ## 41 ID69 42 89 Placebo No ## 42 ID70 23 53 Placebo Yes ## 43 ID72 35 55 Placebo Yes ## 44 ID73 23 18 Placebo Yes ## 45 ID74 25 31 Placebo No ## 46 ID75 38 41 Placebo Yes ## 47 ID76 33 42 Placebo Yes ## 48 ID77 24 34 Placebo No ## 49 ID78 47 80 Task A No ## 50 ID79 37 20 Task A Yes ## 51 ID82 39 42 Task A No ## 52 ID87 29 75 Task A Yes ## 53 ID89 42 24 Task A No ## 54 ID90 31 31 Task A No ## 55 ID91 22 32 Placebo No ## 56 ID92 47 43 Placebo No ## 57 ID93 24 28 Placebo Yes ## 58 ID94 33 39 Task A Yes ## 59 ID95 47 34 Task A No ## 60 ID96 33 81 Task A No ## 61 ID99 44 71 Task A No ## 62 ID100 40 34 Task A Yes ## 63 ID101 21 20 Task A No ## 64 ID102 21 54 Task A No ## 65 ID104 38 26 Placebo No ## 66 ID105 28 57 Placebo No ## 67 ID106 41 31 Placebo No ## 68 ID108 50 65 Task A Yes ## 69 ID109 38 54 Task A No ## 70 ID110 40 34 Placebo No ## 71 ID113 32 22 Placebo Yes ## 72 ID114 46 82 Task A No ## 73 ID117 39 63 Task A No ## 74 ID118 29 55 Placebo No ## 75 ID119 20 26 Placebo Yes ## 76 ID120 26 31 Task A Yes ## 77 ID122 50 37 Placebo No ## 78 ID123 34 52 Placebo No ## 79 ID125 45 33 Task A No ## 80 ID126 22 33 Placebo Yes ## 81 ID127 33 37 Placebo No ## 82 ID128 43 27 Placebo No ## 83 ID129 26 45 Placebo No ## 84 ID132 42 53 Task A Yes ## 85 ID133 44 25 Task A No ## 86 ID134 42 3 Placebo No ## 87 ID137 20 19 Placebo No ## 88 ID138 46 29 Placebo No ## 89 ID139 41 45 Task A Yes ## 90 ID140 23 46 Placebo Yes ## 91 ID141 45 61 Placebo No ## 92 ID143 28 41 Placebo No ## 93 ID144 32 65 Task A Yes Now, put it inside a new dataset, called reduced, you can also specify which variables you may want to keep. reduced_data &lt;- data %&gt;% filter(Task == &#39;Task A&#39; | Task== &#39;Placebo&#39;) %&gt;% select(ID, Age, Memory_score, Task, Saw_twice) 9.7.8 Sort via arrange() We can check the lowest and highest memory scores via sorting in each group: # Task A (lowest) data %&gt;% filter(Task == &quot;Task A&quot;) %&gt;% arrange(Memory_score) ## ID Age Memory_score Task Saw_twice ## 1 ID41 42 13 Task A Yes ## 2 ID56 51 19 Task A Yes ## 3 ID79 37 20 Task A Yes ## 4 ID101 21 20 Task A No ## 5 ID2 27 23 Task A Yes ## 6 ID23 48 24 Task A No ## 7 ID89 42 24 Task A No ## 8 ID133 44 25 Task A No ## 9 ID15 21 28 Task A No ## 10 ID25 44 29 Task A Yes ## 11 ID22 34 30 Task A Yes ## 12 ID59 37 31 Task A Yes ## 13 ID90 31 31 Task A No ## 14 ID120 26 31 Task A Yes ## 15 ID53 32 33 Task A No ## 16 ID125 45 33 Task A No ## 17 ID35 38 34 Task A No ## 18 ID95 47 34 Task A No ## 19 ID100 40 34 Task A Yes ## 20 ID10 35 39 Task A No ## 21 ID94 33 39 Task A Yes ## 22 ID47 24 41 Task A Yes ## 23 ID82 39 42 Task A No ## 24 ID139 41 45 Task A Yes ## 25 ID16 44 48 Task A Yes ## 26 ID7 28 51 Task A No ## 27 ID132 42 53 Task A Yes ## 28 ID1 21 54 Task A No ## 29 ID102 21 54 Task A No ## 30 ID109 38 54 Task A No ## 31 ID27 25 55 Task A No ## 32 ID49 44 56 Task A No ## 33 ID34 33 59 Task A No ## 34 ID117 39 63 Task A No ## 35 ID108 50 65 Task A Yes ## 36 ID144 32 65 Task A Yes ## 37 ID31 32 66 Task A Yes ## 38 ID99 44 71 Task A No ## 39 ID87 29 75 Task A Yes ## 40 ID18 40 76 Task A No ## 41 ID78 47 80 Task A No ## 42 ID36 31 81 Task A Yes ## 43 ID96 33 81 Task A No ## 44 ID114 46 82 Task A No What about the highest in Task B? # Task B (the highest) data %&gt;% filter(Task == &quot;Task B&quot;) %&gt;% arrange(desc(Memory_score)) ## ID Age Memory_score Task Saw_twice ## 1 ID51 40 94 Task B No ## 2 ID42 43 84 Task B No ## 3 ID24 48 82 Task B No ## 4 ID54 47 78 Task B Yes ## 5 ID81 19 75 Task B Yes ## 6 ID135 28 70 Task B Yes ## 7 ID57 36 67 Task B No ## 8 ID103 40 67 Task B No ## 9 ID39 28 66 Task B No ## 10 ID142 25 62 Task B No ## 11 ID130 47 59 Task B No ## 12 ID115 35 58 Task B Yes ## 13 ID112 34 54 Task B No ## 14 ID61 20 53 Task B Yes ## 15 ID64 44 53 Task B No ## 16 ID84 27 53 Task B No ## 17 ID88 43 51 Task B No ## 18 ID111 42 50 Task B Yes ## 19 ID26 44 47 Task B No ## 20 ID13 50 46 Task B No ## 21 ID45 34 46 Task B Yes ## 22 ID124 44 46 Task B No ## 23 ID30 22 44 Task B Yes ## 24 ID5 49 43 Task B Yes ## 25 ID136 33 43 Task B No ## 26 ID52 22 42 Task B Yes ## 27 ID97 28 41 Task B Yes ## 28 ID4 25 38 Task B No ## 29 ID60 42 38 Task B Yes ## 30 ID63 29 38 Task B Yes ## 31 ID65 28 37 Task B No ## 32 ID37 49 36 Task B Yes ## 33 ID71 46 36 Task B Yes ## 34 ID85 31 35 Task B No ## 35 ID11 48 32 Task B Yes ## 36 ID83 24 31 Task B Yes ## 37 ID116 31 30 Task B No ## 38 ID62 25 29 Task B No ## 39 ID17 45 28 Task B Yes ## 40 ID58 22 28 Task B No ## 41 ID86 41 28 Task B No ## 42 ID14 22 26 Task B Yes ## 43 ID21 20 26 Task B Yes ## 44 ID80 40 25 Task B Yes ## 45 ID121 22 25 Task B Yes ## 46 ID131 32 19 Task B No ## 47 ID98 36 15 Task B No ## 48 ID68 32 14 Task B No ## 49 ID107 23 14 Task B No ## 50 ID38 25 8 Task B No ## 51 ID44 28 5 Task B No 9.7.9 Let’s do some specific count() using filter() First check how many people we have in each Task group: data %&gt;% count(Task) ## # A tibble: 3 x 2 ## Task n ## &lt;fct&gt; &lt;int&gt; ## 1 Placebo 49 ## 2 Task A 44 ## 3 Task B 51 Can you show how many people above 40 years of age and saw the information on the task twice in each Task group? data %&gt;% filter(Age &gt;40) %&gt;% filter(Saw_twice == &#39;Yes&#39;) %&gt;% count(Task) ## # A tibble: 3 x 2 ## Task n ## &lt;fct&gt; &lt;int&gt; ## 1 Placebo 2 ## 2 Task A 7 ## 3 Task B 8 For the last one, show how people with the highest memory scores are split by task. Use a memory score threshold of 50 out of 100: data %&gt;% filter(Memory_score &gt; 50) %&gt;% count(Task) ## # A tibble: 3 x 2 ## Task n ## &lt;fct&gt; &lt;int&gt; ## 1 Placebo 16 ## 2 Task A 19 ## 3 Task B 17 Task A has greater queanity of high memory scores. Nicely done! If you got to the end, you have now succefully practiced all the key code and functions we have learnt in previous weeks. Play more if you like for the practice. 9.8 Extra Probability Practice Work with the distribution we created in the tutorial for guessing on homework quizzes. We want to analyse how likely you are to get specific numbers of question right. 9.8.1 TRUE/FALSE questions Work with the TRUE/FALSE example we have seen in the tutorial. What if the multiple choice had only two choices for the answer (i.e. TRUE or FALSE questions)? The right answer will now have a probability of 0.5 if you were to guess at random. Create a tibble() to show this: homework_guess_true_false &lt;- tibble(right_guess = rbinom(n = 100, size = 10, prob = 0.5)) 9.8.2 Count the occurencies homework_guess_true_false %&gt;% count(right_guess) ## # A tibble: 8 x 2 ## right_guess n ## &lt;int&gt; &lt;int&gt; ## 1 1 1 ## 2 2 4 ## 3 3 6 ## 4 4 23 ## 5 5 27 ## 6 6 23 ## 7 7 11 ## 8 8 5 9.8.3 Plot ggplot(data = homework_guess_true_false, aes(x = right_guess)) + geom_bar(fill = &#39;lightgreen&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;count&#39;) + theme_minimal() Plot with y being a probability: ggplot(data = homework_guess_true_false, aes(x = right_guess)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = &#39;lightgreen&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;Probability&#39;) + theme_minimal() 9.8.4 Use dbinom() to study the probability Check what the probability is of getting only one question right when guessing at random: dbinom(1, size = 10, prob = 0.5) ## [1] 0.009765625 Or for five correct answers: dbinom(5, size = 10, prob = 0.5) ## [1] 0.2460938 Or eight: dbinom(8, size = 10, prob = 0.5) ## [1] 0.04394531 Or put it all together at once (make sure that the probability is 0.5): all_probs &lt;- round(dbinom(x = c(0,1,2,3,4,5,6,7,8,9,10), prob = 0.5, size = 10),2) # Note that we use round to see the values to two decimal places all_probs ## [1] 0.00 0.01 0.04 0.12 0.21 0.25 0.21 0.12 0.04 0.01 0.00 9.8.5 Less than five or more than five? We can also study what the chances are of getting less than five questions right versus more than five questions right in a TRUE/FALSE setting (check your notes online). # Five or less pbinom(q = 5, prob = 0.5, size = 10) ## [1] 0.6230469 # Five or more 1 - pbinom(q = 5, prob = 0.5, size = 10) ## [1] 0.3769531 Better chances compared to when you are doing a quiz with four options! :) "],
["week-10-lab-test.html", "Chapter 10 Week 10: Lab test", " Chapter 10 Week 10: Lab test This week there is a Lab test. "],
["chap-sampling-distributions.html", "Chapter 11 Sampling distributions 11.1 Recap 11.2 Population vs sample 11.3 Population parameter vs sample statistic 11.4 Sampling distributions 11.5 The standard error of a statistic 11.6 The effect of sample size on the sampling distribution 11.7 Take-home message 11.8 Lab: Hollywood movies 11.9 Summary 11.10 Glossary 11.11 References", " Chapter 11 Sampling distributions Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand the difference between a population parameter and a sample statistic. LO2. Understand that a sampling distribution shows how sample statistics vary from sample to sample. LO3. Understand the effect of sample size on the sampling distribution, and how to quantify the variability of a statistic. Reading This week’s reading is Chapter 7 of the book by Chester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman and Hall/CRC, 2019. Freely available online at: https://moderndive.com/ 11.1 Recap Figure 11.1: The data analysis pipeline. In Semester 1 (Weeks 1-10), we started walking through the required steps of a statistical investigation, conveniently summarized in Figure 11.1. Suppose that you are interested in a research question that can be answered by collecting data on some statistical units. Once collected, you (a) import/load the data into R; (b) tidy them so that each column corresponds to a single variable of interest; (c) transform the variables if needed; (d) visualise your data and inspect for unusual values; (e) fit statistical models to the data; and (f) communicate your results and conclusions to the wider community. Note: The inner cycle (c-d-e) might need to be re-iterated a few times. In the first semester we saw: how to import data into R; how to tidy datasets (for example by making sure that some variables are factors); how to transform variables (e.g. standardizing via scale(), log-transforming data, …); how to visualise the data using the ggplot2 package. In this semester we will focus on modelling, and communicating our findings to the wider community. 11.2 Population vs sample Typically, it is either infeasible in terms of time or cost to perform an exhaustive data collection on the entire population of interest (also known as census). This is why, for instance, the UK Office for National Statistics only performs a census every 10 years and the next one will be in 2021. To save time and money, we typically record the variables of interest on a smaller subset of the entire population, also known as a sample. In order to make sure that any conclusions we draw from the sample are generalisable to the wider population, this sample needs to be taken at random. This avoids representation bias, where some units are less represented than others, which would lead to wrong conclusions for the entire population. ► Example Average montly salary in Sweden Suppose you are interested in the average monthly salary of people working in Sweden. Unfortunately, you neither have the time nor the money to go to Sweden and ask each single person their own salary. Hence, you decide to ask some people at random. What are the problems of the following sample selection criteria? Asking 500 random people from Facebook that live in Sweden. Asking 1000 random people from the web that live in Sweden. Calling 200 phone numbers from the phone book. Asking 500 people working near the central bank of Sweden. ► Solution All four criteria lead to samples that are not representative of the entire population. This is because the population units will not have the same probability of being included in the sample: some people will have a higher chance of being included in the sample than others. This leads to sampling bias. Because of this, any conclusion we might make from the sample is not generalisable to the whole population. People that do not have Facebook are not considered in the sample. People that do not use the Internet are not represented. People without a landline phone are not included. People working in this area will very likely have higher salaries than the rest of the country. This sample does not represent fairly people working in farming or other sectors. In Week 8 of Semester 1, we discussed how to obtain sample data from a bigger population of interest; this is known as data collection. We now take the opposite direction as we try to use the information from the sample data to draw conclusions about the entire population. This is summarised in 11.2. Furthermore, we will spend some time assessing how accurate our conclusions about the entire population are. Statistical inference Statistical inference is the process of using the sample data to draw conclusions about the entire population. Figure 11.2: Data collection vs statistical inference Caution: The inferential process assumes that the sample is randomly drawn from the population of interest. Any sample selection method that is biased, i.e. leading to samples which are not representative of the entire population, will mean that the results we obtain from the data in the sample can not be generalised to the entire population. In the average salary of Sweden example, none of the four sample selection strategies consisted of random sampling. They systematically excluded some people from the samples, leading to samples that were not representative of the population. For this reason, the average salary computed on any sample could not have been used to draw conclusions about the average salary in Sweden. 11.3 Population parameter vs sample statistic To clarify whether we are referring to the entire population or a sample, we use the term parameter when referring to a numerical summary of the entire population, and the term statistic for a numerical summary of the sample. Parameter vs statistic A parameter is a number describing some aspect of the population. A statistic is a number that is computed from the data in a sample. ► Example Average price of goods sold by ACME Corporation Suppose you work for a company that is interested in buying ACME Corporation3 and your boss wants to know within the next hour what is the average price of goods sold by that company and how the prices of the goods they sell differ from each other. Since ACME Corporation has such a big mail order catalogue, see Figure 11.3, we will assume that the company sells many products. Furthermore, we only have the catalogue in paper-form and no online list of prices is available. Figure 11.3: Product catalogue of ACME corporation. Identify the population of interest and the population parameters. Can we compute the parameters within the next hour? How would you proceed in estimating the population parameters if you just had time to read through 100 item descriptions? Would you pick the first 100 items or would you pick 100 random page numbers? State which statistics you would use to estimate the population parameters. ► Solution The population of interest is all products sold by ACME Corporation. The population parameters are the mean price \\(\\mu\\) and the standard deviation \\(\\sigma\\). Because the catalogue has so many pages, we can not compute the population parameters within the next hour. We must estimate the population mean and standard deviation from a sample of size \\(n = 100\\). We should choose the items entering the sample at random, to avoid sampling bias. If we were to choose 100 consecutive items, we might end up with a very good estimate of the average price for the category those contecutive items belong to (e.g. gardening). However, this would not be a good estimate of the overall price across the multiple categories of products sold. Our best guess of the population mean would be the sample mean, \\(\\bar{x}\\), and our best guess of the population standard deviation would be the sample standard deviation, denoted \\(s\\) or \\(\\hat{\\sigma}\\). From this example, you can see that the population parameter and the sample statistic generally have the same name. However, these are often written with different symbols to convey with just one letter: what feature they represent; if it is a population quantity or a quantity computed on a sample. The following table summarizes standard notation for some population parameters, typically unknown, and the corresponding “best guesses” computed on a sample. Notation for common parameters and statistics. Population parameter Sample statistic Mean \\(\\mu\\) \\(\\bar{x}\\) or \\(\\hat{\\mu}\\) Standard deviation \\(\\sigma\\) \\(s\\) or \\(\\hat{\\sigma}\\) Proportion \\(p\\) \\(\\hat{p}\\) The greek letter \\(\\mu\\) (mu) is used as a parameter to denote the population mean/average, while \\(\\bar{x}\\) (x-bar) or \\(\\hat{\\mu}\\) (mu-hat) is used as a statistic for the mean computed on a sample. The greek letter \\(\\sigma\\) (sigma) is used as a parameter to denote the population standard deviation, while \\(s\\) or \\(\\hat{\\sigma}\\) (sigma-hat) is used as a statistic for the standard deviation of the collected sample. The letter \\(p\\) is used as a parameter to denote the population proportion, while \\(\\hat{p}\\) (p-hat) is used as a statistic for the sample proportion. ► Example Proportion of UK people aged between 25 and 34 with a Bachelor’s degree or higher The last UK Census, done in 2011, reports that 40% of people aged 25 to 34 years had a degree-level or above qualification. Suppose that in a random sample of \\(n = 200\\) UK residents who are between 25 and 34 years old, 58 of them have a Bachelor’s degree or higher. Using the appropriate notation, state what is the population parameter and what is the sample statistic. ► Solution The population parameter is the proportion of all UK people aged between 25 and 34 years old with a Bachelor’s degree or higher: \\(p = 0.4\\). The sample statistic is the proportion with a Bachelor’s degree or higher for those in the sample: \\(\\hat{p} = 58/200 = 0.29\\). As discussed, it is generally infeasible to be able to know the value of the population parameter exactly. This would require collecting data for the entire population and then computing the required quantity. Instead, we typically select a random sample from the population, and then compute the quantity of interest for the sample data. We then use this sample statistic as a (point) estimate or best guess of the population parameter. 11.4 Sampling distributions A population parameter is typically considered to be a fixed value. A sample statistic, however, varies from sample to sample. This is because a sample statistic, i.e. a quantity computed on a sample, will depend on which units are selected to enter the sample. This can be seen as a downside to sampling, but we must remember that time and resources often prevent us from finding out the true value of a population parameter, whereas it is comparatively easy to collect a sample and compute a statistic. A fundamental question that arises when we estimate an unknown population parameter by a sample statistic is: how accurate do we believe our best guess to be? Remembering that the parameter is fixed, while the statistics varies from sample to sample, we might proceed in answering this question by looking at how the computed statistic varies depending on the sample. ► Example Average yearly salary of American National Football League (NFL) players We will read a file containing the yearly salaries (in millions of dollars) for all players being paid at the start of 2015 by a National Football League (NFL) team. This entire dataset represents the population of all National Football League players in 2015.4 We are interested in the following question: what was the average yearly salary of a NFL player in 2015? In this particular example, we actually do know the population parameter, because we have data on the whole population. We resort to sampling, however, to show how the value of a statistic varies when computed on different samples. Read in the data and state, with appropriate notation, what is the population parameter. Select a random sample of \\(n = 50\\) players and compute the average yearly salary for the players in the sample. How does your statistic compare to the population parameter? Take another sample of size \\(n = 50\\) players and compute the average salary for the new sample. How does it compare with the mean from the previous sample? ► Solution Do not worry if some of these functions are new to you, try to follow along and understand what each does. Question 1: Read in the data and state, with appropriate notation, what is the population parameter. Let us start by loading the data, inspecting the initial rows of the tibble, and checking the dimensions: library(tidyverse) nfl &lt;- read_tsv(&#39;https://edin.ac/2TexAFA&#39;) head(nfl) ## # A tibble: 6 x 5 ## Player Position Team TotalMoney YearlySalary ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aaron Rodgers QB Packers 110 22 ## 2 Russell Wilson QB Seahawks 87.6 21.9 ## 3 Ben Roethlisberger QB Steelers 87.4 21.8 ## 4 Philip Rivers QB Chargers 83.2 20.8 ## 5 Cam Newton QB Panthers 104. 20.8 ## 6 Matt Ryan QB Falcons 104. 20.8 dim(nfl) ## [1] 2099 5 The data comprise five measurements on 2099 units. We have data on the players’ names, positions, team names, total money they received while playing on the NFL (cumulative over multiple years), and their yearly salary. Since we are only interested in the yearly salary of the players, we can select only the relevant columns from the data: nfl &lt;- nfl %&gt;% select(Player, YearlySalary) head(nfl) ## # A tibble: 6 x 2 ## Player YearlySalary ## &lt;chr&gt; &lt;dbl&gt; ## 1 Aaron Rodgers 22 ## 2 Russell Wilson 21.9 ## 3 Ben Roethlisberger 21.8 ## 4 Philip Rivers 20.8 ## 5 Cam Newton 20.8 ## 6 Matt Ryan 20.8 We now calculate the average yearly salary for all players: nfl_mean &lt;- nfl %&gt;% summarise(avg = mean(YearlySalary)) nfl_mean ## # A tibble: 1 x 1 ## avg ## &lt;dbl&gt; ## 1 2.24 The population parameter is the average yearly salary in 2015, \\(\\mu =\\) 2.24. Question 2: Select a random sample of \\(n = 50\\) players and compute the average yearly salary for the players in the sample. How does your statistic compare to the population parameter? In order to randomly sample players, we will use the package moderndive. If you do not have it installed, run the following command: install.packages(&quot;moderndive&quot;) We then load the package and create a sample of size \\(n = 50\\) players: library(moderndive) nfl_sample_1 &lt;- nfl %&gt;% rep_sample_n(size = 50) nfl_sample_1 ## # A tibble: 50 x 3 ## # Groups: replicate [1] ## replicate Player YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Craig Dahl 0.87 ## 2 1 Jarrett Bush 1.75 ## 3 1 Shayne Graham 0.98 ## 4 1 Christian Covington 0.593 ## 5 1 Pat McAfee 2.9 ## 6 1 Ron Parker 5 ## 7 1 Bryan Walters 0.723 ## 8 1 John Greco 2.09 ## 9 1 Trenton Robinson 0.68 ## 10 1 Paul Worrilow 0.496 ## # … with 40 more rows This returns a tibble of size \\(50 \\times 3\\). The second column shows the names of the players included in the sample. The additional first column is equal to one for all cases in the sample. This indicates that the 50 rows all belong to the first sample. You are also invited to interactively scroll through the sample by typing the command View(nfl_sample_1). Let’s now compute the average salary for the players in the sample: nfl_sample_1_mean &lt;- nfl_sample_1 %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_1_mean ## # A tibble: 1 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.10 We can see that the average salary in our sample is \\(\\bar{x} =\\) 2.1 million dollars. The sample mean, 2.1, is close to the population mean, 2.24, even if not exactly the same. We are not surprised of this result: we do not expect the mean of every sample to be exactly equal to the population mean, but we do hope that they are somewhat close. Question 3: Take another sample of size \\(n = 50\\) players and compute the average salary for the new sample. How does it compare with the mean from the previous sample? Let’s take another random sample of size 50 and compute the mean salary: nfl_sample_2 &lt;- nfl %&gt;% rep_sample_n(size = 50) nfl_sample_2 ## # A tibble: 50 x 3 ## # Groups: replicate [1] ## replicate Player YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Trevor Scott 0.73 ## 2 1 Scott Crichton 0.755 ## 3 1 NaVorro Bowman 9.05 ## 4 1 Brian Dixon 0.51 ## 5 1 Kyle Williams 0.745 ## 6 1 Tony Jefferson 0.498 ## 7 1 Anthony Hitchens 0.664 ## 8 1 Cedric Ogbuehi 2.33 ## 9 1 Jordan Poyer 0.555 ## 10 1 Brent Celek 4.88 ## # … with 40 more rows nfl_sample_2_mean &lt;- nfl_sample_2 %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_2_mean ## # A tibble: 1 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.05 The statistic computed on the second sample is \\(\\bar{x} =\\) 2.05. Again, this is similar to the population parameter, \\(\\mu =\\) 2.24. We also note that the mean computed on the second sample (2.05) is different from the mean computed on the first sample (2.1). We could have immediately obtained two samples, each of size \\(n = 50\\) units. This requires that we repeat/replicate two times the activity of sampling 50 units from the entire population. Every replicated sampling of 50 units from the population is done “from scratch”, so that the population is always unchanged from replicate to replicate. This is done in the function rep_sample_n(size = 50) by including the extra argument reps = 2: nfl_samples &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 2) nfl_samples ## # A tibble: 100 x 3 ## # Groups: replicate [2] ## replicate Player YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Andrew Luck 7.65 ## 2 1 Antonio Brown 8.39 ## 3 1 Patrick Chung 2.4 ## 4 1 Chris Williams 3.29 ## 5 1 Chris Lewis-Harris 0.585 ## 6 1 Lance Louis 1.01 ## 7 1 Adam Humphries 0.525 ## 8 1 Wade Keliikipi 0.525 ## 9 1 Dustin Colquitt 3.75 ## 10 1 Tyronne Green 0.745 ## # … with 90 more rows If you explore this tibble, it has \\(50 \\times 2 = 100\\) rows. The column replicate takes value 1 for the first 50 rows, and the value 2 for the next 50 rows. This indicates that the players in rows 1 to 50 are selected to be in the first sample, while the players in rows 51 to 100 are those selected to be in the second sample. We can now compute the mean yearly salary for each of the two samples. The salaries of the 50 players in the first sample (replicate = 1) will be summarised by a single mean value, and the salaries of the 50 players in the second sample (replicate = 2) will be summarised into another mean value. In the end we will have a tibble with just two rows: nfl_sample_means &lt;- nfl_samples %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_means ## # A tibble: 2 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.57 ## 2 2 2.19 We see that both are close to the population parameter which, if we remember, is equal to 2.24. Figure 11.4 a histogram of the two means, with a vertical red line denoting the true population mean: ggplot(nfl_sample_means, aes(x = avg)) + geom_histogram(color = &quot;white&quot;) + geom_vline(xintercept = pull(nfl_mean, avg), color = &quot;red&quot;, size = 1) + labs(x = expression(bar(x))) Figure 11.4: Sample means from two samples of size \\(n = 50\\) with population mean \\(\\mu\\) marked by a red vertical line. Clearly, we can extend the repeated sampling procedure to more than just two samples of size = 50. Let us now obtain 2000 replicated samples, all of size 50, from the same NFL population: nfl_samples &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 2000) nfl_samples ## # A tibble: 100,000 x 3 ## # Groups: replicate [2,000] ## replicate Player YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Matthew Masifilo 0.555 ## 2 1 Kyle Juszczyk 0.615 ## 3 1 Gabe Jackson 0.714 ## 4 1 Akeem Ayers 3 ## 5 1 Andy Mulumba 0.497 ## 6 1 Corey Linsley 0.601 ## 7 1 Billy Turner 0.787 ## 8 1 Bennett Jackson 0.435 ## 9 1 Chase Daniel 3.33 ## 10 1 Scott Lutrus 0.465 ## # … with 99,990 more rows This tibble has \\(50 \\times 2000 = 100,000\\) rows. The first 50 players are part of the 1st sample, the next 50 belong to the 2nd sample, and so on… We can now compute the mean of each of the 2000 samples of size 50, obtaining a tibble of 2000 sample means: nfl_sample_means &lt;- nfl_samples %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_means ## # A tibble: 2,000 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.74 ## 2 2 1.69 ## 3 3 2.44 ## 4 4 2.29 ## 5 5 2.37 ## 6 6 2.63 ## 7 7 2.46 ## 8 8 1.99 ## 9 9 2.29 ## 10 10 2.28 ## # … with 1,990 more rows Let us plot the distribution of the sample mean for 2000 random samples, and superimpose in red the true value of the population parameter \\(\\mu\\): ggplot(nfl_sample_means, aes(x = avg)) + geom_histogram(color = &quot;white&quot;) + geom_vline(xintercept = pull(nfl_mean, avg), color = &quot;red&quot;, size = 1) + labs(x = expression(bar(x))) Figure 11.5: Sampling distribution of the mean for \\(n = 50\\) with population mean \\(\\mu\\) marked by a red vertical line. Figure 11.5 shows the values of the sample mean computed from sample to sample. Hence, it shows the variability of the sample mean induced by sampling variation. Such a plot is fundamental in statistical inference, and it is called sampling distribution. Sampling distribution The sampling distribution of a statistic shows the distribution of the statistic for different samples of the same size from the same population. Clearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, … This requires the following steps: Obtaining multiple samples, all of the same size, from the same population; For each sample, calculate the value of the statistic; Plot the distribution of the computed statistics. In Figure 11.5, the distribution of sample means is centred around the population mean \\(\\mu\\) and has a shape that is similar to a bell. This can be generalised further: Centre and shape of a sampling distribution Centre: If samples are randomly selected, the sampling distribution will be centred around the population parameter. (Accuracy) Shape: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will be symmetric and bell-shaped. (Central Limit Theorem) 11.5 The standard error of a statistic The variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be. In Semester 1 you saw how to summarize the variability of data using the standard deviation: sd(). So, the variability of a statistic can be quantified by calculating the standard deviation of its sampling distribution. Technically, this is not different from an ordinary standard deviation as the code and formula is the same. However, the spread of a sample statistic is so fundamental that it has its own name: the standard error of the statistic. In other words, we use: standard deviation to denote the variability among the values in a particular sample; standard error to denote the variability of the statistics computed on many samples. Standard error The standard error of a statistic, denoted \\(SE\\), is the standard deviation of its sampling distribution. The standard error measures the typical error when estimating the population parameter with the sample statistic. You can think of the SE as a “typical distance” of a sample statistic from the population parameter. 11.6 The effect of sample size on the sampling distribution It is of interest to see how the sampling distribution of the mean salary, shown in Figure 11.5 for a sample size \\(n = 50\\), changes with the sample size. In the following code chunk we compute the sampling distribution of the mean for samples of size \\(n = 50\\), \\(n = 100\\), and \\(n = 500\\), always repeating the sampling from the population 2000 times: nfl_sample_means_n_50 &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 2000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_means_n_100 &lt;- nfl %&gt;% rep_sample_n(size = 100, reps = 2000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_means_n_500 &lt;- nfl %&gt;% rep_sample_n(size = 500, reps = 2000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) We now combine the datasets for different sample sizes into a unique tibble, adding a column with the sample size, and then plot the distributions for different sample sizes. Remember that mutate() takes a tibble and adds or changes a column. The function bind_rows() takes multiple tibbles and stacks them under each other. nfl_sample_means_vary_n &lt;- bind_rows( nfl_sample_means_n_50 %&gt;% mutate(n = 50), nfl_sample_means_n_100 %&gt;% mutate(n = 100), nfl_sample_means_n_500 %&gt;% mutate(n = 500) ) ggplot(nfl_sample_means_vary_n, aes(x = avg)) + geom_histogram(color = &quot;white&quot;) + geom_vline(xintercept = pull(nfl_mean, avg), color = &quot;red&quot;, size = 1) + facet_grid(rows = vars(n), labeller = label_both) + labs(x = expression(bar(x)), title = &quot;Distribution of the sample mean for samples of size 50, 100, 500&quot;) Figure 11.6: Three sampling distributions of the mean, with population mean \\(\\mu\\) marked by a red vertical line. Figure 11.6 shows that as the sample size increases, the variability of the sampling distributions decreases, hence the standard error of the statistic decreases as the sample size increases. We can create a tibble that shows, for each sample size, the standard error of the sample mean: nfl_sample_means_vary_n %&gt;% group_by(n) %&gt;% summarise(SE = sd(avg)) ## # A tibble: 3 x 2 ## n SE ## &lt;dbl&gt; &lt;dbl&gt; ## 1 50 0.421 ## 2 100 0.299 ## 3 500 0.121 As we discussed, the tibble shows that as the sample size \\(n\\) increases, the standard error \\(SE\\) decreases. The larger the sample size, the lower the typical error of our estimate will be, and for every sample we will obtain a calculated statistic that is more similar to the population parameter. 11.7 Take-home message You might be wondering: why did we take repeated samples of size \\(n\\) from the population when, in practice, we can only afford to take one? This is a good question. We have taken multiple samples to show how the estimation error varies with the sample size. We saw that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more precise statistics, i.e. the estimates are more concentrated around the true parameter value. This teaches us that, when we have to design a study, it is better to obtain just one sample with size \\(n\\) as large as we can afford. ► Example What would the sampling distribution look like if we could afford to take samples as big as the entire population, i.e. of size \\(n = N\\)? ► Solution If we could afford to census the entire population, then we would find the exact value of the parameter. By taking repeated samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one. To summarize: We have high precision when the estimates are less variable, and this happens for a large sample size. We have high accuracy when our sampling strategy is not biased, and this happens when we do random sampling. High accuracy means that our sampling distribution is centred on the true parameter value. These points are visually displayed in Figure 11.7, where the population parameter is represented as the centre of the target: Figure 11.7: Comparing accuracy and precision. (From www.moderndive.com) 11.8 Lab: Hollywood movies The following code chunk reads in data from over 900 Hollywood movies produced between 2007 and 2013. Consider it as the entire population of movies produced in Hollywood in that time period. hollywood &lt;- read_tsv(&#39;https://edin.ac/2N9yHms&#39;) hollywood ## # A tibble: 970 x 16 ## Movie LeadStudio RottenTomatoes AudienceScore Story Genre TheatersOpenWeek OpeningWeekend ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Spid… Sony 61 54 Meta… Acti… 4252 151. ## 2 Shre… Paramount 42 57 Quest Anim… 4122 122. ## 3 Tran… Paramount 57 89 Mons… Acti… 4011 70.5 ## 4 Pira… Disney 45 74 Resc… Acti… 4362 115. ## 5 Harr… Warner Br… 78 82 Quest Adve… 4285 77.1 ## 6 I Am… Warner Br… 69 69 Quest Thri… 3606 77.2 ## 7 The … Universal 93 91 Purs… Thri… 3660 69.3 ## 8 Nati… Disney 31 72 The … Thri… 3832 44.8 ## 9 Alvi… Fox 26 73 Come… Anim… 3475 44.3 ## 10 300 Warner Br… 60 90 Sacr… Acti… 3103 70.9 ## # … with 960 more rows, and 8 more variables: BOAvgOpenWeekend &lt;dbl&gt;, DomesticGross &lt;dbl&gt;, ## # ForeignGross &lt;dbl&gt;, WorldGross &lt;dbl&gt;, Budget &lt;dbl&gt;, Profitability &lt;dbl&gt;, OpenProfit &lt;dbl&gt;, ## # Year &lt;dbl&gt; Among the recorded variables, three will be of interest: Movie; Genre; Budget. ► Question Extracting relevant variables Extract from the hollywood tibble the three variables of interest (Movie, Genre, Budget) and keep the movies for which we have all information (no missing entries). ► Solution We can extract variables using the function select(), while to keep the rows for which we have all measurements we use na.omit(): hollywood &lt;- hollywood %&gt;% select(Movie, Genre, Budget) %&gt;% na.omit hollywood ## # A tibble: 665 x 3 ## Movie Genre Budget ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Spider-Man 3 Action 258 ## 2 Shrek the Third Animation 160 ## 3 Transformers Action 150 ## 4 Pirates of the Caribbean: At World&#39;s End Action 300 ## 5 Harry Potter and the Order of the Phoenix Adventure 150 ## 6 I Am Legend Thriller 150 ## 7 The Bourne Ultimatum Thriller 110 ## 8 National Treasure: Book of Secrets Thriller 130 ## 9 Alvin and the Chipmunks Animation 70 ## 10 300 Action 65 ## # … with 655 more rows ► Question Proportion of comedy movies What is the population proportion of comedy movies? What is an estimate of the proportion of comedy movies using a sample of size 20? Using the appropriate notation, report your results in one or two sentences. ► Solution prop_comedy &lt;- hollywood %&gt;% summarise(prop = mean(Genre == &quot;Comedy&quot;)) %&gt;% pull(prop) prop_comedy ## [1] 0.2541353 sample_prop_comedy &lt;- hollywood %&gt;% rep_sample_n(size = 20) %&gt;% summarise(prop = mean(Genre == &quot;Comedy&quot;)) %&gt;% pull(prop) sample_prop_comedy ## [1] 0.2 The population proportion of comedy movies is \\(p =\\) 0.25, while the proportion of comedy movies in the sample is \\(\\hat{p} =\\) 0.2. ► Question Sampling distributions What is a sampling distribution? ► Solution The sampling distribution is the distribution of the values that a statistic takes on different samples of the same size and from the same population. ► Question Sampling distribution of the proportion Compute the sampling distribution of the proportion of comedy movies for sample size \\(n = 20\\), using 1000 replicates. Is it centred at the population value? ► Solution sample_props &lt;- hollywood %&gt;% rep_sample_n(size = 20, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(prop = mean(Genre == &#39;Comedy&#39;)) ggplot(sample_props, aes(x = prop)) + geom_histogram(color = &#39;white&#39;) + geom_vline(xintercept = prop_comedy, color = &#39;red&#39;, size = 1) + labs(x = expression(hat(p))) Figure 11.8: Sampling distribution of the proportion for \\(n = 20\\) with population parameter \\(p\\) marked by a red vertical line. Yes, Figure 11.8 shows that the distribution is almost bell-shaped and centred at the population parameter. ► Question Standard error Using the replicated samples from the previous question, what is the standard error of the sample proportion of comedy movies? ► Solution The standard error of the sample proportion is simply the standard deviation of the distribution of sample proportions for many samples. Since we have already computed the proportions for 1000 samples in the previous question, we just have to compute their variability using the standard deviation: se_prop &lt;- sample_props %&gt;% summarise(SE = sd(prop)) %&gt;% pull(SE) se_prop ## [1] 0.09566467 The standard error of the sample proportion for sample size \\(n = 20\\), based on 1000 replicated samples, is \\(SE(\\hat{p}) =\\) 0.1. ► Question The effect of sample size on the standard error of the sample proportion How does the sample size affect the standard error of the sample proportion? Compute the sampling distribution for the proportion of comedy movies using samples of size \\(n = 20\\), \\(n = 50\\), \\(n = 200\\) respectively and 1000 replicated samples. ► Solution sample_props_20 &lt;- hollywood %&gt;% rep_sample_n(size = 20, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(prop = mean(Genre == &#39;Comedy&#39;)) sample_props_50 &lt;- hollywood %&gt;% rep_sample_n(size = 50, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(prop = mean(Genre == &#39;Comedy&#39;)) sample_props_200 &lt;- hollywood %&gt;% rep_sample_n(size = 200, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(prop = mean(Genre == &#39;Comedy&#39;)) sample_props_vary_n &lt;- bind_rows( sample_props_20 %&gt;% mutate(n = 20), sample_props_50 %&gt;% mutate(n = 50), sample_props_200 %&gt;% mutate(n = 200) ) ggplot(sample_props_vary_n, aes(x = prop)) + geom_histogram(color = &#39;white&#39;) + geom_vline(xintercept = prop_comedy, color = &#39;red&#39;, size = 1) + facet_grid(rows = vars(n), labeller = label_both) + labs(x = expression(hat(p)), title = &quot;Sampling distribution of proportion for samples of size 20, 50, 200&quot;) Figure 11.9: Three sampling distributions of the proportion, with population parameter \\(p\\) marked by a red vertical line. From Figure 11.9 we can see that, as the sample size increases, the standard error of the sample proportion decreases. Increasing the sample size, the spread of the statistic values is reduced. ► Question Comparing the budget for action and comedy movies What is the population average budget (in millions of dollars) allocated for making action vs comedy movies? And the standard deviation? ► Solution budgets &lt;- hollywood %&gt;% filter(Genre == &#39;Action&#39; | Genre == &#39;Comedy&#39;) %&gt;% group_by(Genre) %&gt;% summarise(avg_budget = mean(Budget), sd_budget = sd(Budget)) budgets ## # A tibble: 2 x 3 ## Genre avg_budget sd_budget ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Action 85.9 64.0 ## 2 Comedy 36.9 26.7 From the above tibble we see that action movies have been allocated a higher budget (\\(\\mu_{Action} =\\) 85.9) than comedy movies (\\(\\mu_{Comedy} =\\) 36.9). At the same time, action movies have a higher variability of budgets around the mean value (\\(\\sigma_{Action} =\\) 64 vs \\(\\sigma_{Comedy} =\\) 26.7). ► Question (Optional) Estimate the average difference in budget for action vs comedy movies Estimate the average difference in budget for action vs comedy movies, using two samples of size 100. How does it compare to the population difference? Hint: Filter the hollywood tibble to keep only action movies. Then, obtain a sample of 100 action movies and estimate the average budget. Repeat the same steps for comedy movies. ► Solution sample_budget_action &lt;- hollywood %&gt;% filter(Genre == &#39;Action&#39;) %&gt;% rep_sample_n(size = 100) %&gt;% summarise(avg_budget = mean(Budget)) sample_budget_action ## # A tibble: 1 x 2 ## replicate avg_budget ## &lt;int&gt; &lt;dbl&gt; ## 1 1 87.5 sample_budget_comedy &lt;- hollywood %&gt;% filter(Genre == &#39;Comedy&#39;) %&gt;% rep_sample_n(size = 100) %&gt;% summarise(avg_budget = mean(Budget)) sample_budget_comedy ## # A tibble: 1 x 2 ## replicate avg_budget ## &lt;int&gt; &lt;dbl&gt; ## 1 1 36.6 est_diff &lt;- pull(sample_budget_action, avg_budget) - pull(sample_budget_comedy, avg_budget) est_diff ## [1] 50.869 The estimated difference in mean budget is \\(\\bar{x}_{Action} - \\bar{x}_{Comedy} =\\) 87.5 - 36.6 = 50.9. The population difference in budget can be calculated from the previous question, and is equal to \\(\\mu_{Action} - \\mu_{Comedy} =\\) 85.9 - 36.9 = 49. The two values are not exactly equal, as we have estimated the parameter with a sample of size 100. 11.9 Summary In Section 11.3 we have reviewed the difference between a population parameter and a statistic computed on a sample [LO1]. Because of the sampling criteria, which select units at random, those included in the sample vary when the sampling procedure is repeated. The distribution of the values that a sample statistics takes on the different samples is called sampling distribution, and has been defined in Section 11.4 [LO2]. The spread of a statistic gives an idea of how close our estimate of the unknown population parameter is to the population value. Hence, lower variability means a better guess. We quantified the variability of a statistic with its standard error, defined as the standard deviation of the statistic’s sampling distribution, and in Section 11.6 we saw that as the sample size increases, the standard error decreases [LO3]. 11.10 Glossary Statistical inference. The process of drawing conclusions about the population from the data collected in a sample. Population. The entire collection of units of interest. Sample. A subset of the entire population. Random sample. A subset of the entire population, picked at random, so that any conclusion made from the sample data can be generalised to the entire population. Representation bias. Happens when some units of the population are systematically underrepresented in samples. Generalisability. When information from the sample can be used to draw conclusions about the entire population. This is only possible if the sampling procedure leads to samples that are representative of the entire population (such as those drawn at random). Parameter. A fixed but typically unknown quantity describing the population. Statistic. A quantity computed on a sample. Sampling distribution. The distribution of the values that a statistic takes on different samples of the same size and from the same population. Standard error. The standard error of a statistic is the standard deviation of the sampling distribution of the statistic. 11.11 References Chester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman and Hall/CRC, 2019. Freely available online at: https://moderndive.com/ Robin H. Lock et al. Statistics: unlocking the power of data. Wiley, 2013. You might remember it from the cartoon Wile E. Coyote and the Road Runner.↩ Of course a population might change over time, as people can enter or leave at any time, so you might wonder why did we say that a population parameter is fixed? Because of the large number of units in the entire population, it is reasonable to assume that the addition of comparatively few units to the entirety leads to a negligible change in the population parameter.↩ "],
["bootstrapping-and-confidence-intervals.html", "Chapter 12 Bootstrapping and Confidence Intervals 12.1 Recap 12.2 From Sampling to Resampling 12.3 More generally… 12.4 Confidence Intervals 12.5 Summary 12.6 Take-home message 12.7 Lab 12.8 Glossary 12.9 References", " Chapter 12 Bootstrapping and Confidence Intervals Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand how bootstrap resampling with replacement can be used to approximate a sampling distribution. LO2. Understand how the bootstrap distribution can be used to derive a range of highly plausible values (a confidence interval). Reading This week’s reading is Chapter 8 of the book by Chester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman and Hall/CRC, 2019. Freely available online at: https://moderndive.com/ 12.1 Recap Last week, we learnt about how we can take a statistic from a sample to draw conclusions about a parameter of the population from which the sample is drawn. We then focused on how these sample statistics will vary from sample to sample. We saw that by taking lots of samples we could create a sampling distribution for a statistic, allowing us to quantify the variation in the sample statistics due to sampling. Specifically, we learnt that the standard deviation of the sampling distribution is known as the standard error. Finally, we saw how the size of our samples influences the sampling variation, with bigger samples leading to narrower sampling distributions, and more precise estimates. This week, we are going to continue to think about how we can quantify sampling variation, but specifically when we have only a single sample (which is often the case in real life). We will also see how, just as we can take a sample in order to calculate a single point estimate of a population parameter, we can use sampling variation to construct a range of plausible values which, in the case of uncertain estimates, might be more meaningful than a single value. 12.2 From Sampling to Resampling We mentioned last week that we often have neither the time nor the resources to collect data from the entire population (a census). It is also often infeasible to get many samples of size \\(n\\) in order to get an idea of how accurate our estimate of the population parameter is. How can we study the variability of our sample statistic with only one sample? It turns out that we can mimick the act of sampling \\(n\\) units from the population, by resampling with replacement \\(n\\) units from our original sample of \\(n\\) units. This is broadly known as bootstrapping. Bootstrapping Bootstrap definition: Random sampling with replacement from the original sample, using the same sample size. ► The NFL Example Think back to last week. We saw a dataset of all the National Football League players at the beginning of 2015, along with their yearly salaries. This was our population (in real life we often don’t have data on the entire population). We took multiple samples of 50 players in order to study how the mean salaries of those samples varied (this allowed us to determine the accuracy of using a mean from a sample of 50 players (\\(\\bar{x}\\)) as an estimate of the population parameter \\(\\mu\\)). In fact, we took 2000 samples of 50 players, which is not at all feasible in practice. Now let’s imagine we only collect one sample of 50 players. We can approximate the sampling distribution of \\(\\bar{x}\\) (the mean salary of our sample) by bootstrapping. To do this, we: Collect a sample of 50 players. Compute the mean salary of the sample. Take a random sample with replacement of 50 players from our original sample (this is known as a resample), and compute the mean of the resample. Re-do point 3 many times. Think What do we mean by with replacement, and why is it necessary? ► Answer With replacement simply means that as we take our sample, we replace the first item before we choose the second.. and so on. If we resampled our 50 player sample without replacement, we would simply end up with the same 50 players, and therefore the same mean! ► Solution Take a sample. We have a sample of 50 players and their salaries, which we can read in to R as follows: library(tidyverse) library(moderndive) nfl_sample &lt;- read_csv(&quot;data/nfl_sample.csv&quot;) nfl_sample ## # A tibble: 50 x 5 ## Player Position Team TotalMoney YearlySalary ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Marcus Williams CB Jets 1.53 0.51 ## 2 Antonio Smith 34DE Broncos 2 2 ## 3 Dion Jordan 43DE Dolphins 20.6 5.14 ## 4 Carl Davis 34DT Ravens 2.95 0.738 ## 5 Jason Hatcher 34DE Redskins 27.5 6.88 ## 6 Caushaud Lyons 43DE Steelers 1.58 0.528 ## 7 Thomas Rawls RB Seahawks 1.59 0.53 ## 8 Michael Griffin S Titans 35 7 ## 9 Winston Guy S Colts 1.42 0.71 ## 10 Kevin White WR Bears 16.6 4.14 ## # … with 40 more rows Compute the mean salary of our sample: nfl_sample %&gt;% summarise(avg_salary = mean(YearlySalary)) ## # A tibble: 1 x 1 ## avg_salary ## &lt;dbl&gt; ## 1 2.11 Sample our original sample, with replacement, and compute the mean: nfl_resample1 &lt;- nfl_sample %&gt;% rep_sample_n(size = 50, replace = TRUE) nfl_resample1 ## # A tibble: 50 x 6 ## # Groups: replicate [1] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Justin Britt RT Seahawks 3.55 0.864 ## 2 1 Zach Kerr 43DT Colts 1.54 0.512 ## 3 1 Michael Harris LT Vikings 1.54 1.54 ## 4 1 Andre Caldwell WR Broncos 2.7 1.35 ## 5 1 Kyle Brindza K Buccaneers 1.58 0.528 ## 6 1 Brett Kern P Titans 15 3 ## 7 1 Aaron Donald 43DT Rams 10.1 2.53 ## 8 1 Aaron Williams S Bills 26.0 6.50 ## 9 1 Trevor Robinson C Chargers 1.7 0.85 ## 10 1 Josh Mitchell CB Colts 1.58 0.526 ## # … with 40 more rows nfl_resample1 %&gt;% summarise(avg_salary = mean(YearlySalary)) ## # A tibble: 1 x 2 ## replicate avg_salary ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.34 and again.. nfl_resample2 &lt;- nfl_sample %&gt;% rep_sample_n(size = 50, replace = TRUE) nfl_resample2 ## # A tibble: 50 x 6 ## # Groups: replicate [1] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Kelvin Beachum RT Steelers 2.15 0.536 ## 2 1 DJ Williams ILB Bears 1.5 1.5 ## 3 1 Marqueston Huff S Titans 2.65 0.664 ## 4 1 Jason Hatcher 34DE Redskins 27.5 6.88 ## 5 1 Brett Kern P Titans 15 3 ## 6 1 Fletcher Cox 34DE Eagles 10.2 2.56 ## 7 1 Breno Giacomini RT Jets 18 4.5 ## 8 1 Kevin White WR Bears 16.6 4.14 ## 9 1 DJ Williams ILB Bears 1.5 1.5 ## 10 1 Khalif Barnes RT Raiders 1.5 1.5 ## # … with 40 more rows nfl_resample2 %&gt;% summarise(avg_salary = mean(YearlySalary)) ## # A tibble: 1 x 2 ## replicate avg_salary ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1.71 and so on… Key point If we resample with replacement from our original sample enough times, then the distribution of all the means of these resamples begins to approximate the sampling distribution. We can speed up this process by getting R to take many resamples for us, in the same way that last week we asked it to take many samples from a population. nfl_2000resamples &lt;- nfl_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 2000) The above line of code takes 2000 sample of size 50, by sampling with replacement from our original sample of size 50. \\(2000 \\times 50 = 100,000\\), so this results in a tibble with \\(100,000\\) rows. nfl_2000resamples ## # A tibble: 100,000 x 6 ## # Groups: replicate [2,000] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Josh Mitchell CB Colts 1.58 0.526 ## 2 1 Justin Britt RT Seahawks 3.55 0.864 ## 3 1 Kevin White WR Bears 16.6 4.14 ## 4 1 Stephen Hill WR Panthers 0.595 0.595 ## 5 1 Aaron Williams S Bills 26.0 6.50 ## 6 1 Marqueston Huff S Titans 2.65 0.664 ## 7 1 Dion Jordan 43DE Dolphins 20.6 5.14 ## 8 1 David Bass 34OLB Titans 1.26 0.63 ## 9 1 Khalif Barnes RT Raiders 1.5 1.5 ## 10 1 Trai Turner G Panthers 2.79 0.698 ## # … with 99,990 more rows We can compute the mean of each of the 2000 resamples drawn from the original sample (just like last week when we computed the mean of each of 2000 samples drawn from the population). nfl_resample_means &lt;- nfl_2000resamples %&gt;% group_by(replicate) %&gt;% summarise(avg_salary = mean(YearlySalary)) and we can plot them: ggplot(nfl_resample_means, aes(x = avg_salary)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;resample mean&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 12.1: Bootstrap resampling distribution based on 2000 resamples ► Question Where do you think that this histogram is centred? the mean salary of the population (\\(\\mu\\)) the mean salary of the original sample (\\(\\bar{x}\\)) somewhere else ► Solution The distribution of means of resamples will be centred around the mean of our original sample, 2.11 nfl_resample_means %&gt;% summarise(mean_of_means = mean(avg_salary)) ## # A tibble: 1 x 1 ## mean_of_means ## &lt;dbl&gt; ## 1 2.11 ► Question Last week we looked at the standard error (the standard deviation of the sampling distribution). We have seen how the boostrap distribution is an approximation of the sampling distribution. TRUE or FALSE: The standard deviation of the bootstrap distribution is an approximation of the standard error of \\(\\bar{x}\\). ► Solution TRUE! 12.3 More generally… Now let’s think more generally about what we did there… We were interested in estimating some unknown parameter of a population. We had a sample of size \\(n\\) drawn at random from the population. We took 1000 resamples (of size \\(n\\)) from our original sample, and calculated a statistic for each one. We then visualised the distribution of those statistics. The tool below will help to conceptualise these steps: The big blue distribution at the top: The population The vertical blue line: population parameter \\(\\mu\\) The yellow sample button: Take a sample from the population (note you can change the sample \\(n\\)) The green resample button: Samples with replacement from the original sample (the yellow one), and calculates the mean (which is then dropped into the bottom panel) The bottom panel: The distribution of resample means - the bootstrap distribution! Spend 10 minutes changing things such as the sample size. If you have any questions about what is happening, then ask one of the tutors. source: http://wise1.cgu.edu/portfolio/bootstrap 12.4 Confidence Intervals ► Question Look at the bootstrap distribution below. Roughly, between what two values do most of the resample means lie? ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ► Solution “Most” is very vague. Just eyeballing, most of the distribution lies between 1.5 and 2.7. ggplot(nfl_resample_means, aes(x = avg_salary)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;resample mean&quot;) + geom_vline(xintercept = c(1.5, 2.7)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 12.2: Bootstrap resampling distribution based on 2000 resamples Confidence intervals simply answer more exactly where “most” sample means lie - they give us a range of plausible values for our population parameter. To construct a confidence interval, we need two things: a confidence level; a measure of sampling variability. We have the latter, in the form of our bootstrap distribution. The confidence level, instead, needs to be set by us. For instance, we might ask between which values the middle 95% (or 90%, or 80%, etc.) of our distribution falls. In other words, the confidence level is the “success rate”: the proportion of all samples whose intervals contain the true parameter. 12.4 How exactly do we interpret a confidence interval? If we were to do this whole process over and over again: take a random sample of size \\(n\\) sample with replacement from that sample construct a 95% confidence interval If we did this many many times, then about 95% of the confidence intervals we created would contain the population mean. So if we did this 100 times, we would expect about 5 of our 95% confidence intervals to not contain the true population mean. And if we had been constructing 80% confidence intervals instead, we would expect roughly 80 of them to contain the population mean. 12.4.1 Calculating confidence intervals using a bootstrap standard error We can construct confidence intervals using the standard error. However, we can not compute standard errors from just one sample, so we need to estimate the standard error of a statistic using bootstrap. We also use the following rules of thumb: If the distribution is symettric and bell-shaped… 68% of values will lie within 1 standard deviation of the mean. 95% of values will lie within 1.96 standard deviations of the mean. 99.7% of values will lie within 3 standard deviations of the mean. We have our sample mean, and we can calculate the standard deviation of our bootstrap distribution (to approximate the standard error of the sample mean). We therefore have all the information we need to calculate, for instance, a 95% confidence interval - it is simply \\(1.96 \\times \\text{standard error}\\) above and below our mean. Formally, we can write this 95% interval as: \\(\\text{Statistic} \\pm 1.96 \\times SE\\) And in R… #recall that our original sample mean was 2.11496 original_sample_mean &lt;- 2.11496 nfl_resample_means %&gt;% summarise( sd_of_means = sd(avg_salary), ci_lower = original_sample_mean - (1.96 * sd_of_means), ci_upper = original_sample_mean + (1.96 * sd_of_means) ) ## # A tibble: 1 x 3 ## sd_of_means ci_lower ci_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.301 1.52 2.71 12.5 Summary Let’s recap what we’ve done today: We started with a sample from a population. We calculated a statistic from our sample to estimate a parameter in our population. We used bootstrap (random sampling with replacement from our original sample) to estimate the standard error of the statistic. We constructed a range of plausible values by combining the sample statistic and the bootstrap estimate of the standard error of the statistic. We constructed our bootstrap distribution using code like below: bootstrap_distribution &lt;- nfl_sample %&gt;% rep_sample_n(50, replace = TRUE, reps = 2000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) and we used the standard deviation of our bootstrap resample means… bootstrap_distribution %&gt;% summarise(sd_of_means = sd(avg)) ## # A tibble: 1 x 1 ## sd_of_means ## &lt;dbl&gt; ## 1 0.290 to calculate some 95% confidence intervals using the formula: \\(\\bar{x} \\pm 1.96 \\times SE\\) which became: \\(2.11 \\pm 1.96 \\times 0.3\\) giving us a confidence interval of \\([1.52, 2.70]\\). Stop and think What we did today entailed specifying what variable we were interested in, generating replicates, calculating the statistic for each replicate, and finally, we visualised the distribution. This is an important framework for understanding how to estimate sampling variation to evaluate the accuracy of our statistical inferences. The steps for this are visualised in Figure 12.3 below. Figure 12.3: Pipeline of bootstrapping-based inference source: https://moderndive.com/8-confidence-intervals.html 12.6 Take-home message Using just one sample, it is possible to quantify estimation error by taking repeated resamples with replacement from our original sample. We can use this to construct ranges of plausible values of the parameter we are estimating. This teaches us a standardised way of reporting uncertainty in our estimates. 12.7 Lab Exercise 1: Hollywood Movies The following code chunk reads in a sample of the Hollywood movies data we saw last week. hollywood_sample &lt;- read_tsv(&#39;https://edin.ac/2N9yHms&#39;) %&gt;% # read the data select(Movie, Genre, RottenTomatoes) %&gt;% # selects relevant variables na.omit %&gt;% # removes all the NAs sample_n(size=25) # takes our sample ► Question This week, we’re interested in the average Rotten Tomatoes rating for all Hollywood movies between 2007 and 2013. What is our best estimate of this with the data we just read in? ► Solution \\(\\bar{x}\\): The mean Rotten Tomatoes rating for our sample. ► Question Calculate the sample statistic ► Solution hollywood_sample %&gt;% summarise(avg_rating = mean(RottenTomatoes)) ## # A tibble: 1 x 1 ## avg_rating ## &lt;dbl&gt; ## 1 52.2 ► Question Generate 1000 bootstrap resamples to create the bootstrap distribution. ► Solution hwood_bs_distribution &lt;- hollywood_sample %&gt;% rep_sample_n(25, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(avg_rating = mean(RottenTomatoes)) hwood_bs_distribution ## # A tibble: 1,000 x 2 ## replicate avg_rating ## &lt;int&gt; &lt;dbl&gt; ## 1 1 58.5 ## 2 2 51.1 ## 3 3 47.5 ## 4 4 53.1 ## 5 5 56.8 ## 6 6 50.4 ## 7 7 52.2 ## 8 8 44.7 ## 9 9 57.7 ## 10 10 56.9 ## # … with 990 more rows ► Question Estimate the standard error of the sample statistic from your bootstrap distribution. ► Solution hwood_bs_distribution %&gt;% summarise(estimated_SE = sd(avg_rating)) ## # A tibble: 1 x 1 ## estimated_SE ## &lt;dbl&gt; ## 1 6.35 ► Question Compute the 95% confidence intervals around our estimate of the average Rotten Tomatoes rating, and plot the bootstrap distribution and the confidence interval. ► Solution hwood_samplemean &lt;- hollywood_sample %&gt;% summarise(avg_rating = mean(RottenTomatoes)) %&gt;% pull(avg_rating) hwood_se &lt;- hwood_bs_distribution %&gt;% summarise(estimated_SE = sd(avg_rating)) %&gt;% pull(estimated_SE) hwood_ci_lower &lt;- hwood_samplemean - 1.96 * hwood_se hwood_ci_upper &lt;- hwood_samplemean + 1.96 * hwood_se ggplot(hwood_bs_distribution, aes(x=avg_rating)) + geom_histogram() + geom_vline(xintercept = c(hwood_ci_lower, hwood_ci_upper)) + labs(x = &quot;bootstrap avg rating&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ► Question Go back to the top where we read in the data, and change the sample you are collecting from 25 to 50. Run the previous tasks again - how has the confidence interval changed? ► Solution hollywood_sample &lt;- read_tsv(&#39;https://edin.ac/2N9yHms&#39;) %&gt;% select(Movie, Genre, RottenTomatoes) %&gt;% na.omit %&gt;% sample_n(size=50) hwood_bs_distribution &lt;- hollywood_sample %&gt;% rep_sample_n(50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(avg_rating = mean(RottenTomatoes)) hwood_samplemean &lt;- hollywood_sample %&gt;% summarise(avg_rating = mean(RottenTomatoes)) %&gt;% pull(avg_rating) hwood_se &lt;- hwood_bs_distribution %&gt;% summarise(estimated_SE = sd(avg_rating)) %&gt;% pull(estimated_SE) hwood_ci_lower &lt;- hwood_samplemean - 1.96 * hwood_se hwood_ci_upper &lt;- hwood_samplemean + 1.96 * hwood_se ggplot(hwood_bs_distribution, aes(x=avg_rating)) + geom_histogram() + geom_vline(xintercept = c(hwood_ci_lower, hwood_ci_upper)) + labs(x = &quot;bootstrap avg rating&quot;) Exercise 2: NFL Players ► Question Look back to last week. What was the population mean yearly salary for all NFL players at the beginning of 2015? ► Solution 2.238 (million dollers!) ► A bigger Question A researcher lives in Boston. They want to estimate salaries of NFL players, and in 2015 they go around and ask 50 players about their yearly salaries. The code below reads in the sample they collected. nflboston &lt;- read_csv(&quot;https://edin.ac/35QVPwp&quot;) Compute the sample mean, and calculate 99% confidence intervals via bootstrap standard error ► Solution nflb_samplemean &lt;- nflboston %&gt;% summarise(avg_salary = mean(YearlySalary)) %&gt;% pull(avg_salary) nflb_bs_distribution &lt;- nflboston %&gt;% rep_sample_n(50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(avg_salary = mean(YearlySalary)) nflb_se &lt;- nflb_bs_distribution %&gt;% summarise(estimated_SE = sd(avg_salary)) %&gt;% pull(estimated_SE) nflb_ci_lower &lt;- nflb_samplemean - 3 * nflb_se nflb_ci_upper &lt;- nflb_samplemean + 3 * nflb_se ggplot(nflb_bs_distribution, aes(x=avg_salary)) + geom_histogram() + geom_vline(xintercept = c(nflb_ci_lower, nflb_ci_upper)) + labs(x = &quot;bootstrap avg salary&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ► Question This confidence does not include the population mean. Why not? hint: Look at your data, and think about what you know about how it was collected - why might this not be a good sample? ► Solution The researcher, living in Boston, seems to have sampled a lot of players from the New England Patriots (a local team). The key thing here is that the statistical inference we are making (that the sample mean is an estimate of the population mean) assumes that the sample is an unbiased representation. In this case it is not a truly random sample! 12.8 Glossary Population. The entire collection of units of interest. Sample. A subset of the entire population. Parameter. A fixed but typically unknown quantity describing the population. Statistic. A quantity computed on a sample. Sampling distribution. The distribution of the values that a statistic takes on different samples of the same size and from the same population. Standard error. The standard error of a statistic is the standard deviation of the sampling distribution of the statistic. Resample. To sample again from your original sample Bootstrapping. Repeated random sampling with replacement Bootstrap distribution. The distribution of statistics calculated on random resamples. Approximates the sampling distribution of the sample statistic. Confidence interval (CI). A range of plausible values around an estimate (e.g., a sample statistic), taking into account uncertainty in the statistic (e.g., sampling variability) Confidence level. The percentage of confidence intervals which will contain the true population parameter in the long run (i.e., if you sampled the population and constructed confidence intervals many times over). The proportion of all samples whose intervals contain the true parameter. 12.9 References Chester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman and Hall/CRC, 2019. Freely available online at: https://moderndive.com/ "]
]
