[
["index.html", "Multivariate Statistics and Methodology using R Overview of the Course The team R stuff", " Multivariate Statistics and Methodology using R Department of Psychology, University of Edinburgh 2019-2020 Overview of the Course Multivariate Statistics and Methodology using R extends what you learnt last semester in USMR to provide an advanced level overview of statistical analysis techniques and methodology issues relevant to psychological research, introducing analysis tools that extend to cases where multiple outcome variables are being studied simultaneously, and hierarchical data structures (e.g., children nested in classes nested in schools). On this page you will find the weekly lab exercises, along with links to useful content online, walkthroughs, etc. The labs will begin to require a little more initiative than those from USMR, so be prepared to do some googling! Each week, solutions (where available) will be made available here for the previous weeks’ lab. The team Lecturers: Dr Aja Murray: aja.murray@ed.ac.uk (Course Organiser) Dr Dan Mirman: daniel.mirman@ed.ac.uk Senior Teching Coordinators: pg.ppls.stats@ed.ac.uk Dr Umberto Noe Dr Josiah King R stuff We will be getting to grips with a lot of new tools for data manipulation and visualisation, using some of the packages pictured below. Some of you may be familiar with some of these already, but don’t worry if not! R Cheatsheets You can find a collection of cheatshets that summarise some of the most useful commands you will be using for tasks such as data transformation, visualisation, RMarkdown and many others here/ Some key ones for this course are: RMarkdown Data Visualisation (ggplot2) Data transformation with dplyr Data Import R Community R has great representation in Edinburgh. Check out these pages to find out more: R Ladies Edinburgh EdinburghR And worldwide you have: R Studio Community "],
["tidyverse-markdown.html", "Chapter 1 Tidyverse &amp; Markdown 1.1 Data Visualization with ggplot 1.2 Data management with the Tidyverse 1.3 Reproducible research with RMarkdown", " Chapter 1 Tidyverse &amp; Markdown This week, we’re going to introduce you to some really nice packages and tools which will help you to make your analysis and reporting more efficient, aesthetically pleasing, and (importantly) reproducible. Packages If you haven’t previously installed them, install the following packages tidyverse rmarkdown haven (this one is just for reading in data from other software like SPSS or SAS) Lecture slides The lecture slides can be accessed here. The data (in .RData format) for the lecture can be found at https://edin.ac/2suU8XW Background &amp; Reading R for Data Science: https://r4ds.had.co.nz/index.html Data visualization: Chapters 3 and 28 Data management (tidyverse): Chapters 5 and 12 R Markdown: Chapter 27 Extras: Kieran Healey has a brilliant book on getting started with ggplot: Data Visualisation; a practical introduction Another great one is Fundamentals of Data Visualisation by Claus O. Wilke 1.1 Data Visualization with ggplot For plotting, you may be familiar with the popular ggplot2 package from some of the USMR labs last semester. We’re going to be using this more and more, so the first part of today’s lab will focus on ggplot. Visualization is the first step in analysis 1.1.1 Geoms To learn about some of the different functionalities of ggplot, we’re first going to need some data… ► Question Load the ggplot2 package, read in the data using load() and url(), and extract some summary statistics. The data can be found at https://edin.ac/2Erg9ZW. ► Solution library(ggplot2) load(url(&quot;https://edin.ac/2Erg9ZW&quot;)) summary(speech_ses) ## ResponseId Category Accuracy ## Length:912 Social Class:228 Min. : 3.704 ## Class :character Race :228 1st Qu.: 55.556 ## Mode :character Age :228 Median : 66.667 ## Gender :228 Mean : 69.547 ## 3rd Qu.: 82.407 ## Max. :100.000 Data overview Kraus et al. (2019) Evidence for the reproduction of social class in brief speech, Proc. Natl. Acad. Sci. U.S.A. (Study 1) N=189 speakers from the International Dialects of (North American) English Archive. Narrative speech and reading stories. Extracted 7 individual words that were produced by all speakers: “And”, “From”, “Thought”, “Beautiful”, “Imagine”, “Yellow”, and “The”. Participants (N=229, from AMT) Listened to the 7 one-word clips Estimated the speaker’s race, gender, age, and educational attainment Each participant completed this for a random subset of 27 speakers ► Question Make a summary plot showing mean accuracy for each category of judgment hint: try ?stat_summary ► Solution We need to use stat_summary because we want to summarise the y values on our plot into summary value(s) (in the case the mean). We could also calculate the mean accuracy for each category first, and then plot them using geom_bar directly, but stat_summary can be pretty useful #one way of doing this: ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + stat_summary(fun.y=mean, geom=&quot;bar&quot;) We should also note that stat_summary(fun.y=mean, geom=\"bar\") and geom_bar(stat=\"summary\",fun.y=mean) are exactly the same! You could get the same plot using: ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_bar(stat=&quot;summary&quot;,fun.y=mean) ► Question Explore the different ways of showing variability. Construct a plot using each of the following geoms: The top three plots (jitter, boxplots and violins) all show all of the data, so we don’t need to use stat_summary for these. However, the bottom two (errorbars and pointranges) require us to summarise the data into means and standard errors, so we need to use stat_summary(fun.data=mean_se). ► Solution # * Boxplot ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_boxplot() # * Jitter ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) # * Violin plot ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_violin() # * Errorbar ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;) # * Pointrange ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) ► Question Combine two of the geoms used above to create a visualization of the mean accuracy, a measure of variability, and all of the data points. ► Solution ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;, colour=&quot;black&quot;, width=0.4, size=1.5) ► Question Refine the plot by, for example, removing unnecessary elements, adding useful annotations (e.g., chance performance = 50%), selecting a good color scheme, etc. tip: This is where google becomes really helpful, for example ► Solution ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;, colour=&quot;black&quot;, width=0.4, size=1.5) + guides(colour = FALSE) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + geom_hline(yintercept=50, linetype=&quot;dashed&quot;) 1.1.2 Recreating a plot ► Question Recreate the graph below using ggplot (if you like, try to make it better!). Women in computer science The data (in .csv format) can be downloaded from https://edin.ac/2qYA0wr. You can use read.csv(url(\"https://edin.ac/2qYA0wr\")) to read it directly into R. ► Solution women_cs&lt;-read.csv(url(&quot;https://edin.ac/2qYA0wr&quot;)) ggplot(women_cs, aes(x=date, y=pct_women_majors, color=field))+ labs(x=NULL,y=NULL, title=&quot;What happened to women in computer science?&quot;)+ geom_line()+ scale_color_manual(values=c(&#39;#11605E&#39;, &#39;#17807E&#39;, &#39;#8BC0BF&#39;,&#39;#D8472B&#39;))+ scale_y_continuous(label=scales::percent)+ theme_minimal(base_family=&quot;Helvetica&quot;)+ theme(legend.title=element_blank()) # If you want to get fancier, and add the labels at the end of the lines, check out the gghighlight package! 1.2 Data management with the Tidyverse A collection of R packages known as the tidyverse provides so many incredibly useful functions that can speed up your workflow. They are often contrasted to Base R (which is what you have been working with so far) in that they provide an alternative grammar which is aimed at being more predictable and consistent. Some people find the tidyverse a lot more intuitive, but others don’t, and the transition can sometimes be difficult! 1.2.1 Piping! It may look a bit weird (%&gt;%), but the pipe operator in R is incredibly useful. Its fundamental role is to ‘chain’ functions together. Previously we wrapped functions around one another, with lots of brackets, but with %&gt;% we can link the intermediate output of one function and take it as the input of another. The two functions f and g, when used in combination like g(f(x)), can now be written as x %&gt;% f() %&gt;% g(). You don’t even always need the brackets, and coulde write x %&gt;% f %&gt;% g! The default behaviour of %&gt;% is to put the output of the LHS (left hand side) in as the first argument in the RHS. However, you can change this by using %&gt;% in combination with a ., to specify which argument you want it to be inputted as: 100 %&gt;% rnorm(10, ., 1) is equal to rnorm(10, 100, 1) The default behaviour: 100 %&gt;% rnorm(0, 1) is implicitly saying 100 %&gt;% rnorm(., 0, 1), which is equal to rnorm(100, 0, 1). ► Question Translate the following statements between Base R and sequences of pipes. The first is shown for you. 1 Base R: round(mean(rnorm(100,0,1))) Pipes : rnorm(100,0,1) %&gt;% mean() %&gt;% round() 2 Base R: x&lt;-10:100 round(exp(diff(log(x))), 2) Pipes: ► Solution 10:100 %&gt;% log() %&gt;% diff() %&gt;% exp() %&gt;% round(2) 3 Pipes: 6 %&gt;% round(pi, digits=.) Base R: ► Solution round(pi, digits=6) 1.2.2 Grouping, summarising, filtering, mutating and selecting Tidyverse also gives us really useful functions for wrangling data. There are many, but some of the key ones we’ll learn here are: select() extracts columns filter() subsets data based on conditions mutate() adds new variables group_by() group related rows together summarise()/summarize() reduces values down to a single summary For a quick example, if we want to calculate the median accuracy for each category, but only after removing those with an accuracy &lt;50, we could use: speech_ses %&gt;% filter(Accuracy&gt;50) %&gt;% group_by(Category) %&gt;% summarise( mdn_accuracy = median(Accuracy) ) And if we wanted to also calculate the mean accuracy for each category, we could add: speech_ses %&gt;% group_by(Category) %&gt;% summarise( n = n(), mean_acc = mean(Accuracy) ) ► Question Load the tidyverse, and haven package, and read in the data using read_sav() (.sav is the type of file which comes out of another stats software, SPSS). You can download the data from https://edin.ac/34n6AWA to your computer, and then read it in. library(tidyverse) exam &lt;- haven::read_sav(&quot;data/exam.sav&quot;) Using the exam.sav data: ► Question Calculate the mean score for each exam ► Solution exam %&gt;% group_by(exam) %&gt;% summarize(M = mean(scores)) ## # A tibble: 3 x 2 ## exam M ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 42.9 ## 2 2 37.9 ## 3 3 38.9 ► Question Calculate the mean score for each exam for female students only ► Solution exam %&gt;% filter(gender==&quot;f&quot;) %&gt;% group_by(exam) %&gt;% summarize(M = mean(scores)) ## # A tibble: 3 x 2 ## exam M ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 43.1 ## 2 2 36.6 ## 3 3 38.1 ► Question Make a new dataframe containing only the exam scores for males for exam number 1, with a new variable indicating whether they passed or not (pass = a score of 40) ► Solution exam_m1 &lt;- exam %&gt;% filter(exam == 1, gender == &quot;m&quot;) %&gt;% mutate(pass = ifelse(scores&gt;40,&quot;pass&quot;,&quot;fail&quot;)) ► Question Calculate the average score for each exam for male and female students\")` ► Solution exam %&gt;% group_by(exam, gender) %&gt;% summarize(M = mean(scores)) ## # A tibble: 6 x 3 ## # Groups: exam [3] ## exam gender M ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 f 43.1 ## 2 1 m 42.7 ## 3 2 f 36.6 ## 4 2 m 39.1 ## 5 3 f 38.1 ## 6 3 m 39.7 # use spread() to make it easier to compare exam %&gt;% group_by(exam, gender) %&gt;% summarize(M = mean(scores)) %&gt;% spread(gender, M) ## # A tibble: 3 x 3 ## # Groups: exam [3] ## exam f m ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 43.1 42.7 ## 2 2 36.6 39.1 ## 3 3 38.1 39.7 1.2.3 Reshaping The same data can be represented in many different ways. We often discern between long and wide formats, and each of these are useful in different ways. Consider, the below example, showing the same data in long format on the left, and in wide on the right. There are some useful functions which we can use to move between these formats: gather() and spread(). Check out the explanation of them in the reading, section 12.3 A newer version of these are pivot_longer() and pivot_wider(), but gather() and spread() will continue to be available if you want them. Data overview The USArrests data set (comes with R) contains violent crime arrests (per 100,000 residents) in each of the 50 states in the USA in 1973 and the percent of the population of each state that lived in urban areas. You can see it by just typing USArrests in R. ► Question Convert the USArrests data set from a wide to a long format so that instead of separate variables for each crime type (Murder, Assault, Rape), there is one variable that identifies the crime type and one variable that contains the rates for each crime type for each state. ► Solution x &lt;- gather(USArrests, key=&quot;CrimeType&quot;, value=&quot;Rate&quot;, Murder, Assault, Rape) # or x &lt;- pivot_longer(USArrests, cols = c(Murder, Assault, Rape), names_to = &quot;CrimeType&quot;, values_to = &quot;Rate&quot;) ► Question Make a scatterplot showing the relationship between each type of violent crime rate and percent of population living in urban areas. ► Solution ggplot(x, aes(UrbanPop, Rate)) + facet_wrap(~CrimeType, scales=&quot;free&quot;, nrow=1) + geom_point() + stat_smooth(method=&quot;lm&quot;) Less guidance Data overview The ability data set in the psych package contains accuracy of 1525 subjects on 16 multiple choice IQ-test-type questions. The questions are of 4 types: basic reasoning, letter sequence, matrix reasoning, and spatial rotation. There are four questions of each type. You can see the by typing psych::ability (those :: are just a way of accessing something from inside a package without loading it). ► Question Tidy the data and make a graph of average accuracy for each question type. You might have to use as_tibble(ability) or as.data.frame(ability) because it is initially stored as a matrix. Hint: the separate() function may come in handy at some point. ► Solution ability &lt;- psych::ability iq &lt;- as_tibble(ability) %&gt;% pivot_longer(., cols=1:16, names_to = &quot;Item&quot;, values_to = &quot;Correct&quot;) %&gt;% # gather(key=&quot;Item&quot;, value=&quot;Correct&quot;, 1:16) %&gt;% # The gather() alternative... separate(Item, c(&quot;Domain&quot;, &quot;Number&quot;)) ggplot(iq, aes(Domain, Correct)) + stat_summary(fun.y = mean, geom=&quot;bar&quot;) 1.3 Reproducible research with RMarkdown We’re also going to start to use RMarkdown. This is a really useful means of making a report reproducible. Essentially, it is a combination of R code and normal text. It will require learning a few new formatting rules (the “markdown” bit), but it means that in one file you can read in and analyse your data, and compile it to a pdf. Which essentially means that if your data or analysis changes, then the results you report change too without having to edit them! 1.3.1 Convert a script into a R Notebook Open your script from the exercises so far. Compile a HTML report from that script. ► Question Create a new R Notebook file, fill it in with the content of your script from the exercises so far.\")` Hint: R code goes into R chunks, add some text in between chunks. Add formatting to make it look nicer: headers, bold, italics, etc (see the cheat-sheet) Add chunk options to suppress extraneous messages and warnings, and to control the size of figures. ► Question Knit the notebook into a HTML file. "],
["basic-multilevel-regression.html", "Chapter 2 Basic Multilevel Regression 2.1 LME4 2.2 Introduction to MLR 2.3 Logistic MLR", " Chapter 2 Basic Multilevel Regression We’re going to start using multilevel modelling. There are a whole lot of different names people use for this sort of methodology (hierarchical linear models, linear mixed-effect models, mixed models, nested data models, random coefficient, random-effects models, random parameter models… and so on). What the idea boils down to is that model parameters vary at more than one level. Packages lme4 tidyverse effects Load the tidyverse, lme4 and effects packages (install them if you haven’t already). library(tidyverse) library(lme4) library(effects) Lecture Slides The lecture slides can be accessed here. The data for the lecture can be found at https://edin.ac/36bD1s0 (Visual search data) and https://edin.ac/2QwG7SG (Novel world learning data). Background &amp; Reading Winter, 2013 Brauer &amp; Curtin, 2018, (pdf) Luke, 2017 2.1 LME4 We’re going to use the lme4 package, and specifically the functions lmer() and glmer(). “(g)lmer” here stands for “(generalised) linear mixed effects regression”. You will have seen some use of these functions in the lectures. The broad syntax is: lmer(formula, REML = logical, data = dataframe) The formula bit is similar to what we did with a standard linear model (lm()) in that it has the outcome ~ explanatory variables structure. However, we now have the addition of the random effect terms, specified in parenthesis with the | operator separating parameters on the LHS and a grouping factor on the RHS. Below are a selection of different formulas for specifying different random effect structures. Formula Alternative Meaning \\(\\text{(1 | g)}\\) \\(\\text{1 + (1 | g)}\\) Random intercept with fixed mean \\(\\text{0 + offset(o) + (1 | g)}\\) \\(\\text{-1 + offset(o) + (1 | g)}\\) Random intercept with a priori means \\(\\text{(1 | g1/g2)}\\) \\(\\text{(1 | g1) + (1 | g1:g2)}\\) Intercept varying among \\(g1\\) and \\(g2\\) within \\(g1\\) \\(\\text{(1 | g1) + (1 | g2)}\\) \\(\\text{1 + (1 | g1) + (1 | g2)}\\) Intercept varying among \\(g1\\) and \\(g2\\) \\(\\text{x + (x | g)}\\) \\(\\text{1 + x + (1 + x | g)}\\) Correlated random intercept and slope \\(\\text{x + (x || g)}\\) \\(\\text{1 + x + (x | g) + (0 + x | g)}\\) Uncorrelated random intercept and slope Table 1: Examples of the right-hand-sides of mixed effects model formulas. \\(g\\), \\(g1\\), \\(g2\\) are grouping factors, covariates and a priori known offsets are \\(x\\) and \\(o\\). Errors and warnings? For large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning. There are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them). For now, if lmer() gives you convergence errors, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model. 2.2 Introduction to MLR 2.2.1 Exercise 1 County-level suicide rate data from Public Health England (PHE) is available at https://edin.ac/36xdhas and covers the period from 2001 to 2016. It contains information for a number of different indicators over this period for a selection of counties in England. ► Question 1 Using multilevel regression, study the following: Did the regions differ in their suicide rates? Did the regions differ in ther slopes of change of suidice rate? Make a plot of the fitted values from the model Steps: First, get acquainted with the data. It contains various different indicators but we’re only interested in one (suicide rates), so you might want to filter() the data, using the skills you learnt last week. You may also want to center your data on the year 2001, so that the intercept (e.g., at time 0) in your models is interpretable. Then think about what test you can do to answer the questions above. Lastly, passing your model to the effect() function (from the effects package) can give you all the data you need to construct your plot. Hint: Try comparing models with and without different differences in/slopes of suicide rates for each region. Think also about the grouping of the data points, and how this is represented in your random effect structure. ► Solution Loading load(url(&quot;https://edin.ac/36xdhas&quot;)) #load Public Health England data unique(mh_phe[, 1:2]) #check list of mental health indicators: suicide is 41001 ## IndicatorID IndicatorName ## 1 848 Depression: Recorded prevalence (aged 18+) ## 760 41001 Suicide rate ## 8056 90275 Percentage of physically active adults - historical method ## 8664 90646 Depression: QOF incidence (18+) - new diagnosis # select data and shift Year variable so baseline year (2001) is 0 suicide_dat &lt;- filter(mh_phe, IndicatorID == 41001) %&gt;% mutate(Time = Year - 2001) Modelling # base model: just change over time m &lt;- lmer(Value ~ Time + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # add differences between regions m.0 &lt;- lmer(Value ~ Time + Region + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # add slope differences between regions m.1 &lt;- lmer(Value ~ Time * Region + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # compare models anova(m, m.0, m.1) ## Data: suicide_dat ## Models: ## m: Value ~ Time + (Time | County) ## m.0: Value ~ Time + Region + (Time | County) ## m.1: Value ~ Time * Region + (Time | County) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m 6 39961 40002 -19974 39949 ## m.0 14 39924 40019 -19948 39896 53.136 8 1.015e-08 *** ## m.1 22 39918 40068 -19937 39874 21.277 8 0.006447 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It looks like regions differ overall in their suicide rates (addition of Region predictor in m.0) and in slope of change (addition of interaction in m.1) Plotting # you can extract the fitted values using the effect() function from the effects package. ef &lt;- as.data.frame(effect(&quot;Time:Region&quot;, m.1)) ggplot(ef, aes(Time, fit, color=Region)) + geom_line() + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + labs(y=&quot;Fitted values&quot;) 2.2.2 Exercise 2 The weight maintenance data (WeightMaintain3), a made-up data set based on Lowe et al. (2014, Obesity, 22, 94-100), contains information on overweight participants who completed a 12-week weight loss program, and were then randomly assigned to one of three weight maintenance conditions: None (Control) MR (meal replacements): use MR to replace one meal and snack per day ED (energy density intervention): book and educational materials on purchasing and preparing foods lower in ED (reducing fat content and/or increasing water content of foods) Weight was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post. load(url(&quot;https://edin.ac/2tFIedK&quot;)) summary(WeightMaintain3) ## ID Condition Assessment WeightChange ## 101 : 4 None:240 Min. :0.00 Min. :-8.3781 ## 102 : 4 ED :240 1st Qu.:0.75 1st Qu.:-0.5024 ## 103 : 4 MR :240 Median :1.50 Median : 0.7050 ## 104 : 4 Mean :1.50 Mean : 1.4438 ## 105 : 4 3rd Qu.:2.25 3rd Qu.: 2.8806 ## 106 : 4 Max. :3.00 Max. :14.9449 ## (Other):696 ► Question 2 Overall, did the participants maintain their weight loss or did their weights change? ► Solution m.null &lt;- lmer(WeightChange ~ 1 + (Assessment | ID), data=WeightMaintain3, REML=F) m.base &lt;- lmer(WeightChange ~ Assessment + (Assessment | ID), data=WeightMaintain3, REML=F) anova(m.null, m.base) ## Data: WeightMaintain3 ## Models: ## m.null: WeightChange ~ 1 + (Assessment | ID) ## m.base: WeightChange ~ Assessment + (Assessment | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.null 5 2638.0 2660.9 -1314.0 2628.0 ## m.base 6 2579.4 2606.8 -1283.7 2567.4 60.66 1 6.782e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Weights changed over course of assessment period: \\(\\chi^2(1)=56.5, p &lt;&lt; 0.0001\\) ► Question 3 Did the groups differ in overall weight change and rate of weight gain (non-maintenance)? ► Solution m.int &lt;- lmer(WeightChange ~ Assessment + Condition + (Assessment | ID), data=WeightMaintain3, REML=F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) m.full &lt;- lmer(WeightChange ~ Assessment*Condition + (Assessment | ID), data=WeightMaintain3, REML=F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) anova(m.null, m.base, m.int, m.full) ## Data: WeightMaintain3 ## Models: ## m.null: WeightChange ~ 1 + (Assessment | ID) ## m.base: WeightChange ~ Assessment + (Assessment | ID) ## m.int: WeightChange ~ Assessment + Condition + (Assessment | ID) ## m.full: WeightChange ~ Assessment * Condition + (Assessment | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.null 5 2638.0 2660.9 -1314.0 2628.0 ## m.base 6 2579.4 2606.8 -1283.7 2567.4 60.6605 1 6.782e-15 *** ## m.int 8 2573.9 2610.6 -1279.0 2557.9 9.4418 2 0.008907 ** ## m.full 10 2537.5 2583.3 -1258.8 2517.5 40.3814 2 1.703e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes: Group difference: \\(\\chi^2(2)=9.4, p &lt; 0.01\\) Group slope difference: \\(\\chi^2(2)=40.4, p &lt;&lt; 0.0001\\) Note: m.int is difficult to interpret in light of the massive effect on slope coef(summary(m.full)) ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.06038642 0.09807901 517.4922 0.6156916 5.383688e-01 ## Assessment 1.84917936 0.18394628 180.4609 10.0528229 3.677753e-19 ## ConditionED -0.14303302 0.13870466 517.4922 -1.0312056 3.029261e-01 ## ConditionMR -0.14944649 0.13870466 517.4922 -1.0774439 2.817840e-01 ## Assessment:ConditionED -1.74949968 0.26013932 180.4609 -6.7252412 2.234561e-10 ## Assessment:ConditionMR -0.83624053 0.26013932 180.4609 -3.2145872 1.547766e-03 Compared to no intervention, weight (re)gain was 1.75 lbs/year slower for the ED intervention and 0.84 lbs/year slower for the MR intervention. Note that baseline weight difference parameters are not significantly different from 0. ► Question 4 Make a graph of the model fit and the observed data ► Solution There are lots of ways you can do this. Using the effect() function again (and then adding the means and SEs from the original data): ef &lt;- as.data.frame(effect(&quot;Assessment:Condition&quot;, m.full)) ggplot(ef, aes(Assessment, fit, color=Condition)) + geom_line() + stat_summary(data=WeightMaintain3, aes(y=WeightChange), fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) 2) Using the fitted() function to extract and plot fitted values from the model: ggplot(WeightMaintain3, aes(Assessment, WeightChange, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + stat_summary(aes(y=fitted(m.full)), fun=mean, geom=&quot;line&quot;) + theme_bw(base_size=12) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) Or, alternatively, using fortify(): ggplot(fortify(m.full), aes(Assessment, WeightChange, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + stat_summary(aes(y=.fitted), fun=mean, geom=&quot;line&quot;) + theme_bw(base_size=12) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) ► Question 5 Examine the parameter estimates and interpret them (i.e., what does each parameter represent?) ► Solution round(coef(summary(m.full)), 3) ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.060 0.098 517.492 0.616 0.538 ## Assessment 1.849 0.184 180.461 10.053 0.000 ## ConditionED -0.143 0.139 517.492 -1.031 0.303 ## ConditionMR -0.149 0.139 517.492 -1.077 0.282 ## Assessment:ConditionED -1.749 0.260 180.461 -6.725 0.000 ## Assessment:ConditionMR -0.836 0.260 180.461 -3.215 0.002 (Intercept) ==&gt; baseline weight change in None group Assessment ==&gt; slope of weight change in None group ConditionED ==&gt; baseline weight change in ED group relative to None group ConditionMR ==&gt; baseline weight change in MR group relative to None group Assessment:ConditionED ==&gt; slope of weight change in ED group relative to None group Assessment:ConditionMR ==&gt; slope of weight change in MR groups relative to None group 2.3 Logistic MLR 2.3.1 Exercise 3 load(url(&quot;https://edin.ac/2QwG7SG&quot;)) In the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test. Figure 2.1 shows the differences between groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up) Figure 2.1: Differences between groups in the average proportion of correct responses at each block Compare the two groups (those with anterior vs. posterior lesions) with respect to their responses. Tip: Remember that you can use cbind() to specify the numbers of successes and failures as the outcome in a binomial regression (and remember to specify the family argument in glmer()). ► Question 5 Is the learning rate (training data) different between these two groups? ► Solution m.base &lt;- glmer(cbind(NumCorrect, NumError) ~ block + (block | ID), data = filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) m.loc0 &lt;- glmer(cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID), data=filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) m.loc1 &lt;- glmer(cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID), data=filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) #summary(m.loc1) anova(m.base, m.loc0, m.loc1, test=&quot;Chisq&quot;) ## Data: filter(nwl, block &lt; 8, !is.na(lesion_location)) ## Models: ## m.base: cbind(NumCorrect, NumError) ~ block + (block | ID) ## m.loc0: cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ## m.loc0: ID) ## m.loc1: cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ## m.loc1: ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.base 5 454.12 466.27 -222.06 444.12 ## m.loc0 6 454.66 469.25 -221.33 442.66 1.4572 1 0.2274 ## m.loc1 7 454.47 471.48 -220.23 440.47 2.1974 1 0.1382 No significant difference in learning rate between groups: \\(\\chi^2(2)=2.2, p = 0.138\\) ► Question 6 Does immediate test performance differ between lesion location groups, and does their retention from immediate to follow-up test differ? ► Solution nwl_test &lt;- filter(nwl, block &gt; 7, !is.na(lesion_location)) %&gt;% mutate( Phase = fct_relevel(factor(Phase),&quot;Immediate&quot;) ) m.recall.loc &lt;- glmer(cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID), nwl_test,family=&quot;binomial&quot;) summary(m.recall.loc) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID) ## Data: nwl_test ## ## AIC BIC logLik deviance df.resid ## 142.6 150.9 -64.3 128.6 17 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.31708 -0.45014 0.03291 0.46924 1.09355 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 0.47660 0.6904 ## PhaseFollow-up 0.02539 0.1593 -1.00 ## Number of obs: 24, groups: ID, 12 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.11375 0.35128 -0.324 0.746 ## PhaseFollow-up -0.02452 0.24634 -0.100 0.921 ## lesion_locationposterior 0.99918 0.46868 2.132 0.033 * ## PhaseFollow-up:lesion_locationposterior -0.25437 0.33904 -0.750 0.453 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) PhsFl- lsn_lc ## PhaseFllw-p -0.578 ## lsn_lctnpst -0.750 0.435 ## PhsFllw-p:_ 0.421 -0.729 -0.589 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular (Intercept) ==&gt; Anterior lesion group performance in immediate test PhaseFollow-up ==&gt; Change in performance (anterior lesion group) from immediate to follow-up test lesion_locationposterior ==&gt; Posterior lesion group performance in immediate test relative to anterior lesion group performance in immediate test PhaseFollow-up:lesion_locationposterior ==&gt; Change in performance from immediate to follow-up test, posterior lesion group relative to anterior lesion group ► Question 6 Extra: Recreate the visualisation in Figure 2.1. ► Solution ggplot(filter(nwl, !is.na(lesion_location)), aes(block, PropCorrect, color=lesion_location, shape=lesion_location)) + #geom_line(aes(group=ID),alpha=.2) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(data=filter(nwl, !is.na(lesion_location), block &lt;= 7), fun=mean, geom=&quot;line&quot;) + geom_hline(yintercept=0.5, linetype=&quot;dashed&quot;) + geom_vline(xintercept=c(7.5, 8.5), linetype=&quot;dashed&quot;) + scale_x_continuous(breaks=1:9, labels=c(1:7, &quot;Test&quot;, &quot;Follow-Up&quot;)) + theme_bw(base_size=10) + labs(x=&quot;Block&quot;, y=&quot;Proportion Correct&quot;, shape=&quot;Lesion\\nLocation&quot;, color=&quot;Lesion\\nLocation&quot;) "],
["mlr-for-longitudinal-data-growth-curve-analysis.html", "Chapter 3 MLR for longitudinal data (growth curve analysis) 3.1 Introduction 3.2 Exercise 1 3.3 Exercise 2: Writing up, and Logistic GCA", " Chapter 3 MLR for longitudinal data (growth curve analysis) Packages lme4 tidyverse effects We will also be needing to access some useful functions from Dan for getting p-values and coding polynomials. The source() function basically takes in R code and evaluates it. You can download R scripts with Dan’s code here and here. However, you can also source them directly from the URLs, and read them into your environment: library(tidyverse) library(lme4) library(effects) source(&#39;https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R&#39;) source(&quot;https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R&quot;) Lecture Slides The lecture slides can be accessed here. The data for the lecture can be found at https://edin.ac/2TieJK0. Background &amp; Reading Curran et al., 2010 Winter &amp; Wieling, 2016 3.1 Introduction This week we are going to look at how we might use MLR to study longitudinal data. That is, data in which repeated measurements are taken over a continuous domain, with the potential for observations to be unevenly spaced, or missing at certain points, and which are likely to display non-linear patterns. The lab will focus on including higher-order polynomials in MLR to capture non-linearity. Solutions are available Solutions are already available for the first half of today’s lab. We encourage you to try working through the questions yourself before looking at the solutions. 3.2 Exercise 1 Use natural (not orthogonal) polynomials to analyze decline in performance of 30 individuals with probable Alzheimer’s disease on three different kinds of tasks - Memory, complex activities of daily living (ADL), and simple activities of daily living. ► Question 1 Read the data in to R from the following url: https://edin.ac/35Njwpl . The data is in .rda format. ► Solution load(url(&quot;https://edin.ac/35Njwpl&quot;)) summary(Az) ## Subject Time Task Performance ## 1 : 30 Min. : 1.0 cADL :300 Min. : 2.00 ## 2 : 30 1st Qu.: 3.0 sADL :300 1st Qu.:40.00 ## 3 : 30 Median : 5.5 Memory:300 Median :52.00 ## 4 : 30 Mean : 5.5 Mean :49.27 ## 5 : 30 3rd Qu.: 8.0 3rd Qu.:61.00 ## 6 : 30 Max. :10.0 Max. :85.00 ## (Other):720 ► Question 2 Plot the observed data (the performance over time for each type of task). ► Solution ggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + stat_summary(fun.data=mean_se, geom=&quot;ribbon&quot;, color=NA, alpha=0.5) + stat_summary(fun=mean, geom=&quot;line&quot;) ► Question 3 Why are natural polynomials more useful for these data? ► Solution Because it’s useful to know whether there are task differences at the starting baseline point ► Question 4 Fit the GCA model(s). Steps required: Add 1st and 2nd order natural polynomials to the data using the code_poly() function. Create a baseline model, in which performance varies over time, but no differences in Task are estimated. Think about random effect structure - what are the observations grouped by? Are observations nested? Create a new model with a fixed effect of Task Create a new model in which performance varies linearly over time between Task type. Create a new model in which linear and quadratic performance over time varies between Task type. Run model comparisons. ► Solution # prep for analysis Az &lt;- code_poly(Az, predictor=&quot;Time&quot;, poly.order=2, orthogonal=F, draw.poly = F) # fit the full model incrementally m.base &lt;- lmer(Performance ~ (poly1 + poly2) + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.0 &lt;- lmer(Performance ~ (poly1 + poly2) + Task + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.1 &lt;- lmer(Performance ~ poly1*Task + poly2 + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.Az.full &lt;- lmer(Performance ~ (poly1 + poly2)*Task + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) anova(m.base, m.0, m.1, m.Az.full) ## Data: Az ## Models: ## m.base: Performance ~ (poly1 + poly2) + (poly1 + poly2 | Subject) + (poly1 + ## m.base: poly2 | Subject:Task) ## m.0: Performance ~ (poly1 + poly2) + Task + (poly1 + poly2 | Subject) + ## m.0: (poly1 + poly2 | Subject:Task) ## m.1: Performance ~ poly1 * Task + poly2 + (poly1 + poly2 | Subject) + ## m.1: (poly1 + poly2 | Subject:Task) ## m.Az.full: Performance ~ (poly1 + poly2) * Task + (poly1 + poly2 | Subject) + ## m.Az.full: (poly1 + poly2 | Subject:Task) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.base 16 3804.6 3881.5 -1886.3 3772.6 ## m.0 18 3807.8 3894.2 -1885.9 3771.8 0.8242 2 0.6623 ## m.1 20 3781.4 3877.4 -1870.7 3741.4 30.4282 2 2.469e-07 *** ## m.Az.full 22 3612.9 3718.5 -1784.4 3568.9 172.4920 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In these solutions we are using Dan’s code_poly() function. All that this does is add polynomial terms to your dataframe (it can also give you a plot if you want one by setting draw.poly = TRUE). As always with R, you can do the same thing in lots of different ways. We could, for example, pass the poly() function directly to our model (without making storing them as variables in our dataframe), for instance: m.base &lt;- lmer(Performance ~ poly(Time, degree = 2, raw = TRUE) + (poly(Time, degree = 2, raw = TRUE) | Subject) + (poly(Time, degree = 2, raw = TRUE) | Subject:Task), data=Az, REML=F) If you’re struggling with the idea of polynomials, try looking at the output of the following commands: # note, the &quot;raw&quot; here is the same as &quot;natural&quot;. poly(1:10, 3, raw = TRUE) poly(1:10, 3, raw = FALSE) # let&#39;s plot them quickly using matplot() matplot(poly(1:10, 3, raw = TRUE), type = &#39;l&#39;) matplot(poly(1:10, 3, raw = FALSE), type = &#39;l&#39;) Interpret the results of your full model ► Question 5 Look at the summary() of your full model, and try using the get_pvalues() function on it. Which terms show significant effects of experimental factors? ► Solution Get p-values for your full model: get_pvalues(m.Az.full) ## Estimate Std..Error df t.value Pr...t.. p.normal p.normal.star ## (Intercept) 67.161666667 0.9307397 49.93913 72.1594510 3.604254e-52 0.000000e+00 *** ## poly1 -3.287803030 0.3417915 45.46197 -9.6193228 1.556027e-12 0.000000e+00 *** ## poly2 0.009469697 0.0124348 89.99977 0.7615478 4.483209e-01 4.463300e-01 ## TasksADL 0.095000000 0.8117243 60.06639 0.1170348 9.072229e-01 9.068325e-01 ## TaskMemory 1.240000000 0.8117243 60.06639 1.5276124 1.318580e-01 1.266088e-01 ## poly1:TasksADL 1.362196970 0.2675361 61.08255 5.0916376 3.654585e-06 3.549840e-07 *** ## poly1:TaskMemory -3.977070707 0.2675361 61.08255 -14.8655469 5.829334e-22 0.000000e+00 *** ## poly2:TasksADL -0.013257576 0.0174789 89.67383 -0.7584903 4.501465e-01 4.481575e-01 ## poly2:TaskMemory 0.338888889 0.0174789 89.67383 19.3884563 7.876776e-34 0.000000e+00 *** Intercepts are not different: performance in all tasks starts out the same (thanks, natural polynomials) Linear slopes are different: compared to complex ADL tasks, decline in simple ADL tasks is slower and decline in Memory is faster. Quadratic term is different for Memory: decline in cADL and sADL tasks is approximately linear, decline in Memory has more curvature (reaching floor?) ► Question 6 To what extent do model comparisons and the parameter-specific p-values yield the same results? ► Solution Model comparisons suggest: Linear slopes are different: \\(\\chi^2(2)=30.56, p &lt;&lt; 0.0001\\) (comparison m.0 and m.1 above). Quadratic term is different: \\(\\chi^2(2)=172.36, p &lt;&lt; 0.0001\\) (comparison m.1 and m.Az.full above). Note: We can’t investigate the intercept difference via the model comparisons above. Comparison between m.base and m.0 indicates difference holding polynomial terms constant (not the conditional effect where poly1 and poly2 are 0).** Please note: There was an error in last week’s lab about assessing ‘baseline’ differences between groups. Model comparison between y~time and y~time+g does not assess the difference in y between groups in g where time==0, but instead assesses the difference in y between groups in g holding time constant. Last week’s lab has been amended. Apologies for any confusion! ► Question 7 Plot model fit ► Solution ggplot(Az, aes(Time, Performance, color=Task)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(fun=mean, geom=&quot;line&quot;, aes(y=fitted(m.Az.full))) 3.3 Exercise 2: Writing up, and Logistic GCA We saw in the lecture a walk-through of using GCA to model the eye-tracking data from a spoken word-to-picture matching task. ► Question 8 The model we saw in the lecture had the following structure and results: &gt; m.full &lt;- lmer(meanFix ~ (poly1+poly2+poly3)*Condition + (poly1+poly2+poly3 | Subject) + (poly1+poly2 | Subject:Condition), control = lmerControl(optimizer=&quot;bobyqa&quot;), data=TargetFix, REML=F) &gt; coef(summary(m.full)) Estimate Std. Error t value (Intercept) 0.4773227513 0.01385240 34.457775306 poly1 0.6385603705 0.05993519 10.654181583 poly2 -0.1095979256 0.03848819 -2.847573180 poly3 -0.0932611870 0.02041640 -4.567955536 ConditionLow -0.0581122429 0.01901291 -3.056462582 poly1:ConditionLow 0.0003188189 0.06330556 0.005036191 poly2:ConditionLow 0.1635455113 0.05426498 3.013831365 poly3:ConditionLow -0.0020869051 0.02014728 -0.103582452 Write up the results of the model ► Solution There are two rules of thumb for reporting growth curve analysis results: Clearly describe each of the three key components of the model: the functional form (third-order orthogonal polynomial), the fixed effects (effect of Condition on all time terms), and the random effects (effect of Subject on each of the time terms and nested effects of Subject-by-Condition on each of the time terms except the cubic). Depending on the circumstances and complexity of the model, you may want to include additional information about the factors and why they were included or not. It’s also a good idea to report which method was used for computing p-values. For key findings, report parameter estimates and standard errors along with significance tests. In some cases the model comparison is going to be enough, but for key findings, the readers should want to see the parameter estimates. The parameter estimate standard errors are critical for interpreting the estimates, so those should be reported as well. The t-values are not critical to report (they are just Estimate divided by the Std Error, so they can always be computed from the reported estimates and standard errors). If there are many estimated parameters, it may be a good idea to focus the main text discussion on the most important ones and report the full set in a table or appendix. Here is how we might report the results from the example above: [Note, we haven’t included Table 1 here. If you want a nice way of creating tables, try installing the sjPlot package, and using tab_model() on your model.] Growth curve analysis (Mirman, 2014) was used to analyze the target gaze data from 300ms to 1000ms after word onset. The overall time course of target fixations was modeled with a third-order (cubic) orthogonal polynomial and fixed effects of Condition (Low vs. High frequency; within-participants) on all time terms. The model also included participant random effects on all time terms and participant-by-condition random effects on all time terms except the cubic (estimating random effects is “expensive” in terms of the number of observation required, so this cubic term was excluded because it tends to capture less-relevant effects in the tails). There was a significant effect of Condition on the intercept term, indicating lower overall target fixation proportions for the Low condition relative to the High condition (Estimate = -0.058, SE = 0.019, p &lt; 0.01). There was also a significant effect on the quadratic term, indicating shallower curvature - slower word recognition - in the Low condition relative to the High condition (Estimate = 0.16, SE = 0.054, p &lt; 0.01). All other effects of Condition were not significant (see Table 1 for full results). ► Question 9 Above, we analysed the proportion of fixations to the target picture in a given 50~ms time bin (the meanFix variable). We can express this differently, in terms of the number of samples in each 50~ms bin in which there were fixations to the target, and the total number of samples. This can lend itself to being modelled as a binomial (where success is fixation on the target). In the data, the sumFix variable contains the number of samples in which the target was fixated upon, and the N variable contains the total number of samples in that bin. Like we saw last week, we can model a binomial using cbind(num_successes, num_failures), so here we can use cbind(sumFix, N-sumFix)~ ... Re-analyze TargetFix data using logistic GCA. The data (.rda format) is available at https://edin.ac/2TieJK0 ► Solution load(url(&quot;https://edin.ac/2TieJK0&quot;)) #make 3rd-order orth poly TargetFix &lt;- code_poly(TargetFix, predictor=&quot;timeBin&quot;, poly.order=3, draw.poly=F) # fit logisitc GCA model m.log &lt;- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition + (poly1+poly2+poly3 | Subject) + (poly1+poly2 | Subject:Condition), data=TargetFix, family=binomial, control = glmerControl(optimizer = &quot;bobyqa&quot;)) summary(m.log) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(sumFix, N - sumFix) ~ (poly1 + poly2 + poly3) * Condition + ## (poly1 + poly2 + poly3 | Subject) + (poly1 + poly2 | Subject:Condition) ## Data: TargetFix ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 1419.1 1508.0 -685.6 1371.1 276 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.75430 -0.40973 -0.00307 0.37868 2.06240 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject:Condition (Intercept) 0.032340 0.17983 ## poly1 0.401864 0.63393 -0.68 ## poly2 0.147989 0.38469 -0.23 0.73 ## Subject (Intercept) 0.001751 0.04185 ## poly1 0.343612 0.58618 1.00 ## poly2 0.001991 0.04462 -1.00 -1.00 ## poly3 0.027493 0.16581 -1.00 -1.00 1.00 ## Number of obs: 300, groups: Subject:Condition, 20; Subject, 10 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.11675 0.06548 -1.783 0.074591 . ## poly1 2.81834 0.29834 9.447 &lt; 2e-16 *** ## poly2 -0.55911 0.16952 -3.298 0.000973 *** ## poly3 -0.32075 0.12771 -2.512 0.012017 * ## ConditionLow -0.26157 0.09095 -2.876 0.004030 ** ## poly1:ConditionLow 0.06400 0.33134 0.193 0.846849 ## poly2:ConditionLow 0.69503 0.23977 2.899 0.003747 ** ## poly3:ConditionLow -0.07066 0.16617 -0.425 0.670684 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) poly1 poly2 poly3 CndtnL pl1:CL pl2:CL ## poly1 -0.288 ## poly2 -0.128 0.272 ## poly3 -0.100 -0.228 -0.015 ## ConditionLw -0.690 0.297 0.081 0.012 ## ply1:CndtnL 0.372 -0.552 -0.292 -0.024 -0.541 ## ply2:CndtnL 0.080 -0.230 -0.701 0.034 -0.116 0.415 ## ply3:CndtnL 0.013 -0.020 0.037 -0.637 -0.003 0.031 -0.056 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Simpler random effects: note that the correlations between Subject-level random effects are all +1.00 or -1.00, so can simplify the structure by removing them: m.log_zc &lt;- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition + (poly1+poly2+poly3 || Subject) + (poly1+poly2 | Subject:Condition), data=TargetFix, family=binomial, control = glmerControl(optimizer = &quot;bobyqa&quot;)) summary(m.log_zc) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(sumFix, N - sumFix) ~ (poly1 + poly2 + poly3) * Condition + ## (poly1 + poly2 + poly3 || Subject) + (poly1 + poly2 | Subject:Condition) ## Data: TargetFix ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 1411.6 1478.3 -687.8 1375.6 282 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.69598 -0.41491 -0.00141 0.33691 2.07563 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject.Condition (Intercept) 0.03404 0.1845 ## poly1 0.42307 0.6504 -0.63 ## poly2 0.15312 0.3913 -0.25 0.70 ## Subject poly3 0.00000 0.0000 ## Subject.1 poly2 0.00000 0.0000 ## Subject.2 poly1 0.44471 0.6669 ## Subject.3 (Intercept) 0.00000 0.0000 ## Number of obs: 300, groups: Subject:Condition, 20; Subject, 10 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.11770 0.06544 -1.798 0.07210 . ## poly1 2.82162 0.31822 8.867 &lt; 2e-16 *** ## poly2 -0.55892 0.17054 -3.277 0.00105 ** ## poly3 -0.31340 0.11646 -2.691 0.00712 ** ## ConditionLow -0.26066 0.09280 -2.809 0.00497 ** ## poly1:ConditionLow 0.06593 0.33782 0.195 0.84526 ## poly2:ConditionLow 0.69049 0.24206 2.853 0.00434 ** ## poly3:ConditionLow -0.06654 0.16627 -0.400 0.68904 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) poly1 poly2 poly3 CndtnL pl1:CL pl2:CL ## poly1 -0.379 ## poly2 -0.129 0.301 ## poly3 -0.018 0.029 -0.054 ## ConditionLw -0.705 0.267 0.092 0.012 ## ply1:CndtnL 0.357 -0.528 -0.284 -0.027 -0.509 ## ply2:CndtnL 0.092 -0.212 -0.703 0.038 -0.131 0.402 ## ply3:CndtnL 0.012 -0.020 0.037 -0.699 -0.003 0.033 -0.056 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Plot model fit ggplot(TargetFix, aes(Time, meanFix, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(aes(y=fitted(m.log)), fun=mean, geom=&quot;line&quot;) + stat_summary(aes(y=fitted(m.log_zc)), fun=mean, geom=&quot;line&quot;, linetype=&quot;dashed&quot;) + theme_bw() + expand_limits(y=c(0,1)) + labs(y=&quot;Fixation Proportion&quot;, x=&quot;Time since word onset (ms)&quot;) "],
["other-random-effects-structures.html", "Chapter 4 Other random effects structures 4.1 Crossed random effects 4.2 Dealing with issue of non-convergence and singular fits", " Chapter 4 Other random effects structures Packages library(tidyverse) library(lme4) library(lmerTest) library(effects) Lecture Slides The lecture slides can be accessed here. Data from the lecture can be found at https://edin.ac/36UYA00 (active_math_sim.csv) and https://edin.ac/2ShRCNl (problem_solving.Rdata). Background &amp; Reading Baayen et al., 2008 Barr et al., 2013 Matuschek et al., 2017 4.1 Crossed random effects 4.1.1 Exercise 1 The data load(url(&quot;https://edin.ac/2ShRCNl&quot;)) str(problem_solving) ## &#39;data.frame&#39;: 2864 obs. of 5 variables: ## $ Item : Factor w/ 30 levels &quot;AA&quot;,&quot;AB&quot;,&quot;AC&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Prob_Type: Factor w/ 2 levels &quot;Hard&quot;,&quot;Easy&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;contrasts&quot;)= chr &quot;contr.sum&quot; ## $ Subject : Factor w/ 120 levels &quot;S101&quot;,&quot;S102&quot;,..: 119 25 1 102 65 105 56 53 84 86 ... ## $ Condition: Factor w/ 2 levels &quot;Control&quot;,&quot;Treatment&quot;: 2 1 1 1 2 2 1 1 1 2 ... ## ..- attr(*, &quot;contrasts&quot;)= chr &quot;contr.sum&quot; ## $ RT : int 3366 1584 2008 3089 1831 2863 3444 4102 2251 1950 ... Item: word problem, can be Hard or Easy Prob_Type: difficulty level of word problem (16 hard problems, 14 easy problems) Subject: Participant ID, N=120 Condition: whether the participant received the Treatment or not RT: time to solve the problem Note: there is some missing data because trials where the participants failed to solve the problem are excluded. ► Question 1 Check the contrasts of the Condition and Prob_Type variables. ► Solution contrasts(problem_solving$Condition) ## [,1] ## Control 1 ## Treatment -1 contrasts(problem_solving$Prob_Type) ## [,1] ## Hard 1 ## Easy -1 They are sum-coded. Remember what this means when you are interpreting the intercept and coefficients! ► Question 2 Does Prob_Type vary within subjects? Does Prob_Type vary within items? Does Condition vary within subjects? Does Condition vary within items? ► Solution Yes: Each subject solved both Easy and Hard problems. No: Each item was either Easy or Hard. No: Each subject was either in the control condition or the treatment condition. Yes: Each item was solved both by subjects in the control condition and subjects in the treatment condition. ► Question 3 Conduct MLR with crossed random effects of subjects and items to answer these research questions Did the treatment improve problem solving ability? Did the treatment effect differ between hard and easy problems? Hint: You can do this with one model, given what you know about the contrasts. ► Solution mod_ps &lt;- lmer(RT ~ Prob_Type*Condition + (Prob_Type | Subject) + (Condition | Item), data=problem_solving, REML=FALSE) summary(mod_ps) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: RT ~ Prob_Type * Condition + (Prob_Type | Subject) + (Condition | Item) ## Data: problem_solving ## ## AIC BIC logLik deviance df.resid ## 46245.1 46310.7 -23111.6 46223.1 2853 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.8354 -0.5429 -0.1105 0.3643 12.3008 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 74023 272.07 ## Prob_Type1 8912 94.40 1.00 ## Item (Intercept) 153298 391.53 ## Condition1 1391 37.29 1.00 ## Residual 541953 736.17 ## Number of obs: 2864, groups: Subject, 120; Item, 30 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1753.51 77.16 37.28 22.726 &lt; 2e-16 *** ## Prob_Type1 325.24 73.50 30.88 4.425 0.000111 *** ## Condition1 58.34 29.43 111.64 1.982 0.049891 * ## Prob_Type1:Condition1 36.14 17.73 86.76 2.038 0.044607 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Prb_T1 Cndtn1 ## Prob_Type1 -0.023 ## Condition1 0.217 -0.015 ## Prb_Typ1:C1 -0.024 0.379 0.398 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Because both Condition and Prob_Type are sum-coded, the intercept term is the grand mean. The Prob_Type1 coefficient corresponds to the mean for Prob_Type == \"Hard\" minus the grand mean. Note that sum-coding means that we no longer interpret this as the effect of Prob_Type at a reference level of Condition - instead it may help to think of it as the effect of Prob_Type ‘when Condition == 0’. In the default coding, 0 corresponds to the reference level, but in sum-coding, where Control = 1 and Treatment = -1, what is 0? It’s mid-way between the two (it may help to think of sum-coding similar to what happens when we ‘center’ continuous variables). The Condition1 coefficient corresponds to the mean for Condition == \"Control\" minus the grand mean. The Prob_Type1:Condition1 coefficient corresponds to the effect of Prob_Type when Condition == \"Control\" relative to the effect of Prob_Type at the grand mean. Or, equivalently, the effect of Condition when Prob_Type == \"Hard\" relative to the effect of Condition at the grand mean. Treatment improves problem solving ability (slower reaction times in the control group across both easy and hard problems), \\(\\beta = 58.34, SE = 29.43, p = 0.0499\\) Treatment effect differs between easy and hard problems, with a greater effect for harder problems (greater difference in reaction times between control and treatment groups for hard problems), \\(\\beta = 36.14, SE = 17.73, p = 0.0446\\) ► Question 4 Make a graph of the condition means and variability based on your analysis model. Hint: start withas.data.frame(effect(........)), and pass to it the parameter you’re interested in and your model. ► Solution efx &lt;- as.data.frame(effect(&quot;Prob_Type:Condition&quot;, mod_ps)) ggplot(efx, aes(Condition, fit, color=Prob_Type, ymin=fit-se, ymax=fit+se)) + geom_pointrange() + theme_light() + scale_color_manual(values=c(&quot;lightgreen&quot;,&quot;tomato1&quot;)) + labs(y=&quot;Response Time&quot;, color=&quot;Problem\\nType&quot;) 4.2 Dealing with issue of non-convergence and singular fits Singular fits You may have noticed that a lot of our models over the last few weeks have been giving a warning: boundary (singular) fit: see ?isSingular. Up to now, we’ve been largely ignoring these warnings. However, in this exercise we’re going to look at how to deal with this issue. The warning is telling us that our model has resulted in a ‘singular fit’. Singular fits often indicate that the model is ‘overfitted’ - that is, the random effects structure which we have specified is too complex to be supported by the data. Perhaps the most intuitive advice would be remove the most complex part of the random effects structure (i.e. random slopes). This leads to a simpler model that is not over-fitted. Additionally, when variance estimates are very low for a specific random effect term, this indicates that the model is not estimating this parameter to differ much between the levels of your grouping variable. It might, in some experimental designs, be perfectly acceptable to remove this. A key point here is that when fitting a mixed model, we should think about how the data are generated. Asking yourself questions such as “do we have good reason to assume subjects might vary over time, or to assume that they will have different starting points (i.e., different intercepts)?” can help you in specifying your random effect structure You can read in depth about what this means by reading the help documentation for ?isSingular. For our purposes, a relevant section is copied below: …intercept-only models, or 2-dimensional random effects such as intercept+slope models, singularity is relatively easy to detect because it leads to random-effect variance estimates of (nearly) zero, or estimates of correlations that are (almost) exactly -1 or 1. Convergence warnings Issues of non-convergence can be caused by many things. If you’re model doesn’t converge, it does not necessarily mean the fit is incorrect, however it is is cause for concern, and should be addressed, else you may end up reporting inferences which do not hold. There are lots of different things which you could do which might help your model to converge. A select few are detailed below: double-check the model specification and the data adjust stopping (convergence) tolerances for the nonlinear optimizer, using the optCtrl argument to [g]lmerControl. (see ?convergence for convergence controls). center and scale continuous predictor variables (e.g. with scale) change the optimiser, or use allFit() to try the fit with all available optimizers. This will of course be slow, but is considered ‘the gold standard’; “if all optimizers converge to values that are practically equivalent, then we would consider the convergence warnings to be false positives.” 4.2.1 Exercise 2 The data An experiment was run to replicate “test-enhanced learning” (Roediger &amp; Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (StudyStudy), the other group studied the material once then did a test (StudyTest). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (Test_word). The critical (replication) prediction is that the StudyStudy group should perform somewhat better on the immediate recall test, but the StudyTest group will retain the material better and thus perform better on the 1-week follow-up test. load(url(&quot;https://edin.ac/2RWbl6g&quot;)) str(tel) ## &#39;data.frame&#39;: 17498 obs. of 5 variables: ## $ Subject_ID: chr &quot;StudyTest_L&quot; &quot;StudyTest_L&quot; &quot;StudyTest_L&quot; &quot;StudyTest_L&quot; ... ## $ Group : chr &quot;StudyTest&quot; &quot;StudyTest&quot; &quot;StudyTest&quot; &quot;StudyTest&quot; ... ## $ Delay : chr &quot;min&quot; &quot;week&quot; &quot;min&quot; &quot;min&quot; ... ## $ Test_word : chr &quot;van&quot; &quot;dinosaur&quot; &quot;typewriter&quot; &quot;chimney&quot; ... ## $ Correct : num 1 0 0 0 1 1 1 1 0 0 ... ► Question 5 Plot the data. Does it look like the effect was replicated? ► Solution You can make use of stat_summary() again! ggplot(tel, aes(Delay, Correct, col=Group)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;)+ theme_light() It’s more work, but some people might rather calculate the numbers and then plot them directly. It does just the same thing: tel %&gt;% group_by(Delay, Group) %&gt;% summarise( mean = mean(Correct), se = sd(Correct)/sqrt(n()) ) %&gt;% ggplot(., aes(x=Delay, col = Group)) + geom_pointrange(aes(y=mean, ymin=mean-se, ymax=mean+se))+ theme_light() + labs(y = &quot;Correct&quot;) That looks like test-enhanced learning to me! ► Question 6 Test the critical hypothesis using a mixed-effects model. Fit the maximal random effect structure supported by the experimental design. Some questions to consider: Item accuracy is a binary variable. What kind of model will you use? We can expect variability across subjects (some people are better at learning than others) and across items (some of the recall items are harder than others). How should this be represented in the random effects? ► Solution m &lt;- glmer(Correct ~ Delay*Group + (1 + Delay | Subject_ID) + (1 + Delay + Group | Test_word), data=tel, family=&quot;binomial&quot;, glmerControl(optimizer = &quot;bobyqa&quot;)) summary(m) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: Correct ~ Delay * Group + (1 + Delay | Subject_ID) + (1 + Delay + Group | Test_word) ## Data: tel ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 16289.8 16390.8 -8131.9 16263.8 17485 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -7.4352 -0.4778 0.2763 0.5182 7.5360 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Test_word (Intercept) 1.160006 1.07704 ## Delayweek 0.005543 0.07445 -0.80 ## GroupStudyTest 0.011905 0.10911 -0.93 0.97 ## Subject_ID (Intercept) 2.527104 1.58969 ## Delayweek 0.044399 0.21071 -0.60 ## Number of obs: 17498, groups: Test_word, 175; Subject_ID, 50 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.28215 0.33082 3.876 0.000106 *** ## Delayweek -1.07133 0.07068 -15.157 &lt; 2e-16 *** ## GroupStudyTest -0.42866 0.45344 -0.945 0.344480 ## Delayweek:GroupStudyTest 0.79433 0.10189 7.796 6.41e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Delywk GrpStT ## Delayweek -0.441 ## GropStdyTst -0.688 0.309 ## Dlywk:GrpST 0.292 -0.679 -0.430 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular ► Question 7 Your model with maximal random effects will probably not converge, or will obtain a singular fit. Simplify the model until you achieve convergence. What we’re aiming to do here is to follow Barr et al.’s advice of defining our maximal model and then removing only the terms to allow a non-singular fit. Note: This strategy - starting with the maximal random effects structure and removing terms until obtaining model convergence, is just one approach, and there are drawbacks (see Matuschek et al., 2017). There is no consensus on what approach is best (see ?isSingular). Tip: you can look at the variance estimates and correlations easily by using the VarCorr() function. What jumps out? Hint: Generalization over subjects could be considered more important than over items - if the estimated variance of slopes for Delay and Group by-items are comparatively small, it might be easier to remove them? ► Solution VarCorr(m) ## Groups Name Std.Dev. Corr ## Test_word (Intercept) 1.077036 ## Delayweek 0.074453 -0.803 ## GroupStudyTest 0.109112 -0.930 0.966 ## Subject_ID (Intercept) 1.589687 ## Delayweek 0.210710 -0.600 The by-item slope of Group seems to be quite highly correlated with other by-item terms. For now, we will just simply remove the term (however, we could - if we had theoretical justification - constrain our model so that there was 0 correlation) m2 &lt;- glmer(Correct ~ Delay*Group + (1 + Delay | Subject_ID) + (1 + Delay | Test_word), data=tel, family=&quot;binomial&quot;, glmerControl(optimizer = &quot;bobyqa&quot;)) VarCorr(m2) ## Groups Name Std.Dev. Corr ## Test_word (Intercept) 1.027503 ## Delayweek 0.055413 -1.000 ## Subject_ID (Intercept) 1.598988 ## Delayweek 0.208731 -0.599 It’s still a singular fit, and the Delay random slope by Test_word variance is extremely low and perfectly correlated with the intercept, let’s try removing that: m3 &lt;- glmer(Correct ~ Delay*Group + (1 + Delay | Subject_ID) + (1 | Test_word), data=tel, family=&quot;binomial&quot;, glmerControl(optimizer = &quot;bobyqa&quot;)) Hooray, the model converged! summary(m3) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: Correct ~ Delay * Group + (1 + Delay | Subject_ID) + (1 | Test_word) ## Data: tel ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 16285.3 16347.4 -8134.6 16269.3 17490 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -7.5190 -0.4795 0.2758 0.5199 8.0698 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Test_word (Intercept) 0.9961 0.9980 ## Subject_ID (Intercept) 2.5170 1.5865 ## Delayweek 0.0387 0.1967 -0.52 ## Number of obs: 17498, groups: Test_word, 175; Subject_ID, 50 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.25720 0.32837 3.829 0.000129 *** ## Delayweek -1.04684 0.06750 -15.510 &lt; 2e-16 *** ## GroupStudyTest -0.39822 0.45243 -0.880 0.378765 ## Delayweek:GroupStudyTest 0.77634 0.09916 7.829 4.91e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Delywk GrpStT ## Delayweek -0.370 ## GropStdyTst -0.687 0.267 ## Dlywk:GrpST 0.252 -0.679 -0.371 ► Question 8 Plot the model-estimated condition means and variability. Hint: This is very similar to question 4. ► Solution ef &lt;- as.data.frame(effect(&quot;Delay:Group&quot;, m3)) ggplot(ef, aes(Delay, fit, color=Group)) + geom_pointrange(aes(ymax=upper, ymin=lower), position=position_dodge(width = 0.2))+ theme_classic() # just for a change :) ► Question 9 What should we do with this information? How can we apply test-enhanced learning to learning R and statistics? ► Solution You’ll get the benefits of test-enhanced learning if you try yourself before looking at the solutions! If you don’t test yourself, you’re more likely to forget it in the long run. 4.2.2 Exercise 3 The data Made-up data from a RCT treatment study: 5 therapists randomly assigned participants to control or treatment group and monitored the participants’ performance over time. There was a baseline test, then 6 weeks of treatment, with test sessions every week (7 total sessions). load(url(&quot;https://edin.ac/2GPGgev&quot;)) summary(tx) ## group session therapist Score PID ## treatment:490 Min. :1 Length:945 Min. :0.1091 Length:945 ## control :455 1st Qu.:2 Class :character 1st Qu.:0.5149 Class :character ## Median :4 Mode :character Median :0.6164 Mode :character ## Mean :4 Mean :0.6315 ## 3rd Qu.:6 3rd Qu.:0.7427 ## Max. :7 Max. :1.2244 ► Question 10 Plot the data. Does it look like the treatment had an effect on the performance score? ► Solution ggplot(tx, aes(session, Score, color=group)) + stat_summary(fun.data = mean_se, geom=&quot;pointrange&quot;) + stat_smooth() + theme_classic() Just for fun, let’s add on the individual participant scores, and also make a plot for each therapist. ggplot(tx, aes(session, Score, color=group)) + stat_summary(fun.data = mean_se, geom=&quot;pointrange&quot;) + stat_smooth() + theme_classic() + geom_line(aes(group=PID), alpha=.2)+facet_wrap(~therapist) ► Question 11 Consider these questions when you’re designing your model(s) and use your answers to motivate your model design and interpretation of results: What are the levels of nesting? How should that be reflected in the random effect structure? What is the shape of change over time? Do you need polynomials to model this shape? If yes, what order polynomials? ► Solution There are repeated measures of participants (session). There are also repeated measures of therapists (each one treated many participants). Looks like linear change, don’t need polynomials. Good to know that there is no difference at baseline, so no need for orthogonal time. ► Question 12 Test whether the treatment had an effect using mixed-effects modeling. Specify the maximal model. ► Solution # start with maximal model m1 &lt;- lmer(Score ~ session * group + (1 + session | PID) + (1 + session | therapist), data=tx, REML=FALSE) summary(m1) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Score ~ session * group + (1 + session | PID) + (1 + session | therapist) ## Data: tx ## ## AIC BIC logLik deviance df.resid ## -1643.8 -1590.4 832.9 -1665.8 934 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.63292 -0.58800 0.01438 0.55505 2.87871 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## PID (Intercept) 1.373e-02 1.172e-01 ## session 9.848e-04 3.138e-02 -0.60 ## therapist (Intercept) 1.225e-09 3.500e-05 ## session 1.267e-11 3.560e-06 -1.00 ## Residual 5.368e-03 7.327e-02 ## Number of obs: 945, groups: PID, 135; therapist, 5 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.526849 0.015841 134.976208 33.258 &lt; 2e-16 *** ## session 0.033688 0.004100 134.987110 8.217 1.51e-13 *** ## groupcontrol 0.018136 0.022830 134.980597 0.794 0.428360 ## session:groupcontrol -0.020138 0.005908 134.988836 -3.408 0.000862 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) sessin grpcnt ## session -0.655 ## groupcontrl -0.694 0.454 ## sssn:grpcnt 0.454 -0.694 -0.655 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular ► Question 13 Try adjusting your model by removing random effects or correlations, examine the model again, and so on.. ► Solution VarCorr(m1) ## Groups Name Std.Dev. Corr ## PID (Intercept) 1.1718e-01 ## session 3.1382e-02 -0.601 ## therapist (Intercept) 3.5001e-05 ## session 3.5595e-06 -1.000 ## Residual 7.3269e-02 There’s a correlation of exactly -1 between the random intercepts and slopes for therapists, and the standard deviation estimate for session|therapist is pretty small. Let’s remove it. m2 &lt;- lmer(Score ~ session * group + (1 + session | PID) + (1 | therapist), data=tx, REML=FALSE) VarCorr(m2) ## Groups Name Std.Dev. Corr ## PID (Intercept) 0.11717 ## session 0.03138 -0.601 ## therapist (Intercept) 0.00000 ## Residual 0.07327 It now looks like estimates for random intercepts for therapists is now 0. If we remove this, our model finally is non-singular: m3 &lt;- lmer(Score ~ session * group + (1 + session | PID), data=tx, REML=FALSE) summary(m3) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Score ~ session * group + (1 + session | PID) ## Data: tx ## ## AIC BIC logLik deviance df.resid ## -1649.8 -1611.0 832.9 -1665.8 937 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.63284 -0.58803 0.01439 0.55505 2.87860 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## PID (Intercept) 0.0137278 0.11717 ## session 0.0009847 0.03138 -0.60 ## Residual 0.0053686 0.07327 ## Number of obs: 945, groups: PID, 135 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.526849 0.015840 135.024518 33.262 &lt; 2e-16 *** ## session 0.033688 0.004100 135.003868 8.217 1.51e-13 *** ## groupcontrol 0.018136 0.022827 135.024519 0.794 0.428305 ## session:groupcontrol -0.020138 0.005908 135.003868 -3.409 0.000861 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) sessin grpcnt ## session -0.655 ## groupcontrl -0.694 0.454 ## sssn:grpcnt 0.454 -0.694 -0.655 Lastly, it’s then a good idea to check that the parameter estimates and SE are not radically different across these models (they are virtually identical) summary(m1)$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.52684907 0.015841487 134.9762 33.2575521 6.821583e-67 ## session 0.03368821 0.004099740 134.9871 8.2171564 1.511024e-13 ## groupcontrol 0.01813605 0.022830001 134.9806 0.7943954 4.283598e-01 ## session:groupcontrol -0.02013829 0.005908354 134.9888 -3.4084434 8.616018e-04 summary(m2)$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.52684907 0.015840424 135.0051 33.2597829 6.637414e-67 ## session 0.03368821 0.004099570 135.0026 8.2174987 1.507284e-13 ## groupcontrol 0.01813605 0.022828481 135.0051 0.7944483 4.283288e-01 ## session:groupcontrol -0.02013829 0.005908109 135.0026 -3.4085852 8.611671e-04 summary(m3)$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.52684907 0.015839602 135.0245 33.2615092 6.511331e-67 ## session 0.03368821 0.004099593 135.0039 8.2174516 1.507605e-13 ## groupcontrol 0.01813605 0.022827296 135.0245 0.7944895 4.283047e-01 ## session:groupcontrol -0.02013829 0.005908142 135.0039 -3.4085656 8.612216e-04 ► EXTRA: Question 14 Try the code below to use the allFit() function to fit your final model with all the available optimizers. You might need to install the dfoptim package to get one of the optimizers If you have an older version of lme4, then allFit() might not be directly available, and you will need to run the following: source(system.file(\"utils\", \"allFit.R\", package=\"lme4\")). fits &lt;- allFit(yourmodel) summary(fits) "],
["recap-individual-differences.html", "Chapter 5 Recap &amp; Individual differences Mixed model recap 5.1 THE LAB 5.2 Exercise 2", " Chapter 5 Recap &amp; Individual differences Packages lme4 require(tidyverse) require(lme4) library(lmerTest) library(effects) Lecture Slides Lecture slides are available here Mixed model recap In a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals. Multi-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them. We can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes). The first part of this lab will guide you through a walkthrough of these concepts, before you tackle some exercises. Simple regression Formula: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) R command: lm(outcome ~ predictor, data = dataframe) Note: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +). Clustered (multi-level) data structures When our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this. If we separate out our data to show an individual plot for each subject, we can see how the fitted regression line from lm() is assumed to be the same for each subject. Random intercept models By including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters. Formula: Level 1: \\(Y_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + \\epsilon_{ij}\\) Level 2: \\(\\beta_{0j} = \\gamma_{00} + u_{0j}\\) Where the expected values of \\(u_0\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{u_0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed. We can now see that the \\(\\beta_0\\) estimate for a particular group \\(j\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(u_{0j}\\)). R command: lmer(outcome ~ predictor + (1 | grouping), data = dataframe) Notice how the fitted line of the random intercept model has an adjustment for each subject. Each subject’s line has been moved up or down accordingly. Shrinkage If you think about it, we might have done a similar thing with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept. However, the estimate of these models will be slightly different: Why? One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters. Random slopes Formula: Level 1: \\(Y_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + \\epsilon_{ij}\\) Level 2: \\(\\beta_{0j} = \\gamma_{00} + u_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + u_{1j}\\) Where the expected values of \\(u_0\\), \\(u_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{u_0}^2\\), \\(\\sigma_{u_1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed. As with the intercept \\(\\beta_0\\), the slope of the predictor \\(\\beta_1\\) is now modelled by a mean and a random effect for each group (\\(u_{1j}\\)). R command: lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe) Note: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part. Fixef(), Ranef, and Coef The plots below show the fitted values from each model for each subject: In the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_u\\). The standard deviation (and variance, which is \\(\\sigma_u^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function). In the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. ► Fixed effects We can extract the fixed effects using the fixef() function: fixef(random_intercept_model) ## (Intercept) x1 ## 413.3736136 -0.9135829 ► Random effects We can extract the deviations for each group from these fixed effect estimates using the ranef() function. ranef(random_intercept_model) ## $subject ## (Intercept) ## sub_308 -29.278163 ## sub_309 -6.830994 ## sub_310 8.492231 ## sub_330 78.673944 ## sub_331 84.713164 ## sub_332 82.639416 ## sub_333 91.245686 ## sub_334 -74.788101 ## sub_335 27.022779 ## sub_337 3.361008 ## sub_349 -93.706202 ## sub_350 -57.004012 ## sub_351 65.920694 ## sub_352 -33.857447 ## sub_369 -64.358442 ## sub_370 -78.286762 ## sub_371 70.669067 ## sub_372 -52.841464 ## sub_373 -119.756827 ## sub_374 97.970426 ## ## with conditional variances for &quot;subject&quot; ► Group-level coefficients We can also see the estimate for each subject directly, using the coef() function. These sometimes get referred to as “Best Linear Unbiased Estimates (BLUPS)”. coef(random_intercept_model) ## $subject ## (Intercept) x1 ## sub_308 384.0955 -0.9135829 ## sub_309 406.5426 -0.9135829 ## sub_310 421.8658 -0.9135829 ## sub_330 492.0476 -0.9135829 ## sub_331 498.0868 -0.9135829 ## sub_332 496.0130 -0.9135829 ## sub_333 504.6193 -0.9135829 ## sub_334 338.5855 -0.9135829 ## sub_335 440.3964 -0.9135829 ## sub_337 416.7346 -0.9135829 ## sub_349 319.6674 -0.9135829 ## sub_350 356.3696 -0.9135829 ## sub_351 479.2943 -0.9135829 ## sub_352 379.5162 -0.9135829 ## sub_369 349.0152 -0.9135829 ## sub_370 335.0869 -0.9135829 ## sub_371 484.0427 -0.9135829 ## sub_372 360.5321 -0.9135829 ## sub_373 293.6168 -0.9135829 ## sub_374 511.3440 -0.9135829 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; ► Plotting random effects The quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4. randoms &lt;- ranef(random_intercept_model, condVar=TRUE) dotplot.ranef.mer(randoms) ## $subject –&gt; More levels: Nested and Crossed random-effects The same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups). ► Question Consider the example where we have observations for each student in every class within a number of schools: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”? ► Solution No. The classes in one school are distinct from the classes in another even though they are named the same. The classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable. In R, we can specify this using: (1 | school) + (1 | class:school) or, more succinctly: (1 | school/class) ► Question Consider another example, where we administer the same set of tasks at multiple time-points for every participant. Are tasks nested within participants? ► Solution No - tasks are seen by multiple participants (and participants see multiple tasks). We could visualise this as the below: In the sense that these are not nested, they are crossed random effects. In R, we can specify this using: (1 | subject) + (1 | task) Nested vs Crossed Nested: Each group belongs uniquely to a higher-level group. Crossed: Not-nested. Note that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results. 5.1 THE LAB The data 44 participants across 4 groups (between-subjects) were tested 5 times (waves) in 11 domains. In each wave, participants received a score (on a 20-point scale) for each domain and a set of questions which were they answered either correctly or incorrectly. load(url(&quot;https://edin.ac/2Hd6V4Q&quot;)) summary(dat5) ## Anonymous_Subject_ID IndivDiff Wave Domain Correct Error Group ## Length:2011 Min. :39.30 Min. :1.000 Length:2011 Min. : 0.000 Min. :0.00000 Length:2011 ## Class :character 1st Qu.:69.20 1st Qu.:2.000 Class :character 1st Qu.: 4.000 1st Qu.:0.00000 Class :character ## Mode :character Median :79.70 Median :3.000 Mode :character Median : 8.000 Median :0.00000 Mode :character ## Mean :77.73 Mean :2.712 Mean : 9.904 Mean :0.06216 ## 3rd Qu.:88.10 3rd Qu.:4.000 3rd Qu.:12.000 3rd Qu.:0.00000 ## Max. :95.20 Max. :5.000 Max. :45.000 Max. :1.00000 ## NA&#39;s :1474 ## Score ## Min. : 0.0 ## 1st Qu.: 8.0 ## Median :14.0 ## Mean :12.2 ## 3rd Qu.:17.0 ## Max. :20.0 ## 5.1.1 Exercise 1 Research question Did the groups differ in overall performance? There are different ways to test this: use the 20-point score or the accuracy? Keep the domains separate or calculate an aggregate across all domains? Which way makes the most sense to you? ► Question 1 Make a plot that corresponds to the reseach question. Does it look like there’s a difference? ► Solution Lots of options for this one, here is one that shows Group and Domain differences: ggplot(dat5, aes(Domain, Score, color=Group)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + coord_flip() Looks like there are group differences and domain differences, but not much in the way of group-by-domain differences. ► Question 2 Use a mixed-effects model to test the difference. Will you use a linear or logistic model? What should the fixed(s) effect be? What should the random effect(s) be? We have observations clustered by subjects and by domains - are they nested? Tip: For now, forget about the longitudinal aspect to the data. ► Solution We’re interested in the amount to which Groups vary in their overall performance, so we want a fixed effect of Group. Subjects and Domains are not nested - each subject sees different domains, and each domain is seen by multiple subjects. # maximal model doesn&#39;t converge, removed random Group slopes for Domain mod_grp &lt;- lmer(Score ~ Group + (1 | Anonymous_Subject_ID) + (1 | Domain), data=dat5, REML=FALSE) summary(mod_grp) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Score ~ Group + (1 | Anonymous_Subject_ID) + (1 | Domain) ## Data: dat5 ## ## AIC BIC logLik deviance df.resid ## 10398.2 10437.5 -5192.1 10384.2 2004 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.0251 -0.4981 0.0639 0.6338 3.2779 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Anonymous_Subject_ID (Intercept) 19.486 4.414 ## Domain (Intercept) 1.064 1.031 ## Residual 9.122 3.020 ## Number of obs: 2011, groups: Anonymous_Subject_ID, 44; Domain, 11 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 15.832 1.121 50.067 14.122 &lt; 2e-16 *** ## GroupB -4.159 2.262 44.369 -1.839 0.0727 . ## GroupC -3.621 1.768 43.968 -2.048 0.0465 * ## GroupW -7.270 1.673 44.042 -4.345 8.09e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) GroupB GroupC ## GroupB -0.457 ## GroupC -0.585 0.290 ## GroupW -0.618 0.306 0.392 Yes, substantial Group differences: overall, group A does the best, group B is slightly behind, group C next, and group W does the worst. 5.2 Exercise 2 Research question Did performance change over time (across waves)? Did the groups differ in pattern of change? ► Question 3 Make a plot that corresponds to the reseach question. Does it look like there was a change? A group difference? ► Solution ggplot(dat5, aes(Wave, Score, color=Group, fill=Group)) + stat_summary(fun.data=mean_se, geom=&quot;ribbon&quot;, alpha=0.3, color=NA) + stat_summary(fun.y=mean, geom=&quot;line&quot;) Yes, looks like groups A, C, and W are improving, but group B is getting worse. ► Question 4 Use mixed-effects model(s) to test this. Hint: Fit a baseline model in which scores change over time (wave), then assess improvement in model fit due to inclusion of overall group effect and finally the interaction of group with time. ► Solution mod_wv &lt;- lmer(Score ~ Wave + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave | Domain), data=dat5, REML=FALSE, lmerControl(optimizer = &quot;bobyqa&quot;)) mod_wv_grp &lt;- lmer(Score ~ Wave+Group + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave | Domain), data=dat5, REML=FALSE, lmerControl(optimizer = &quot;bobyqa&quot;)) mod_wv_x_grp &lt;- lmer(Score ~ Wave*Group + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave | Domain), data=dat5, REML=FALSE, lmerControl(optimizer = &quot;bobyqa&quot;)) anova(mod_wv, mod_wv_grp, mod_wv_x_grp) ## Data: dat5 ## Models: ## mod_wv: Score ~ Wave + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave | ## mod_wv: Domain) ## mod_wv_grp: Score ~ Wave + Group + (1 + Wave | Anonymous_Subject_ID) + (1 + ## mod_wv_grp: Wave | Domain) ## mod_wv_x_grp: Score ~ Wave * Group + (1 + Wave | Anonymous_Subject_ID) + (1 + ## mod_wv_x_grp: Wave | Domain) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## mod_wv 9 9719.6 9770.1 -4850.8 9701.6 ## mod_wv_grp 12 9710.3 9777.6 -4843.1 9686.3 15.3456 3 0.001544 ** ## mod_wv_x_grp 15 9710.5 9794.6 -4840.2 9680.5 5.8105 3 0.121204 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod_wv_x_grp) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Score ~ Wave * Group + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave | Domain) ## Data: dat5 ## Control: lmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 9710.5 9794.6 -4840.2 9680.5 1996 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.0053 -0.5764 0.0049 0.6132 3.7519 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Anonymous_Subject_ID (Intercept) 22.36604 4.7293 ## Wave 0.74787 0.8648 -0.34 ## Domain (Intercept) 2.05332 1.4329 ## Wave 0.02189 0.1479 -0.99 ## Residual 6.06856 2.4634 ## Number of obs: 2011, groups: Anonymous_Subject_ID, 44; Domain, 11 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.77276 1.24928 52.57242 10.224 4.23e-14 *** ## Wave 1.25475 0.23886 44.74521 5.253 4.00e-06 *** ## GroupB -1.36480 2.47446 45.83802 -0.552 0.58393 ## GroupC -4.14669 1.91661 43.86581 -2.164 0.03599 * ## GroupW -6.31853 1.81665 44.18633 -3.478 0.00115 ** ## Wave:GroupB -1.14231 0.52920 50.14297 -2.159 0.03570 * ## Wave:GroupC -0.03687 0.36863 36.73490 -0.100 0.92087 ## Wave:GroupW -0.50887 0.35468 38.78415 -1.435 0.15937 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Wave GroupB GroupC GroupW Wv:GrB Wv:GrC ## Wave -0.412 ## GroupB -0.444 0.175 ## GroupC -0.574 0.226 0.290 ## GroupW -0.605 0.239 0.306 0.395 ## Wave:GroupB 0.157 -0.436 -0.389 -0.102 -0.108 ## Wave:GroupC 0.225 -0.625 -0.114 -0.366 -0.155 0.282 ## Wave:GroupW 0.234 -0.650 -0.118 -0.153 -0.370 0.293 0.421 ► Question 5 Plot the group-level data (see Question 3) and model fitted values for each group from your final model from Question 4. Hint: using fortify(model) or broom::augment(model) as your starting point will help. ► Solution ggplot(fortify(mod_wv_x_grp), aes(Wave, Score, color=Group)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(aes(y=.fitted), fun.y=mean, geom=&quot;line&quot;) We fit a linear model, but the model fit lines are not straight lines. Why is that? ► Question 6 Create individual subject plots for the data and the model’s fitted values. Will these show straight lines? Hint: make use of facet_wrap() to create a different panel for each level of a grouping variable. ► Solution ggplot(fortify(mod_wv_x_grp), aes(Wave, Score, color=Group)) + facet_wrap(~ Anonymous_Subject_ID) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(aes(y=.fitted), fun.y=mean, geom=&quot;line&quot;) The individual subject plots show linear fits, which is a better match to the model. But now we see the missing data – some participants only completed the first few waves. ► Question 7 Make a plot of the actual (linear) model prediction. Hint: Use the effect() function from the effects package. ► Solution ef &lt;- as.data.frame(effect(&quot;Wave:Group&quot;, mod_wv_x_grp)) ggplot(ef, aes(Wave, fit, color=Group, fill=Group)) + geom_ribbon(aes(ymax=fit+se, ymin=fit-se), color=NA, alpha=0.1) + geom_line() ► Question 8 What important things are different between the plot from question 7 and that from question 5? ► Solution Group B was not actually getting worse. The appearance that it was getting worse is an artifact of selective drop-out: there’s only a few people in this group and the better-performing ones only did the first few waves so they are not represented in the later waves, but the worse-performing ones are contributing to the later waves. The model estimates how the better-performing ones would have done in later waves based on their early-wave performance and the pattern of performance of other participants in the study. summary(mod_wv_x_grp)$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.77275565 1.2492775 52.57242 10.2241143 4.233953e-14 ## Wave 1.25474611 0.2388610 44.74521 5.2530381 3.998024e-06 ## GroupB -1.36479858 2.4744579 45.83802 -0.5515546 5.839325e-01 ## GroupC -4.14668919 1.9166107 43.86581 -2.1635532 3.598767e-02 ## GroupW -6.31853259 1.8166464 44.18633 -3.4781301 1.146411e-03 ## Wave:GroupB -1.14230833 0.5292011 50.14297 -2.1585523 3.569665e-02 ## Wave:GroupC -0.03687025 0.3686301 36.73490 -0.1000196 9.208726e-01 ## Wave:GroupW -0.50887203 0.3546769 38.78415 -1.4347482 1.593740e-01 Note that the Group A slope (coefficient for Wave) is 1.255 and, relative to that slope, the Group B slope is -1.142 (coefficient for Wave:GroupB). This means that the model-estimated slope for Group B is 0.112, which is very slightly positive, not strongly negative as appeared in the initial plots. One of the valuable things about mixed-effects (aka multilevel) modeling is that individual-level and group-level trajectories are estimated. This helps the model overcome missing data in a sensible way. In fact, MLM/MLR models are sometimes used for imputing missing data. However, one has to think carefully about why data are missing. Group B is small and it might just be a coincidence that the better-performing participants dropped out after the first few waves, which would make it easier to generalize the patterns to them. On the other hand, it might be the case that there is something about the study that makes better-performing members of Group B drop out, which should make us suspicious of generalizing to them. ► Question 9 Create a plot of the subject and domain random effects. Notice the pattern between the random intercept and random slope estimates for the 11 domains - what in our model is this pattern representing? ► Solution randoms &lt;- ranef(mod_wv_x_grp, condVar=TRUE) dotplot.ranef.mer(randoms) ## $Anonymous_Subject_ID ## ## $Domain Notice that the domains with the lower relative intercept tend to have a higher relative slope (and vice versa). This is the negative correlation between random intercepts and slopes for domain in our model: VarCorr(mod_wv_x_grp) ## Groups Name Std.Dev. Corr ## Anonymous_Subject_ID (Intercept) 4.72927 ## Wave 0.86479 -0.336 ## Domain (Intercept) 1.43294 ## Wave 0.14795 -0.993 ## Residual 2.46344 Try removing the correlation (hint: use the ||) to see what happens. Does it make sense that these would be correlated? (Answer: we don’t really know enough about the study, but it’s something to think about!) "],
["break-week.html", "Chapter 6 Break Week", " Chapter 6 Break Week "],
["efa-and-pca.html", "Chapter 7 EFA and PCA 7.1 Today’s Exercises", " Chapter 7 EFA and PCA Packages psych GPArotation car GGally (optional) Lecture Slides The lecture slides can be accessed here and here. 7.1 Today’s Exercises Background A researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours: Stealing Lying Skipping school Vandalism Breaking curfew Threatening others Bullying Spreading malicious rumours Using a weapon Fighting Your task is to use the dimension reduction techniques you learned about in the lecture to help inform how to organise the items she has developed into subscales ► Question 1 Load the psych package and read in the dataset from https://edin.ac/2Vk1BVU. The first column is clearly an ID column, and it is easiest just to discard this for when we are doing factor analysis. Create a correlation matrix for the items. Inspect the items to check their suitability for exploratory factor analysis. You can use a function such as corr.test(df) from the psych package to create the correlation matrix. You can check the factorability of the correlation matrix using KMO(df). You can check linearity of relations using scatterplotMatrix(df) (from the car package). If you add the argument diagonal=histogram You can view the histograms on the diagonals, allowing you to check univariate normality (which is usually a good enough proxy for multivariate normality). You can do the same using the ggpairs function from the GGally package. NOTE. df=dataframe ► Solution library(psych) df &lt;- read.csv(&quot;https://edin.ac/2Vk1BVU&quot;) # discard the first column df &lt;- df[,-1] corr.test(df) ## Call:corr.test(x = df) ## Correlation matrix ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## item1 1.00 0.59 0.49 0.48 0.60 0.17 0.30 0.32 0.26 0.20 ## item2 0.59 1.00 0.53 0.51 0.66 0.20 0.33 0.30 0.29 0.19 ## item3 0.49 0.53 1.00 0.49 0.55 0.15 0.25 0.24 0.25 0.15 ## item4 0.48 0.51 0.49 1.00 0.65 0.23 0.29 0.32 0.28 0.25 ## item5 0.60 0.66 0.55 0.65 1.00 0.21 0.30 0.29 0.27 0.21 ## item6 0.17 0.20 0.15 0.23 0.21 1.00 0.54 0.57 0.41 0.44 ## item7 0.30 0.33 0.25 0.29 0.30 0.54 1.00 0.83 0.61 0.58 ## item8 0.32 0.30 0.24 0.32 0.29 0.57 0.83 1.00 0.61 0.59 ## item9 0.26 0.29 0.25 0.28 0.27 0.41 0.61 0.61 1.00 0.44 ## item10 0.20 0.19 0.15 0.25 0.21 0.44 0.58 0.59 0.44 1.00 ## Sample Size ## [1] 450 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## item1 0 0 0 0 0 0 0 0 0 0 ## item2 0 0 0 0 0 0 0 0 0 0 ## item3 0 0 0 0 0 0 0 0 0 0 ## item4 0 0 0 0 0 0 0 0 0 0 ## item5 0 0 0 0 0 0 0 0 0 0 ## item6 0 0 0 0 0 0 0 0 0 0 ## item7 0 0 0 0 0 0 0 0 0 0 ## item8 0 0 0 0 0 0 0 0 0 0 ## item9 0 0 0 0 0 0 0 0 0 0 ## item10 0 0 0 0 0 0 0 0 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option KMO(df) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = df) ## Overall MSA = 0.87 ## MSA for each item = ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## 0.90 0.88 0.92 0.88 0.84 0.94 0.82 0.81 0.95 0.94 car::scatterplotMatrix(df) or alternatively. library(GGally) ggpairs(data=df, diag=list(continuous=&quot;density&quot;), axisLabels=&quot;show&quot;) ► Question 2 How many dimensions should be retained? Use a scree plot, parallel analysis, and MAP test to guide you. ► Solution You can use fa.parallel(df) to conduct both parallel analysis and view a scree plot. fa.parallel(df) ## Parallel analysis suggests that the number of factors = 2 and the number of components = 2 In this case the scree plot has a kink at the third factor, so we probably want to retain 2 factors. We can conduct the MAP test using vss(df). vss(df) ## ## Very Simple Structure ## Call: vss(x = df) ## Although the VSS complexity 1 shows 7 factors, it is probably more reasonable to think about 2 factors ## VSS complexity 2 achieves a maximimum of 0.92 with 2 factors ## ## The Velicer MAP achieves a minimum of 0.03 with 2 factors ## BIC achieves a minimum of NA with 2 factors ## Sample Size adjusted BIC achieves a minimum of NA with 3 factors ## ## Statistics by number of factors ## vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex eChisq SRMR eCRMS eBIC ## 1 0.77 0.00 0.106 35 8.8e+02 3.6e-161 6.0 0.77 0.231 662 773 1.0 1.0e+03 1.6e-01 0.179 797 ## 2 0.78 0.92 0.034 26 4.0e+01 3.9e-02 2.1 0.92 0.035 -119 -36 1.1 1.4e+01 1.8e-02 0.024 -145 ## 3 0.78 0.91 0.058 18 1.2e+01 8.6e-01 1.9 0.93 0.000 -98 -41 1.2 3.4e+00 9.1e-03 0.014 -107 ## 4 0.63 0.90 0.104 11 6.1e+00 8.7e-01 1.4 0.95 0.000 -61 -26 1.3 1.6e+00 6.3e-03 0.013 -66 ## 5 0.69 0.87 0.149 5 2.4e+00 7.9e-01 1.6 0.94 0.000 -28 -12 1.3 7.5e-01 4.3e-03 0.013 -30 ## 6 0.71 0.89 0.252 0 1.2e-01 NA 1.4 0.95 NA NA NA 1.4 2.9e-02 8.4e-04 NA NA ## 7 0.79 0.89 0.397 -4 1.1e-07 NA 1.5 0.94 NA NA NA 1.3 2.2e-08 7.4e-07 NA NA ## 8 0.79 0.89 0.455 -7 1.6e-08 NA 1.5 0.94 NA NA NA 1.4 2.8e-09 2.6e-07 NA NA The MAP test suggests retaining 2 factors. ► Question 3 Having decided how many dimensions to retain in the previous question, conduct an EFA to extract this many factors, using a suitable rotation and extraction method. ► Solution You can use the fa() function from the psych package, for example, you could choose an oblimin rotation to allow factors to correlate and use minres as the extraction method. conduct_efa &lt;- fa(df, nfactors=2, rotate=&#39;oblimin&#39;, fm=&#39;minres&#39;) ► Question 4 Inspect the loadings and give the factors you extracted labels based on the patterns of loadings. Look back to the description of the items, and suggest a name for you factors ► Solution You can inspect the loadings using: conduct_efa$loadings ## ## Loadings: ## MR1 MR2 ## item1 0.706 ## item2 0.772 ## item3 0.681 ## item4 0.676 ## item5 0.872 ## item6 0.634 ## item7 0.890 ## item8 0.924 ## item9 0.629 ## item10 0.669 ## ## MR1 MR2 ## SS loadings 2.90 2.784 ## Proportion Var 0.29 0.278 ## Cumulative Var 0.29 0.568 We can see that the first five items have high loadings for one factor and the second five items have high loadings for the other. The first five items all have in common that they are non-aggressive forms of conduct problems, while the last five items are all aggressive behaviours. We could, therefore, label our factors: ‘non-aggressive’ and ‘aggressive’ conduct problems. ► Question 5 How correlated are your factors? ► Solution We can inspect the factor correlations (if we used an oblique rotation) using: conduct_efa$Phi ## MR1 MR2 ## MR1 1.0000000 0.4299169 ## MR2 0.4299169 1.0000000 We can see here that there is a moderate correlation between the two factors. An oblique rotation would be appropriate here. ► Question 6 Drawing on your previous answers and conducting any additional analyses you believe would be necessary to identify an optimal factor structure for the 10 conduct problems, write a brief text that summarises your method and the results from your chosen optimal model. ► Solution The main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description). An example summary would be: First, the data were checked for their suitability for factor analysis. Normality was checked using visual inspection of histograms, linearity was checked through the inspection of the linear and lowess lines for the pairwise relations of the variables, and factorability was confirmed using a KMO test, which yielded an overall KMO of \\(.87\\) with no variable KMOs \\(&lt;.50\\). An exploratory factor analysis was conducted to inform the structure of a new conduct problems test. Inspection of a scree plot alongside parallel analysis (using principal components analysis; PA-PCA) and the MAP test were used to guide the number of factors to retain. All three methods suggested retaining two factors; however, a one-factor and three-factor solution were inspected to confirm that the two-factor solution was optimal from a substantive and practical perspective, e.g., that it neither blurred important factor distinctions nor included a minor factor that would be better combined with the other in a one-factor solution. These factor analyses were conducted using minres extraction and (for the two- and three-factor solutions) an oblimin rotation, because it was expected that the factors would correlate. Inspection of the factor loadings and correlations reinforced that the two-factor solution was optimal: both factors were well-determined, including 5 loadings \\(&gt;|0.3|\\) and the one-factor model blurred the distinction between different forms of conduct problems. The factor loadings are provided in Table 7.11. Based on the pattern of factor loadings, the two factors were labelled ‘aggressive conduct problems’ and ‘non-aggressive conduct problems’. These factors had a correlation of \\(r=.43\\). Overall, they accounted for 52% of the variance in the items, suggesting that a two-factor solution effectively summarised the variation in the items. Table 7.1: Factor loadings. MR1 MR2 item1 0.71 item2 0.77 item3 0.68 item4 0.68 item5 0.87 item6 0.63 item7 0.89 item8 0.92 item9 0.63 item10 0.67 ► Question 7 Using the same data, conduct a PCA using the principal() function. What differences do you notice compared to your EFA? Do you think a PCA or an EFA is more appropriate in this particular case? ► Solution We can use: principal(df, nfactors=2) ## Principal Components Analysis ## Call: principal(r = df, nfactors = 2) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## item1 0.17 0.77 0.62 0.38 1.1 ## item2 0.17 0.81 0.68 0.32 1.1 ## item3 0.11 0.75 0.58 0.42 1.0 ## item4 0.21 0.74 0.60 0.40 1.2 ## item5 0.16 0.85 0.75 0.25 1.1 ## item6 0.73 0.08 0.53 0.47 1.0 ## item7 0.87 0.20 0.80 0.20 1.1 ## item8 0.88 0.19 0.82 0.18 1.1 ## item9 0.72 0.21 0.56 0.44 1.2 ## item10 0.75 0.09 0.57 0.43 1.0 ## ## RC1 RC2 ## SS loadings 3.29 3.22 ## Proportion Var 0.33 0.32 ## Cumulative Var 0.33 0.65 ## Proportion Explained 0.51 0.49 ## Cumulative Proportion 0.51 1.00 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.06 ## with the empirical chi square 166.43 with prob &lt; 1.9e-22 ## ## Fit based upon off diagonal values = 0.98 We can see that while the loadings differ somewhat between the EFA and the PCA, the overall pattern is quite similar. This is not always the case, especially when the item communalities are low. In terms of which method is more appropriate, arguably EFA would be more appropriate in this case because our researcher wishes to measure a theoretical construct (conduct problems), rather than simply reduce the dimensions of her data. You should provide the table of factor loadings. It is conventional to omit factor loadings \\(&lt;|0.3|\\); however, be sure to ensure that you mention this in a table note.↩ "],
["confirmatory-factor-analysis-cfa.html", "Chapter 8 Confirmatory Factor Analysis (CFA) Quick recap Data One factor model Two factor model", " Chapter 8 Confirmatory Factor Analysis (CFA) Packages lavaan Lecture Slides The lecture slides can be accessed here. Quick recap Last week we learned about two methods of data reduction: Principal Components Analysis (PCA) and Factor Analysis. In brief, PCA aims to summarise a set of measured variables into a set of uncorrelated (orthogonal) components, which are linear combinations (a weighted average) of the measured variables. Factor analysis, on the other hand, assumes that the relationships between a set of measured variables can be explained by a number of underlying latent factors. Note how the directions of the arrows in Figure 8.1 are different between PCA and FA - in PCA, each component \\(C_i\\) is the weighted combination of the observed variables \\(y_1, ...,y_n\\), whereas in FA, each measured variable \\(y_i\\) is seen as generated by some latent factor \\(F\\) plus some unexplained variance \\(u_i\\). Figure 8.1: Path diagrams for PCA and FA In Exploratory Factor Analysis (EFA), we tend to start with no hypothesis about either the number of latent factors or about the specific relationships between latent factors and measured variables (the factor structure). Typically, all variables will load on all factors, and a transformation method such as a rotation is used to help make the results more easily interpretable. When we have a clear hypothesis about relationships between measured variables and latent factors, we might want to impose a specific factor structure on the data. These sort of models are called Confirmatory Factor Analysis (CFA) models. The purpose of CFA can be seen of as twofold: Firstly to obtain parameter estimates (i.e., factor loadings, variances and covariances of factors, residual variances of measured variables), and secondly to assess whether the model provides a good fit to the data. Data Last week we conducted an exploratory factor analysis of a dataset to try and identify an optimal factor structure for a new measure of conduct (i.e., antisocial behavioural) problems. This week, we’ll conduct some confirmatory factor analyses (CFA) of the same inventory, using some new data collected by the researchers from n=600 adolescents. The questionnaire items referred to the following 10 behaviours: item 1- Stealing item 2- Lying item 3- Skipping school item 4- Vandalism item 5- Breaking curfew item 6- Threatening others item 7- Bullying item 8- Spreading malicious rumours item 9- Using a weapon item 10- Fighting ► Question 1 Begin by reading in the new data. It is available as a .csv at https://edin.ac/32MGsFx ► Solution df &lt;- read.csv(&quot;https://edin.ac/32MGsFx&quot;) One factor model ► Question 2a Using lavaan syntax, specify a model in which all 10 items load on one latent variable. Hint: To specify items loading on a latent variable we use the ‘=~’ operator. The latent variables goes on the left hand side and the list of indicators (i.e., items used to measure the latent variable) go on the right hand side separated by ‘+’ ► Solution model1&lt;-&#39;CP=~item1+item2+item3+item4+item5+item6+item7+item8+item9+item10&#39; ► Question 2b Why is it not necessary to refer to ALL of your CFA parameters in your model specification if you use the cfa() function to estimate your model? Which parameters do you not need to include in the model specification? ► Solution Some parameters are estimated or fixed by default when you estimate the model with the cfa() function. In this case, the residual factor variances and loadings are missing because the former are estimated by default and the latter are fixed to 1 by default. Latent factor variances are also estimated by default. ► Question 2c Estimate your model using the cfa()` function from the lavaan package. Scale your latent variable by fixing the latent variable variance to 1. Remember to load the lavaan library first. The default scaling constraint/identifcation constraints imposed when using the cfa() function are to fix the loading of the first item of each latent variable to 1. We can override this by setting std.lv=TRUE. This will instead scale the latent variables by fixing them to 1. It is helpful to save the results of cfa() to a new object so that we can later inspect that object (to look at the model fit and parameter estimates). ► Solution library(lavaan) model1.est&lt;-cfa(model1, data=df, std.lv=TRUE) ► Question 2d Now examine the global fit of your model. Does it fit well? Remember: If \\(\\textrm{RMSEA} &lt; .05\\), \\(\\textrm{SRMR} &lt; .05\\), \\(\\textrm{TLI} &gt; .95\\) and \\(\\textrm{CFI} &gt; .95\\) then the model fits well. Smaller values of RMSEA and SRMR mean better fit while larger values of CFI and TLI mean better fit. ► Solution To obtain the global fit measures, we can use the summary() function to inspect our estimated model, setting fit.measures=TRUE. summary(model1.est, fit.measures=TRUE) ## lavaan 0.6-5 ended normally after 45 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 20 ## ## Number of observations 600 ## ## Model Test User Model: ## ## Test statistic 1089.192 ## Degrees of freedom 35 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 2836.905 ## Degrees of freedom 45 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.622 ## Tucker-Lewis Index (TLI) 0.515 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7722.655 ## Loglikelihood unrestricted model (H1) -7178.058 ## ## Akaike (AIC) 15485.309 ## Bayesian (BIC) 15573.248 ## Sample-size adjusted Bayesian (BIC) 15509.753 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.224 ## 90 Percent confidence interval - lower 0.213 ## 90 Percent confidence interval - upper 0.236 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.181 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## CP =~ ## item1 0.378 0.042 9.036 0.000 ## item2 0.429 0.044 9.782 0.000 ## item3 0.327 0.042 7.794 0.000 ## item4 0.392 0.045 8.689 0.000 ## item5 0.410 0.043 9.522 0.000 ## item6 0.602 0.037 16.202 0.000 ## item7 0.879 0.033 26.448 0.000 ## item8 0.859 0.032 26.712 0.000 ## item9 0.680 0.038 18.001 0.000 ## item10 0.579 0.037 15.557 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .item1 0.885 0.052 17.038 0.000 ## .item2 0.963 0.057 16.984 0.000 ## .item3 0.909 0.053 17.114 0.000 ## .item4 1.037 0.061 17.061 0.000 ## .item5 0.933 0.055 17.004 0.000 ## .item6 0.581 0.036 16.213 0.000 ## .item7 0.224 0.021 10.721 0.000 ## .item8 0.202 0.019 10.379 0.000 ## .item9 0.559 0.035 15.843 0.000 ## .item10 0.596 0.037 16.324 0.000 ## CP 1.000 According to conventionally accepted criteria for RMSEA, SRMR, TLI and CFI, the model fits poorly. Two factor model ► Question 3a Now let’s try a different model. Specify a CFA model with two correlated latent factors. Consider items 1 to 5 as indicators of the first latent factor and items 6 to 10 as indicators of the second latent factor. ► Solution We can specify the model as below. model2&lt;-&#39;LV1=~item1+item2+item3+item4+item5 LV2=~item6+item7+item8+item9+item10 LV1~~LV2&#39; We now define two latent variables ‘Lv1’ and ‘Lv2’ using the ‘=~’ operator. We also introduce a new operator: ‘~~’ in order to specify the covariance between the two latent variables. ► Question 3b Estimate this model. Scale the latent variables using a reference indicator. Does the model fit well? ► Solution We can estimate the model using the cfa() function. As it uses a reference indicator to scale the latent variables by default we only need to specify the name of the object with the model syntax and the name of the dataset. model2.est&lt;-cfa(model2, data=df) summary(model2.est, fit.measures=T) ## lavaan 0.6-5 ended normally after 27 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 600 ## ## Model Test User Model: ## ## Test statistic 65.765 ## Degrees of freedom 34 ## P-value (Chi-square) 0.001 ## ## Model Test Baseline Model: ## ## Test statistic 2836.905 ## Degrees of freedom 45 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.989 ## Tucker-Lewis Index (TLI) 0.985 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7210.941 ## Loglikelihood unrestricted model (H1) -7178.058 ## ## Akaike (AIC) 14463.881 ## Bayesian (BIC) 14556.217 ## Sample-size adjusted Bayesian (BIC) 14489.548 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.039 ## 90 Percent confidence interval - lower 0.025 ## 90 Percent confidence interval - upper 0.054 ## P-value RMSEA &lt;= 0.05 0.884 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.035 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## LV1 =~ ## item1 1.000 ## item2 1.217 0.076 15.965 0.000 ## item3 1.012 0.070 14.424 0.000 ## item4 1.146 0.077 14.967 0.000 ## item5 1.360 0.078 17.412 0.000 ## LV2 =~ ## item6 1.000 ## item7 1.419 0.082 17.331 0.000 ## item8 1.437 0.081 17.662 0.000 ## item9 1.093 0.077 14.125 0.000 ## item10 0.950 0.073 13.073 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## LV1 ~~ ## LV2 0.156 0.023 6.856 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .item1 0.574 0.037 15.398 0.000 ## .item2 0.477 0.035 13.778 0.000 ## .item3 0.551 0.036 15.262 0.000 ## .item4 0.597 0.040 14.867 0.000 ## .item5 0.264 0.028 9.537 0.000 ## .item6 0.561 0.035 16.193 0.000 ## .item7 0.228 0.021 10.767 0.000 ## .item8 0.153 0.019 8.122 0.000 ## .item9 0.566 0.035 15.978 0.000 ## .item10 0.587 0.036 16.351 0.000 ## LV1 0.453 0.052 8.701 0.000 ## LV2 0.381 0.045 8.396 0.000 This model fits well according to RMSEA, SRMR, CFI and TLI. The \\(\\chi^2\\) value is significant but we don’t need to worry about this because the \\(\\chi^2\\) test has a strong tendency to reject even trivially mis-specified models. ► Question 3c Are there any areas of local mis-fit? ► Solution We can look for local mis-fit by using the modindices() function. It gives us the modification indices (the expected improvement in \\(\\chi^2\\) if a parameter was added) and the expected parameter change associated with the addition of the parameter (an estimate of what the parameter estimate would be if the parameter was included in the model). modindices(model2.est, sort=T) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 72 item6 ~~ item10 10.747 0.082 0.082 0.144 0.144 ## 25 LV1 =~ item7 8.106 0.119 0.080 0.080 0.080 ## 65 item5 ~~ item7 6.720 0.039 0.039 0.160 0.160 ## 71 item6 ~~ item9 6.675 -0.065 -0.065 -0.115 -0.115 ## 24 LV1 =~ item6 6.100 -0.136 -0.092 -0.094 -0.094 ## 33 LV2 =~ item5 5.273 -0.122 -0.075 -0.072 -0.072 ## 60 item4 ~~ item7 4.156 -0.039 -0.039 -0.105 -0.105 ## 27 LV1 =~ item9 4.086 0.113 0.076 0.075 0.075 ## 78 item9 ~~ item10 4.029 -0.051 -0.051 -0.089 -0.089 ## 44 item2 ~~ item4 4.021 -0.058 -0.058 -0.109 -0.109 ## 26 LV1 =~ item8 3.912 -0.078 -0.053 -0.054 -0.054 ## 66 item5 ~~ item8 3.702 -0.027 -0.027 -0.134 -0.134 ## 61 item4 ~~ item8 3.126 0.031 0.031 0.103 0.103 ## 29 LV2 =~ item1 2.920 0.105 0.065 0.064 0.064 ## 55 item3 ~~ item8 2.869 -0.028 -0.028 -0.098 -0.098 ## 64 item5 ~~ item6 2.812 -0.034 -0.034 -0.090 -0.090 ## 37 item1 ~~ item5 2.750 -0.044 -0.044 -0.114 -0.114 ## 70 item6 ~~ item8 2.726 0.034 0.034 0.115 0.115 ## 49 item2 ~~ item9 2.563 0.039 0.039 0.074 0.074 ## 54 item3 ~~ item7 2.295 0.027 0.027 0.077 0.077 ## 73 item7 ~~ item8 1.964 -0.041 -0.041 -0.222 -0.222 ## 30 LV2 =~ item2 1.869 0.081 0.050 0.046 0.046 ## 74 item7 ~~ item9 1.602 0.028 0.028 0.077 0.077 ## 69 item6 ~~ item7 1.395 -0.025 -0.025 -0.069 -0.069 ## 28 LV1 =~ item10 1.357 -0.065 -0.044 -0.046 -0.046 ## 68 item5 ~~ item10 1.356 -0.024 -0.024 -0.062 -0.062 ## 45 item2 ~~ item5 1.352 0.035 0.035 0.099 0.099 ## 34 item1 ~~ item2 1.322 0.031 0.031 0.060 0.060 ## 52 item3 ~~ item5 1.175 0.029 0.029 0.076 0.076 ## 43 item2 ~~ item3 1.120 -0.028 -0.028 -0.056 -0.056 ## 42 item1 ~~ item10 0.944 0.025 0.025 0.043 0.043 ## 76 item8 ~~ item9 0.731 0.018 0.018 0.062 0.062 ## 59 item4 ~~ item6 0.688 0.022 0.022 0.037 0.037 ## 67 item5 ~~ item9 0.672 -0.017 -0.017 -0.044 -0.044 ## 41 item1 ~~ item9 0.613 0.020 0.020 0.035 0.035 ## 36 item1 ~~ item4 0.604 0.022 0.022 0.038 0.038 ## 53 item3 ~~ item6 0.541 -0.018 -0.018 -0.033 -0.033 ## 35 item1 ~~ item3 0.528 -0.020 -0.020 -0.035 -0.035 ## 51 item3 ~~ item4 0.467 0.019 0.019 0.034 0.034 ## 56 item3 ~~ item9 0.373 0.015 0.015 0.027 0.027 ## 32 LV2 =~ item4 0.341 0.037 0.023 0.021 0.021 ## 58 item4 ~~ item5 0.241 0.014 0.014 0.036 0.036 ## 31 LV2 =~ item3 0.224 -0.029 -0.018 -0.017 -0.017 ## 40 item1 ~~ item8 0.185 0.007 0.007 0.025 0.025 ## 38 item1 ~~ item6 0.145 -0.010 -0.010 -0.017 -0.017 ## 50 item2 ~~ item10 0.082 -0.007 -0.007 -0.013 -0.013 ## 48 item2 ~~ item8 0.077 0.004 0.004 0.017 0.017 ## 62 item4 ~~ item9 0.057 0.006 0.006 0.011 0.011 ## 47 item2 ~~ item7 0.054 -0.004 -0.004 -0.012 -0.012 ## 75 item7 ~~ item10 0.048 -0.005 -0.005 -0.012 -0.012 ## 63 item4 ~~ item10 0.031 -0.005 -0.005 -0.008 -0.008 ## 57 item3 ~~ item10 0.024 0.004 0.004 0.007 0.007 ## 77 item8 ~~ item10 0.011 -0.002 -0.002 -0.007 -0.007 ## 46 item2 ~~ item6 0.007 0.002 0.002 0.004 0.004 ## 39 item1 ~~ item7 0.006 -0.001 -0.001 -0.004 -0.004 While the modification indices suggest that the model could be improved with the addition of parameters, none of the expected parameter changes are very large. If we included any additonal parameters here it is likely we would be capitalising on chance. We would also have to consider whether we could justify their inclusion on substantive grounds, i.e., we would ask ourselves ‘does the addition of the parameter makes sense given our background knowledge of the construct?’. ► Question 3d Take a look at the parameter estimates, are all of your loadings satisfactory? Which items are the best measures of the underlying latent variables? ► Solution It helps to look at the standardised parameter estimates: summary(model2.est, standardized=T) ## lavaan 0.6-5 ended normally after 27 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 600 ## ## Model Test User Model: ## ## Test statistic 65.765 ## Degrees of freedom 34 ## P-value (Chi-square) 0.001 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## LV1 =~ ## item1 1.000 0.673 0.664 ## item2 1.217 0.076 15.965 0.000 0.819 0.765 ## item3 1.012 0.070 14.424 0.000 0.681 0.676 ## item4 1.146 0.077 14.967 0.000 0.771 0.706 ## item5 1.360 0.078 17.412 0.000 0.915 0.872 ## LV2 =~ ## item6 1.000 0.618 0.636 ## item7 1.419 0.082 17.331 0.000 0.877 0.878 ## item8 1.437 0.081 17.662 0.000 0.887 0.915 ## item9 1.093 0.077 14.125 0.000 0.675 0.668 ## item10 0.950 0.073 13.073 0.000 0.587 0.608 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## LV1 ~~ ## LV2 0.156 0.023 6.856 0.000 0.375 0.375 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .item1 0.574 0.037 15.398 0.000 0.574 0.559 ## .item2 0.477 0.035 13.778 0.000 0.477 0.416 ## .item3 0.551 0.036 15.262 0.000 0.551 0.543 ## .item4 0.597 0.040 14.867 0.000 0.597 0.501 ## .item5 0.264 0.028 9.537 0.000 0.264 0.239 ## .item6 0.561 0.035 16.193 0.000 0.561 0.595 ## .item7 0.228 0.021 10.767 0.000 0.228 0.229 ## .item8 0.153 0.019 8.122 0.000 0.153 0.162 ## .item9 0.566 0.035 15.978 0.000 0.566 0.554 ## .item10 0.587 0.036 16.351 0.000 0.587 0.630 ## LV1 0.453 0.052 8.701 0.000 1.000 1.000 ## LV2 0.381 0.045 8.396 0.000 1.000 1.000 We can see that all loadings are statistically significant and all standardised loadings are &gt;|.3|. This is good as it suggests all our items measure the relevant latent variable reasonably well. We can identify the ‘best’ indicators of our latent constructs according to those with the highest standardised loadings. We must, however, be aware that item loadings are model-specific: if we changed the model (e.g., the number of factors or which items we included) the loadings could change and so would our interpretation of the latent variables. ► Question 3e Using R, represent the model, including the standardised parameters as an SEM diagram ► Solution We can use the semPaths() function from the semPlot package to create a SEM diagram from the estimated model object. We can include the unstandardised estimates by setting what='est' and the standardised estimates by setting what='stand'. library(semPlot) semPaths(model2.est, what=&#39;stand&#39;) semPaths(model1.est, what=&#39;stand&#39;) If we wanted to include a SEM diagram in a report the raw output from semPaths() would not usually meet publication standards. Instead, we can draw SEM diagrams in a programme such as powerpoint. ► Question 3f Write a short paragraph summarising the method and results of the two factor model. ► Solution The main principle behind reporting any analysis is that you should be as transparent as possible (e.g., reporting any model modifications made) and a reader should be able to reproduce your analysis based on your description. An example description would be: A two-factor model was tested. Items 1-5 loaded on a ‘non-aggressive conduct problems’ factor and items 6-10 loaded on an ‘aggression’ factor and these factors were allowed to correlate. Scaling and identification were achieved by fixing the loading of item 1 on the non-aggressive conduct problems factor and item 6 on the aggression factor to 1. The model was estimated using maximum likelihood estimation. The model fit well with CFI=.99, TLI=0.99, RMSEA=.04, and SRMR=.04 (Hu &amp; Bentler, 1999). All loadings were statistically significant and &gt;|.3| on the standardised scale. Modification indices and expected parameter changes were inspected but no modifications were made because no expected parameter changes were judged large enough to merit the inclusion of additional parameters given that there was little theoretical rationale for their inclusion. Overall, therefore, a two-factor oblique model was supported for the conduct problems items. The correlation between the factors was r=.38 (p&lt;.001). When you write up a CFA for a report, you should also include the parameter estimates. This can be in the form of a table with the unstandardised loadings and factor covariances, their standard errors, p-values, and corresponding standardised values. Note: according to APA style, you should include a leading zero when a parameter can take values out of the 0-1 range (e.g., SD=0.99) but you should not include a leading zero when a parameter can only take values in the 0-1 range (e.g., r=.5). This is why we have TLI=0.99 but CFI=.99. The former can technically (though rarely does) take values &gt;1 while the latter only takes values between 0 and 1. "],
["path-analysis.html", "Chapter 9 Path Analysis Quick recap Exercises", " Chapter 9 Path Analysis Packages library(lavaan) library(semPlot) Lecture Slides The lecture slides can be accessed here. Quick recap Over the last two weeks we applied exploratory and then confirmatory factor analysis to develop and then test a factor model for conduct problems. This week’s lab focuses on the technique of path analysis, which is used to test sets of regression paths simultaneously. In this week’s example, a researcher has collected data on n=557 adolescents and would like to know whether there are associations between conduct problems (both aggressive and non-aggressive) and academic performance and whether the relations are mediated by the quality of relationships with teachers. Exercises ► Question 1 First, read in the dataset: cp_teach_acad.csv. It is available at https://edin.ac/38yYz2P. Either download it and read it in locally, or read it directly from the url. ► Solution cp_teach_acad&lt;-read_csv(&quot;https://edin.ac/38yYz2P&quot;) ► Question 2a Use the sem() function in lavaan to specify and estimate a linear regression model to test whether aggressive and non-aggressive conduct problems significantly predict academic performance. How do your results compare to those you obtain using the lm() function? ► Solution # we can fit the model in lavaan as follows: # firt we specify the model using lavaan syntax sr_lavaan&lt;-&#39;Acad~Non_agg+Agg&#39; # next we can estimate the model using the sem() function sr_lavaan.est&lt;-sem(sr_lavaan, data=cp_teach_acad) # we can inspect the results using the summary() function summary(sr_lavaan.est) ## lavaan 0.6-5 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 3 ## ## Number of observations 557 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard errors Standard ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Acad ~ ## Non_agg 0.182 0.057 3.178 0.001 ## Agg 0.318 0.057 5.599 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Acad 0.943 0.057 16.688 0.000 # the same model can be fit using lm(): sr_lm&lt;-lm(Acad~Non_agg+Agg, data=cp_teach_acad) summary(sr_lm) ## ## Call: ## lm(formula = Acad ~ Non_agg + Agg, data = cp_teach_acad) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.89617 -0.59575 0.00731 0.62189 3.13248 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.04204 0.04135 -1.017 0.30973 ## Non_agg 0.18238 0.05755 3.169 0.00161 ** ## Agg 0.31809 0.05697 5.583 3.7e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9737 on 554 degrees of freedom ## Multiple R-squared: 0.1937, Adjusted R-squared: 0.1908 ## F-statistic: 66.54 on 2 and 554 DF, p-value: &lt; 2.2e-16 We can see that both non-aggressive and aggressive conduct problems significantly predict academic perfofmance.We can also see that we get the same results when we use the sem() function as we do when we use the lm() function. Lavaan will give essentially the same results as lm() for simple and multiple regression problems. However, if we have multiple outcome variables in our model it is advantageous to do this using path mediation model with lavaan. This allows us to include all the regressions in a single model. ► Question 2b Now specify a model in which non-aggressive conduct problems have both a direct and indirect effect (via teacher relationships) on academic performance ► Solution model1&lt;-&#39;Acad~Non_agg #we regress academic performance on non-aggressive conduct problems (the direct effect) Acad~Teach_r #we regress academic peformance on teacher relationship quality Teach_r~Non_agg #we regress teacher relationship quality on non-aggressive conduct problems&#39; ► Question 2c Now test the hypothesis that non-aggressive conduct problems have both a direct and an indirect effect (via teacher relationships) on academic performance ► Solution To test this we need to create a new parameter for the indirect effect and examine its 95% confidence interval. #model specification model1&lt;-&#39;Acad~Non_agg Acad~b*Teach_r #we label the two parameters that comprise the indirect effect b and c Teach_r~c*Non_agg # the indirect effect is the product of b and c ind:=b*c #we can create a new parameter (ind) to estimate the indirect effect&#39; #model estimation model1.est&lt;-sem(model1, data=cp_teach_acad,se=&#39;bootstrap&#39;) # we request bootstrapped standard errors to assess the signifance of the indirect effect summary(model1.est, ci=T) ## lavaan 0.6-5 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 5 ## ## Number of observations 557 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## Acad ~ ## Non_agg 0.158 0.056 2.793 0.005 0.048 0.277 ## Teach_r (b) 0.328 0.047 7.034 0.000 0.234 0.420 ## Teach_r ~ ## Non_agg (c) 0.769 0.034 22.666 0.000 0.703 0.836 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .Acad 0.919 0.058 15.970 0.000 0.805 1.036 ## .Teach_r 0.713 0.043 16.599 0.000 0.628 0.798 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ind 0.252 0.036 6.997 0.000 0.180 0.323 We can see that the 95% bootstrapped confidence interval for the indirect effect of non-aggressive conduct problems on academic performance (‘ind’) does not include zero. We can conclude that the indirect effect is significant at \\(p &lt;.05\\). The direct effect is also statistically significant at \\(p &lt; .05\\). ► Question 2d Calculate the total (direct+indirect) effect of non-aggressive conduct problems on academic performance ► Solution We can create a new parameter that is the sum of the direct and indirect effect to evaluate the total effect of non-aggressive conduct problems on academic performance. #model specification model1&lt;-&#39;Acad~a*Non_agg # we now also label the indirect effect of non-aggressive conduct problems on academic performance Acad~b*Teach_r Teach_r~c*Non_agg ind:=b*c #we can create a new parameter (ind) to estimate the indirect effect total:=b*c+a #the total effect is the indirect effect plus the direct effect&#39; #model estimation model1.est&lt;-sem(model1, data=cp_teach_acad,se=&#39;bootstrap&#39;) # we request bootstrapped standard errors to assess the signifance of the indirect effect summary(model1.est, ci=T) ## lavaan 0.6-5 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 5 ## ## Number of observations 557 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 999 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## Acad ~ ## Non_agg (a) 0.158 0.058 2.720 0.007 0.035 0.269 ## Teach_r (b) 0.328 0.049 6.685 0.000 0.230 0.430 ## Teach_r ~ ## Non_agg (c) 0.769 0.034 22.714 0.000 0.704 0.840 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .Acad 0.919 0.055 16.690 0.000 0.811 1.026 ## .Teach_r 0.713 0.043 16.398 0.000 0.626 0.799 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ind 0.252 0.038 6.696 0.000 0.178 0.327 ## total 0.410 0.042 9.762 0.000 0.325 0.490 ► Question 2e Now visualise the estimated model and its parameters using the semPaths() function from the semPlot package. ► Solution semPaths(model1.est, what=&#39;est&#39;) #to include the parameter estimates we set what=&#39;est&#39; We can use semPaths() to help us visualise and interpret the model; however, for reporting purposes we should create our own diagram using a programme such as powerpoint as the output from the semPaths() function would not be considered ‘publication quality’ ► Question 3a Now specify a model in which both aggressive and non-aggressive conduct problems have both direct and indirect effects (via teacher relationships) on academic performance. Include the parameters for the indirect effects. ► Solution model2&lt;- &#39;Acad~Agg+Non_agg+b*Teach_r Teach_r~c1*Agg+c2*Non_agg ind1:=b*c1 #indirect effect for aggressive conduct problems ind2:=b*c2 #indirect effect for non-aggressive conduct problems &#39; We now have two predictors, one mediator and one outcome (and two indirect effects, one for each predictor). We can represent this in two lines: one where we specify academic performance as the outcome variable and one where we specify teacher relationships (the mediator) as the outcome variable. ► Question 3b Now estimate the model and test the significance of the indirect effects ► Solution model2.est&lt;-sem(model2, data=cp_teach_acad,se=&#39;bootstrap&#39;) summary(model2.est, ci=T) ## lavaan 0.6-5 ended normally after 18 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 7 ## ## Number of observations 557 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## Acad ~ ## Agg 0.171 0.063 2.732 0.006 0.044 0.294 ## Non_agg 0.091 0.062 1.462 0.144 -0.034 0.210 ## Teach_r (b) 0.256 0.055 4.658 0.000 0.151 0.362 ## Teach_r ~ ## Agg (c1) 0.574 0.043 13.285 0.000 0.493 0.661 ## Non_agg (c2) 0.358 0.041 8.759 0.000 0.279 0.439 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .Acad 0.908 0.056 16.209 0.000 0.793 1.010 ## .Teach_r 0.540 0.030 17.911 0.000 0.478 0.593 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ind1 0.147 0.033 4.446 0.000 0.084 0.215 ## ind2 0.092 0.022 4.081 0.000 0.051 0.139 We can see that the 95% confidence intervals for both indirect effects do not include zero, therefore, we can conclude that they are significant at \\(p &lt; .05\\). ► Question 3c Write a brief paragraph reporting on the results of the model estimates in Question 3b. Include a Figure or Table to display the parameter estimates. ► Solution A path mediation model was used to test the direct and indirect effects (via teacher relationship quality) of aggressive and non-aggressive conduct problems on academic performance. In the model, academic performance was regressed on teacher relationship quality, non-aggressive conduct problems and aggressive conduct problems while teacher relationship quality (the mediator) was regressed on aggressive and non-aggressive conduct problems. The indirect effects were tested using the product of the coefficient for the regression of outcome on mediator and the coefficient for the regression of mediator on predictor. The statistical significance of the indirect effects were evaluated using boostrapped 95% confidence intervals with 1000 draws. Unstandardised parameter estimates are provided in Figure 1. Solid lines indicate that a parameter is significant at \\(p &lt;. 05\\), while dashed lines represent non-significant paths.The indirect effects of both non-aggressive (\\(b = 0.09\\), 95% CI=0.05-0.14) and aggressive (\\(b = 0.15\\), 95% CI=0.08-0.22) conduct problems on academic performance were statistcally significant. knitr::include_graphics(&quot;images/pathanaly.png&quot;) "],
["sem-i.html", "Chapter 10 SEM I Overview Exercises", " Chapter 10 SEM I Packages library(lavaan) library(semPlot) Lecture Slides The lecture slides can be accessed here. Overview Last week we tried our hand at path analysis, which is used to test a set of relations between observed variables. This week’s lab is focused on full structural equation modelling, which involves testing sets of relations between observed AND latent variables. In this week’s example, a researcher wants to apply the theory of planned behaviour to understand engagement in physical activity. The theory of planned behaviour is summarised in Figure 10.1 (only the latent variables and not the items are shown). Attitudes refer to the extent to which a person had a favourable view of exercising; subjective norms refer to whether they believe others whose opinions they care about believe exercise to be a good thing; and perceived behavioural control refers to the extent to which they believe exercising is under their control. Intentions refer to whether a person intends to exercise and ‘behaviour’ is a measure of the extent to which they exercised. Each construct is measured using four items. Figure 10.1: Theory of planned behaviour (latent variables only) The data is available either: as a .RData file here: https://edin.ac/39Uvn7R as a .txt file here: https://edin.ac/3d2svI9 Exercises ► Last week’s write-up Did you do the final exercise (writing up your results) from last week’s lab? Our example write-up is now visible (see last week’s lab) against which you can compare yours. Please note: Writing-up is an important skill, and attempting these questions yourself (rather than simply reading through the example write-ups) will help a lot when it comes to writing your assessment. ► Question 1 Read in the data using the appropriate function. Can you sucessfully read in from both types of data? ► Solution Either one or the other of: load(url(&quot;https://edin.ac/39Uvn7R&quot;)) TPB_data &lt;- read.table(&quot;https://edin.ac/3d2svI9&quot;, header = TRUE, sep = &quot;\\t&quot;) ► Question 2 Test a one-factor model for each construct. Are the measurement models satisfactory? ► Solution #specify one factor CFA model for attitudes attitude_model&lt;-&#39;att=~attitude1+attitude2+attitude3+attitude4&#39; #estimate the model attitude_model.est&lt;-cfa(attitude_model, data=TPB_data) #inspect the output summary(attitude_model.est, fit.measures=T, standardized=T) ## lavaan 0.6-5 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 8 ## ## Number of observations 890 ## ## Model Test User Model: ## ## Test statistic 0.175 ## Degrees of freedom 2 ## P-value (Chi-square) 0.916 ## ## Model Test Baseline Model: ## ## Test statistic 1032.173 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.005 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -4620.704 ## Loglikelihood unrestricted model (H1) -4620.616 ## ## Akaike (AIC) 9257.408 ## Bayesian (BIC) 9295.737 ## Sample-size adjusted Bayesian (BIC) 9270.331 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.025 ## P-value RMSEA &lt;= 0.05 0.990 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.002 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## att =~ ## attitude1 1.000 0.705 0.685 ## attitude2 1.052 0.062 17.093 0.000 0.742 0.722 ## attitude3 0.989 0.060 16.432 0.000 0.698 0.681 ## attitude4 1.060 0.061 17.270 0.000 0.747 0.736 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .attitude1 0.562 0.035 16.245 0.000 0.562 0.531 ## .attitude2 0.505 0.033 15.077 0.000 0.505 0.478 ## .attitude3 0.563 0.034 16.365 0.000 0.563 0.537 ## .attitude4 0.474 0.032 14.582 0.000 0.474 0.459 ## att 0.497 0.048 10.371 0.000 1.000 1.000 #check the model fit and that RMSEA&lt;.05, SRMR&lt;.05, TLI&gt;0.95 and CFI&gt;.95 and that all loadings are significant and &gt;|.30| modindices(attitude_model.est) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 10 attitude1 ~~ attitude2 0.040 -0.006 -0.006 -0.011 -0.011 ## 11 attitude1 ~~ attitude3 0.049 -0.006 -0.006 -0.011 -0.011 ## 12 attitude1 ~~ attitude4 0.176 0.013 0.013 0.025 0.025 ## 13 attitude2 ~~ attitude3 0.176 0.013 0.013 0.024 0.024 ## 14 attitude2 ~~ attitude4 0.049 -0.007 -0.007 -0.015 -0.015 ## 15 attitude3 ~~ attitude4 0.040 -0.006 -0.006 -0.012 -0.012 #check the largest modification indices to see if the expected parameter changes are large enough to justify any additional parameters in the model The above code shows you the steps you should run through for the attitudes construct. You should repeat these steps for each of: attitudes subjective norms perceived behavioural control intentions behaviour ► Question 3a Using lavaan syntax, specify a structural equation model that corresponds to the model in Figure 10.1; however, for each construct use a latent variable measured by the corresponding items in the dataset ► Solution TPB_model&lt;-&#39; #measurement models for attitudes, subjective norms and perceived behavioural control att=~attitude1+attitude2+attitude3+attitude4 SN=~SN1+SN2+SN3+SN4 PBC=~PBC1+PBC2+PBC3+PBC4 intent=~int1+int2+int3+int4 beh=~beh1+beh2+beh3+beh4 #regressions beh~intent+PBC intent~att+SN+PBC #covariances between attitudes, subjective norms and perceived behavioural control att~~SN att~~PBC SN~~PBC&#39; ► Question 3b Estimate the model from Q2a and evaluate the model Does the model fit well? Are the hypothesised paths significant? ► Solution We can estimate the model using the sem() function. By default the sem() function will scale the latent variables by fixing the loading of the first item for each latent variable to 1 but we can override this by setting std.lv=T if we wanted to. By default, maximum likelihood estimation will be used. TPB_model.est&lt;-sem(TPB_model, data=TPB_data, std.lv=T) summary(TPB_model.est, fit.measures=T, standardized=T) ## lavaan 0.6-5 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 48 ## ## Number of observations 890 ## ## Model Test User Model: ## ## Test statistic 157.059 ## Degrees of freedom 162 ## P-value (Chi-square) 0.595 ## ## Model Test Baseline Model: ## ## Test statistic 5335.624 ## Degrees of freedom 190 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -24403.833 ## Loglikelihood unrestricted model (H1) -24325.304 ## ## Akaike (AIC) 48903.667 ## Bayesian (BIC) 49133.646 ## Sample-size adjusted Bayesian (BIC) 48981.207 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.014 ## P-value RMSEA &lt;= 0.05 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.023 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## att =~ ## attitude1 0.710 0.034 21.152 0.000 0.710 0.689 ## attitude2 0.746 0.033 22.590 0.000 0.746 0.726 ## attitude3 0.707 0.033 21.155 0.000 0.707 0.689 ## attitude4 0.730 0.033 22.298 0.000 0.730 0.719 ## SN =~ ## SN1 0.685 0.036 18.786 0.000 0.685 0.661 ## SN2 0.673 0.036 18.466 0.000 0.673 0.651 ## SN3 0.646 0.037 17.369 0.000 0.646 0.616 ## SN4 0.681 0.038 18.064 0.000 0.681 0.638 ## PBC =~ ## PBC1 0.776 0.029 27.045 0.000 0.776 0.799 ## PBC2 0.728 0.028 25.777 0.000 0.728 0.772 ## PBC3 0.709 0.028 25.023 0.000 0.709 0.756 ## PBC4 0.747 0.029 25.813 0.000 0.747 0.773 ## intent =~ ## int1 0.459 0.033 14.067 0.000 0.708 0.584 ## int2 0.535 0.035 15.144 0.000 0.824 0.646 ## int3 0.513 0.035 14.794 0.000 0.791 0.625 ## int4 0.492 0.034 14.300 0.000 0.759 0.597 ## beh =~ ## beh1 0.623 0.039 16.044 0.000 0.808 0.649 ## beh2 0.584 0.039 15.022 0.000 0.758 0.599 ## beh3 0.576 0.039 14.782 0.000 0.748 0.588 ## beh4 0.599 0.040 15.153 0.000 0.777 0.605 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## beh ~ ## intent 0.426 0.057 7.513 0.000 0.506 0.506 ## PBC 0.241 0.071 3.373 0.001 0.185 0.185 ## intent ~ ## att 0.553 0.069 8.010 0.000 0.359 0.359 ## SN 0.302 0.064 4.692 0.000 0.196 0.196 ## PBC 0.756 0.073 10.364 0.000 0.490 0.490 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## att ~~ ## SN 0.320 0.040 7.938 0.000 0.320 0.320 ## PBC 0.253 0.038 6.611 0.000 0.253 0.253 ## SN ~~ ## PBC 0.196 0.041 4.800 0.000 0.196 0.196 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .attitude1 0.556 0.034 16.511 0.000 0.556 0.525 ## .attitude2 0.498 0.032 15.415 0.000 0.498 0.472 ## .attitude3 0.551 0.033 16.509 0.000 0.551 0.525 ## .attitude4 0.498 0.032 15.658 0.000 0.498 0.483 ## .SN1 0.605 0.039 15.384 0.000 0.605 0.563 ## .SN2 0.616 0.039 15.689 0.000 0.616 0.577 ## .SN3 0.681 0.041 16.608 0.000 0.681 0.620 ## .SN4 0.676 0.042 16.047 0.000 0.676 0.593 ## .PBC1 0.341 0.023 15.163 0.000 0.341 0.362 ## .PBC2 0.359 0.022 16.144 0.000 0.359 0.404 ## .PBC3 0.377 0.023 16.639 0.000 0.377 0.429 ## .PBC4 0.376 0.023 16.119 0.000 0.376 0.403 ## .int1 0.969 0.054 18.096 0.000 0.969 0.659 ## .int2 0.949 0.056 16.914 0.000 0.949 0.583 ## .int3 0.976 0.056 17.371 0.000 0.976 0.610 ## .int4 1.042 0.058 17.889 0.000 1.042 0.644 ## .beh1 0.900 0.057 15.656 0.000 0.900 0.579 ## .beh2 1.026 0.061 16.918 0.000 1.026 0.641 ## .beh3 1.057 0.062 17.152 0.000 1.057 0.654 ## .beh4 1.045 0.062 16.783 0.000 1.045 0.634 ## att 1.000 1.000 1.000 ## SN 1.000 1.000 1.000 ## PBC 1.000 1.000 1.000 ## .intent 1.000 0.421 0.421 ## .beh 1.000 0.593 0.593 We can see that the model fits well according to RMSEA, SRMR, TLI and CFI. All of the hypothesised paths in the theory of planned behaviour are statistically significant. ► Question 3c Examine the modification indices and expected parameter changes - are there any additional parameters you would consider including? ► Solution modindices(TPB_model.est) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 54 att =~ SN1 1.663 0.049 0.049 0.047 0.047 ## 55 att =~ SN2 3.197 0.067 0.067 0.065 0.065 ## 56 att =~ SN3 1.283 -0.043 -0.043 -0.041 -0.041 ## 57 att =~ SN4 4.294 -0.081 -0.081 -0.076 -0.076 ## 58 att =~ PBC1 0.350 -0.016 -0.016 -0.017 -0.017 ## 59 att =~ PBC2 1.394 0.032 0.032 0.034 0.034 ## 60 att =~ PBC3 0.133 -0.010 -0.010 -0.011 -0.011 ## 61 att =~ PBC4 0.043 -0.006 -0.006 -0.006 -0.006 ## 62 att =~ int1 0.002 -0.003 -0.003 -0.002 -0.002 ## 63 att =~ int2 0.014 -0.007 -0.007 -0.005 -0.005 ## 64 att =~ int3 0.379 0.035 0.035 0.027 0.027 ## 65 att =~ int4 0.243 -0.028 -0.028 -0.022 -0.022 ## 66 att =~ beh1 0.134 -0.016 -0.016 -0.013 -0.013 ## 67 att =~ beh2 0.844 0.042 0.042 0.034 0.034 ## 68 att =~ beh3 0.033 0.008 0.008 0.007 0.007 ## 69 att =~ beh4 0.420 -0.030 -0.030 -0.024 -0.024 ## 70 SN =~ attitude1 0.923 0.035 0.035 0.034 0.034 ## 71 SN =~ attitude2 0.454 0.024 0.024 0.023 0.023 ## 72 SN =~ attitude3 0.054 0.008 0.008 0.008 0.008 ## 73 SN =~ attitude4 3.291 -0.064 -0.064 -0.063 -0.063 ## 74 SN =~ PBC1 0.216 -0.013 -0.013 -0.013 -0.013 ## 75 SN =~ PBC2 1.679 0.036 0.036 0.038 0.038 ## 76 SN =~ PBC3 0.485 -0.019 -0.019 -0.021 -0.021 ## 77 SN =~ PBC4 0.034 -0.005 -0.005 -0.005 -0.005 ## 78 SN =~ int1 2.659 -0.080 -0.080 -0.066 -0.066 ## 79 SN =~ int2 0.050 -0.011 -0.011 -0.009 -0.009 ## 80 SN =~ int3 0.269 0.027 0.027 0.021 0.021 ## 81 SN =~ int4 1.081 0.054 0.054 0.042 0.042 ## 82 SN =~ beh1 2.904 -0.075 -0.075 -0.060 -0.060 ## 83 SN =~ beh2 0.263 0.023 0.023 0.019 0.019 ## 84 SN =~ beh3 1.906 0.064 0.064 0.050 0.050 ## 85 SN =~ beh4 0.202 0.021 0.021 0.016 0.016 ## 86 PBC =~ attitude1 1.435 -0.039 -0.039 -0.038 -0.038 ## 87 PBC =~ attitude2 0.256 0.016 0.016 0.016 0.016 ## 88 PBC =~ attitude3 1.785 0.043 0.043 0.042 0.042 ## 89 PBC =~ attitude4 0.415 -0.020 -0.020 -0.020 -0.020 ## 90 PBC =~ SN1 0.103 0.011 0.011 0.011 0.011 ## 91 PBC =~ SN2 1.946 0.048 0.048 0.046 0.046 ## 92 PBC =~ SN3 3.081 -0.061 -0.061 -0.058 -0.058 ## 93 PBC =~ SN4 0.005 -0.002 -0.002 -0.002 -0.002 ## 94 PBC =~ int1 0.043 -0.012 -0.012 -0.010 -0.010 ## 95 PBC =~ int2 0.064 0.016 0.016 0.012 0.012 ## 96 PBC =~ int3 0.000 0.001 0.001 0.001 0.001 ## 97 PBC =~ int4 0.007 -0.005 -0.005 -0.004 -0.004 ## 98 PBC =~ beh1 0.005 -0.004 -0.004 -0.003 -0.003 ## 99 PBC =~ beh2 1.367 0.063 0.063 0.050 0.050 ## 100 PBC =~ beh3 0.001 -0.001 -0.001 -0.001 -0.001 ## 101 PBC =~ beh4 1.119 -0.058 -0.058 -0.045 -0.045 ## 102 intent =~ attitude1 0.043 0.006 0.009 0.009 0.009 ## 103 intent =~ attitude2 0.353 0.016 0.025 0.025 0.025 ## 104 intent =~ attitude3 3.425 0.051 0.079 0.077 0.077 ## 105 intent =~ attitude4 6.558 -0.070 -0.108 -0.106 -0.106 ## 106 intent =~ SN1 0.024 0.004 0.006 0.006 0.006 ## 107 intent =~ SN2 4.489 0.056 0.086 0.084 0.084 ## 108 intent =~ SN3 1.063 -0.028 -0.043 -0.041 -0.041 ## 109 intent =~ SN4 1.761 -0.036 -0.056 -0.052 -0.052 ## 110 intent =~ PBC1 0.058 -0.006 -0.009 -0.010 -0.010 ## 111 intent =~ PBC2 0.656 0.020 0.031 0.033 0.033 ## 112 intent =~ PBC3 0.069 -0.007 -0.010 -0.011 -0.011 ## 113 intent =~ PBC4 0.092 -0.008 -0.012 -0.012 -0.012 ## 114 intent =~ beh1 0.008 0.004 0.006 0.005 0.005 ## 115 intent =~ beh2 0.306 -0.025 -0.039 -0.031 -0.031 ## 116 intent =~ beh3 1.632 0.059 0.091 0.071 0.071 ## 117 intent =~ beh4 0.630 -0.037 -0.057 -0.044 -0.044 ## 118 beh =~ attitude1 0.187 -0.012 -0.016 -0.015 -0.015 ## 119 beh =~ attitude2 1.049 -0.028 -0.036 -0.035 -0.035 ## 120 beh =~ attitude3 6.156 0.068 0.089 0.087 0.087 ## 121 beh =~ attitude4 0.815 -0.024 -0.031 -0.031 -0.031 ## 122 beh =~ SN1 0.084 0.008 0.011 0.010 0.010 ## 123 beh =~ SN2 1.865 0.039 0.051 0.049 0.049 ## 124 beh =~ SN3 2.447 -0.046 -0.059 -0.057 -0.057 ## 125 beh =~ SN4 0.009 0.003 0.004 0.003 0.003 ## 126 beh =~ PBC1 0.923 0.026 0.033 0.034 0.034 ## 127 beh =~ PBC2 0.127 -0.009 -0.012 -0.013 -0.013 ## 128 beh =~ PBC3 0.478 0.018 0.024 0.025 0.025 ## 129 beh =~ PBC4 1.692 -0.035 -0.046 -0.047 -0.047 ## 130 beh =~ int1 0.556 -0.038 -0.049 -0.040 -0.040 ## 131 beh =~ int2 1.705 0.069 0.089 0.070 0.070 ## 132 beh =~ int3 0.140 -0.020 -0.025 -0.020 -0.020 ## 133 beh =~ int4 0.260 -0.027 -0.035 -0.027 -0.027 ## 134 attitude1 ~~ attitude2 0.587 -0.022 -0.022 -0.041 -0.041 ## 135 attitude1 ~~ attitude3 0.930 -0.026 -0.026 -0.048 -0.048 ## 136 attitude1 ~~ attitude4 1.278 0.031 0.031 0.059 0.059 ## 137 attitude1 ~~ SN1 0.738 0.021 0.021 0.035 0.035 ## 138 attitude1 ~~ SN2 0.068 0.006 0.006 0.011 0.011 ## 139 attitude1 ~~ SN3 0.258 0.013 0.013 0.020 0.020 ## 140 attitude1 ~~ SN4 0.446 -0.017 -0.017 -0.027 -0.027 ## 141 attitude1 ~~ PBC1 2.436 -0.029 -0.029 -0.066 -0.066 ## 142 attitude1 ~~ PBC2 0.070 -0.005 -0.005 -0.011 -0.011 ## 143 attitude1 ~~ PBC3 0.033 0.003 0.003 0.007 0.007 ## 144 attitude1 ~~ PBC4 0.031 0.003 0.003 0.007 0.007 ## 145 attitude1 ~~ int1 0.075 0.008 0.008 0.011 0.011 ## 146 attitude1 ~~ int2 0.476 -0.020 -0.020 -0.028 -0.028 ## 147 attitude1 ~~ int3 3.310 0.054 0.054 0.073 0.073 ## 148 attitude1 ~~ int4 0.254 0.015 0.015 0.020 0.020 ## 149 attitude1 ~~ beh1 1.811 0.039 0.039 0.055 0.055 ## 150 attitude1 ~~ beh2 0.045 0.006 0.006 0.008 0.008 ## 151 attitude1 ~~ beh3 0.490 -0.021 -0.021 -0.028 -0.028 ## 152 attitude1 ~~ beh4 2.283 -0.046 -0.046 -0.060 -0.060 ## 153 attitude2 ~~ attitude3 0.234 -0.014 -0.014 -0.026 -0.026 ## 154 attitude2 ~~ attitude4 0.582 0.022 0.022 0.044 0.044 ## 155 attitude2 ~~ SN1 9.972 0.073 0.073 0.133 0.133 ## 156 attitude2 ~~ SN2 2.493 -0.037 -0.037 -0.066 -0.066 ## 157 attitude2 ~~ SN3 0.257 -0.012 -0.012 -0.021 -0.021 ## 158 attitude2 ~~ SN4 0.314 -0.014 -0.014 -0.023 -0.023 ## 159 attitude2 ~~ PBC1 0.342 -0.010 -0.010 -0.025 -0.025 ## 160 attitude2 ~~ PBC2 0.529 0.013 0.013 0.031 0.031 ## 161 attitude2 ~~ PBC3 0.302 0.010 0.010 0.023 0.023 ## 162 attitude2 ~~ PBC4 0.002 -0.001 -0.001 -0.002 -0.002 ## 163 attitude2 ~~ int1 2.209 0.042 0.042 0.060 0.060 ## 164 attitude2 ~~ int2 0.020 -0.004 -0.004 -0.006 -0.006 ## 165 attitude2 ~~ int3 0.001 -0.001 -0.001 -0.001 -0.001 ## 166 attitude2 ~~ int4 0.063 0.007 0.007 0.010 0.010 ## 167 attitude2 ~~ beh1 0.447 -0.019 -0.019 -0.028 -0.028 ## 168 attitude2 ~~ beh2 1.233 0.032 0.032 0.045 0.045 ## 169 attitude2 ~~ beh3 8.914 -0.088 -0.088 -0.121 -0.121 ## 170 attitude2 ~~ beh4 0.000 0.000 0.000 0.000 0.000 ## 171 attitude3 ~~ attitude4 0.084 0.008 0.008 0.015 0.015 ## 172 attitude3 ~~ SN1 2.870 -0.040 -0.040 -0.070 -0.070 ## 173 attitude3 ~~ SN2 4.497 0.051 0.051 0.087 0.087 ## 174 attitude3 ~~ SN3 0.029 -0.004 -0.004 -0.007 -0.007 ## 175 attitude3 ~~ SN4 0.303 -0.014 -0.014 -0.022 -0.022 ## 176 attitude3 ~~ PBC1 1.372 0.021 0.021 0.049 0.049 ## 177 attitude3 ~~ PBC2 0.044 0.004 0.004 0.009 0.009 ## 178 attitude3 ~~ PBC3 0.001 0.000 0.000 -0.001 -0.001 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 155 rows ] In this case, none of the expected parameter changes are large enough that we would consider including any additional parameters ► Question 3d Test the indirect effect of attitudes, subjective norms, and perceived behavioural control on behaviour via intentions. When you fit the model with sem(), use se='bootstrap' to get boostrapped standard errors (it may take a few minutes). When you inspect the model using summary(), get the 95% confidence intervals for parameters with ci = TRUE. ► Solution To test these indirect effects we create new a parameter for each indirect effect: TPB_model2&lt;-&#39; #measurement models for attitudes, subjective norms and perceived behavioural control att=~attitude1+attitude2+attitude3+attitude4 SN=~SN1+SN2+SN3+SN4 PBC=~PBC1+PBC2+PBC3+PBC4 intent=~int1+int2+int3+int4 beh=~beh1+beh2+beh3+beh4 #regressions beh~b*intent+PBC intent~a1*att+a2*SN+a3*PBC ##covariances between attitudes, subjective norms and perceived behavioural control att~~SN att~~PBC SN~~PBC ind1:=a1*b #indirect effect of attitudes via intentions ind2:=a2*b #indirect effect of subjective norms via intentions ind3:=a3*b #indirect effect of perceived behavioural control via intentions &#39; When we estimate the model, we request boostrapped standard errors: TPB_model2.est&lt;-sem(TPB_model2, se=&#39;bootstrap&#39;, data=TPB_data) When we inspect the model, we request the 95% confidence intervals for parameters: summary(TPB_model2.est, fit.measures=T, standardized=T, ci=T) ## lavaan 0.6-5 ended normally after 39 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 48 ## ## Number of observations 890 ## ## Model Test User Model: ## ## Test statistic 157.059 ## Degrees of freedom 162 ## P-value (Chi-square) 0.595 ## ## Model Test Baseline Model: ## ## Test statistic 5335.624 ## Degrees of freedom 190 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -24403.833 ## Loglikelihood unrestricted model (H1) -24325.304 ## ## Akaike (AIC) 48903.667 ## Bayesian (BIC) 49133.646 ## Sample-size adjusted Bayesian (BIC) 48981.207 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.014 ## P-value RMSEA &lt;= 0.05 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.023 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper Std.lv Std.all ## att =~ ## attitude1 1.000 1.000 1.000 0.710 0.689 ## attitude2 1.051 0.062 17.001 0.000 0.931 1.172 0.746 0.726 ## attitude3 0.995 0.061 16.339 0.000 0.882 1.114 0.707 0.689 ## attitude4 1.029 0.060 17.200 0.000 0.917 1.150 0.730 0.719 ## SN =~ ## SN1 1.000 1.000 1.000 0.685 0.661 ## SN2 0.982 0.078 12.650 0.000 0.847 1.151 0.673 0.651 ## SN3 0.944 0.070 13.486 0.000 0.809 1.085 0.646 0.616 ## SN4 0.995 0.071 14.000 0.000 0.872 1.141 0.681 0.638 ## PBC =~ ## PBC1 1.000 1.000 1.000 0.776 0.799 ## PBC2 0.938 0.041 23.008 0.000 0.863 1.024 0.728 0.772 ## PBC3 0.914 0.040 22.880 0.000 0.836 0.996 0.709 0.756 ## PBC4 0.963 0.041 23.616 0.000 0.888 1.048 0.747 0.773 ## intent =~ ## int1 1.000 1.000 1.000 0.708 0.584 ## int2 1.164 0.085 13.661 0.000 1.016 1.349 0.824 0.646 ## int3 1.116 0.084 13.268 0.000 0.964 1.302 0.791 0.625 ## int4 1.072 0.082 13.038 0.000 0.927 1.248 0.759 0.597 ## beh =~ ## beh1 1.000 1.000 1.000 0.808 0.649 ## beh2 0.937 0.072 13.108 0.000 0.798 1.081 0.758 0.599 ## beh3 0.925 0.076 12.216 0.000 0.788 1.094 0.748 0.588 ## beh4 0.961 0.073 13.121 0.000 0.822 1.123 0.777 0.605 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper Std.lv Std.all ## beh ~ ## intent (b) 0.578 0.081 7.125 0.000 0.435 0.753 0.506 0.506 ## PBC 0.193 0.062 3.095 0.002 0.068 0.310 0.185 0.185 ## intent ~ ## att (a1) 0.358 0.042 8.433 0.000 0.279 0.444 0.359 0.359 ## SN (a2) 0.203 0.044 4.615 0.000 0.116 0.281 0.196 0.196 ## PBC (a3) 0.447 0.041 10.954 0.000 0.371 0.529 0.490 0.490 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper Std.lv Std.all ## att ~~ ## SN 0.156 0.024 6.394 0.000 0.109 0.205 0.320 0.320 ## PBC 0.140 0.024 5.730 0.000 0.094 0.189 0.253 0.253 ## SN ~~ ## PBC 0.104 0.022 4.848 0.000 0.065 0.148 0.196 0.196 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper Std.lv Std.all ## .attitude1 0.556 0.036 15.265 0.000 0.489 0.622 0.556 0.525 ## .attitude2 0.498 0.032 15.350 0.000 0.431 0.556 0.498 0.472 ## .attitude3 0.551 0.033 16.444 0.000 0.483 0.615 0.551 0.525 ## .attitude4 0.498 0.030 16.882 0.000 0.442 0.557 0.498 0.483 ## .SN1 0.605 0.043 13.993 0.000 0.518 0.686 0.605 0.563 ## .SN2 0.616 0.041 15.174 0.000 0.529 0.693 0.616 0.577 ## .SN3 0.681 0.041 16.708 0.000 0.599 0.763 0.681 0.620 ## .SN4 0.676 0.044 15.430 0.000 0.586 0.761 0.676 0.593 ## .PBC1 0.341 0.023 14.778 0.000 0.297 0.386 0.341 0.362 ## .PBC2 0.359 0.023 15.682 0.000 0.314 0.403 0.359 0.404 ## .PBC3 0.377 0.021 17.738 0.000 0.336 0.418 0.377 0.429 ## .PBC4 0.376 0.023 16.070 0.000 0.331 0.422 0.376 0.403 ## .int1 0.969 0.050 19.231 0.000 0.873 1.071 0.969 0.659 ## .int2 0.949 0.057 16.555 0.000 0.836 1.060 0.949 0.583 ## .int3 0.976 0.058 16.811 0.000 0.855 1.090 0.976 0.610 ## .int4 1.042 0.053 19.501 0.000 0.932 1.138 1.042 0.644 ## .beh1 0.900 0.059 15.230 0.000 0.787 1.019 0.900 0.579 ## .beh2 1.026 0.061 16.878 0.000 0.905 1.144 1.026 0.641 ## .beh3 1.057 0.061 17.325 0.000 0.936 1.175 1.057 0.654 ## .beh4 1.045 0.064 16.392 0.000 0.918 1.167 1.045 0.634 ## att 0.504 0.049 10.362 0.000 0.415 0.608 1.000 1.000 ## SN 0.469 0.050 9.396 0.000 0.375 0.567 1.000 1.000 ## PBC 0.603 0.045 13.473 0.000 0.517 0.690 1.000 1.000 ## .intent 0.211 0.030 7.092 0.000 0.156 0.271 0.421 0.421 ## .beh 0.388 0.050 7.776 0.000 0.295 0.493 0.593 0.593 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper Std.lv Std.all ## ind1 0.207 0.033 6.326 0.000 0.148 0.275 0.182 0.182 ## ind2 0.117 0.030 3.964 0.000 0.062 0.177 0.099 0.099 ## ind3 0.258 0.040 6.502 0.000 0.188 0.345 0.248 0.248 We can see that all of the indirect effects are statistically significant at p&lt;.05 as none of the 95% confidence intervals for the coefficients include zero. ► Question 4 Write up your analysis as if you were presenting the work in academic paper, with brief separate ‘Method’ and ‘Results’ sections ► Solution An example write up would be as follows: Method We tested a theory of planned behaviour model of physical activity by fitting a structural equation model in which attitudes, subjective norms, perceived behavioural control, intentions and behaviour were latent variables defined by four items. We first tested the measurement models for each construct by fitting a one-factor CFA model. Latent variable scaling was by fixing the loading of the first item for each construct to 1. Models were judged to fit well if \\(CFI\\) and \\(TLI\\) were \\(&gt;.95\\) and \\(RMSEA\\) and \\(SRMR\\) were \\(&lt;.05\\). Within the SEM, behaviour was regressed on intentions and perceived behavioural control and intentions were regressed on attitudes, subjective norms, and perceived behavioiural control. In addition, attitudes, subjective norms, and perceived behavioural control were allowed to covary. The indirect effects of attitudes, subjective norms and perceived behavioural control on behaviour were calculated as the product of the effect of the relevant predictor on the mediator (intentions) and the effect of the mediator on the outcome. The statistical significance of the indirect effects were evaluated using bootstrapped 95% confidence intervals with 1000 resamples. In all cases models were fit using maximum likelihood estimation and model fit was judged to be good if CFI and TLI were \\(&gt;.95\\) and RMSEA and SRMR were \\(&lt;.05\\). Modification indices and expected parameter changes were inspected to identify any areas of local mis-fit but model modifications were only made if they could be justified on substantive grounds. Results All measurement models fit well (\\(CFI\\) and \\(TLI &gt;.95\\) and \\(RMSEA\\) and \\(SRMR &lt;.05\\)). The full structural equation model also fit well (\\(CFI = 1.00\\), \\(TLI = 1.00\\), \\(RMSEA &lt;.001\\), \\(SRMR = 0.023\\)). Standardised parameter estimates are provided in Table 1. All of the hypothesised paths were statistically significant at \\(p&lt;.05\\). The significant indirect effects suggested that intentions mediate the effects of attitudes, subjective norms, and perceived behavioural control on behaviour whilst perceived behavioural control also has a direct effect on behaviour. Results thus provide support for a theory of planned behaviour model of physical activity. knitr::include_graphics(&quot;images/lab4 table.png&quot;) "]
]
