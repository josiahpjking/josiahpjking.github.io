[
["index.html", "Data Analysis for Psychology in R (dapR1) - Labs Overview of the Course The team R Cheatsheets R Community", " Data Analysis for Psychology in R (dapR1) - Labs Department of Psychology, University of Edinburgh 2019-2020 Overview of the Course Data Analysis for Psychology in R 1 (dapR1) is your first step on the road to being a data, programming and applied statistics guru! This course provides a introduction to data, R and statistics. It is designed to work slowly through conceptual content that form the basis of understanding and working with data to perform statistical testing. At the same time, we will be introducing you to basic programming in R, covering the fundamentals of working with data, visualization and simple statistical tests. The overall aim of the course is to provide you with all the necessary skills to feel confident working with R and data, before we move on to discuss a broader array of statistical methods in year 2. On this page you will find the notes for your weekly labs including practice exercises and solutions. You can also read a gentle introduction to R and installation guide. As you progress through the course, the content will build up. Find the general overview of topics below so you know where we are heading: SEMESTER 1 Week Lecture Lab topic 1 Introduction lecture: research process, planning and design What is R and installation 2 Measurement: types of data RStudio environment, libraries, packages, scrips and notebooks. Introduction to Rmarkdown 3 Organising data: data sets, tables, plots Assignment: vectors, lists, data frames, data types 4 Describing data: central tendency Introduction to plots and geoms 5 Describing data: variability Central tendency &amp; variability Break 6 Functions &amp; data transformation Hand plotting functions. Introduction to data transformation 7 Statistical models, chance and probability R practice: read in data, merge 8 Fundamentals of probability Introduction to probability. Sampling in R 9 Probability &amp; probability distributions Introduction to probability distributions. Revision 10 Probability distributions: Binomial &amp; Normal Lab test 1 SEMESTER 2 Week Topic 11 Sampling 12 Bootstrapping and confidence intervals 13 Hypothesis testing 14 Type I and Type II Errors, and Power 15 Normal distribution and probability Break 16 Test for one mean 17 Test for two means (independent samples) 18 Test for two means (paired samples) 19 Chi-squared 20 Covariance &amp; correlation The team Dr Tom Booth Tom.Booth@ed.ac.uk Lecturer and Course Organiser Dr Umberto Noe ug.ppls.stats@ed.ac.uk Senior Teching Coordinator (Labs) Dr Josiah King ug.ppls.stats@ed.ac.uk Senior Teching Coordinator (Labs) Ms Emma Waterston Emma.Waterston@ed.ac.uk PhD student in Psychology (Labs) (with special thanks to Dr Anastasia Ushakova) R Cheatsheets You can find a collection of cheatshets that summarise some of the most useful commands you will be using for tasks such as data transformation, visualisation, RMarkdown and many others here/ The key ones you will need to get for this year are: RMarkdown Data Visualisation (ggplot2) Data transformation with dplyr Data Import R Community R has great representation in Edinburgh. Check out these pages to find out more: R Ladies Edinburgh EdinburghR And worldwide you have: R Studio Community "],
["week-1.html", "Chapter 1 Week 1 1.1 Introduction to R and RStudio 1.2 Install the recent version of R and R Studio 1.3 Getting Started in RStudio 1.4 R as an interactive environment 1.5 Check your settings 1.6 Setting Up Your Working Directory 1.7 The Console 1.8 Spacing 1.9 Typos 1.10 Unfinishe…. d 1.11 Basic Arithmetic 1.12 Using Functions for Calculations 1.13 R Scripts 1.14 Short Example 1.15 Naming variables 1.16 Now over to you 1.17 Solutions", " Chapter 1 Week 1 1.1 Introduction to R and RStudio Welcome to Data Analysis for Psychology in R! This week we would like to focus on getting you started in R, get your software installation issues sorted and attempt some very quick and basic practice. There is a lot to cover as we will be moving through the year so if something may look overwhelming, just bear with us, we will make sure that we cover as much as possible so by the end of the year you will become a confident user of R. Everyone in our team really likes R and we hope that you will love it too :) R has a great community built over years and includes researchers, industry practitioners and curious minds across disciplines and fields. R also has been getting better and better over the years and now R has a large online community which shares solutions to any issues/encounters while you are learning R during your time at UoE and after the university. If unsure about something, don’t be afraid to simply Google it! If getting an error, sometimes the fastest you can do is to actually copy and paste it and let Google find you an explanation of what happened. But before doing that, come to labs regularly, ask questions, read the course materials carefully. R can be confusing at the start but we all have been there so stay with us ;) 1.2 Install the recent version of R and R Studio 1.2.1 First Step Depending on the operating system you may check the right version you need. You will need first to install the body of R. For MacOS press here - select R 3.6.1 For Windows press here 1.2.2 Second Step We then will install RStudio. Rsudio is a useful interface that allows us to interact with R and where can see our data, results from analyses and various plots/visualisations. Get a right version for your operating system here For Windows pick RStudio 1.2.1335 - Windows 7+ (64-bit) For MacOS pick RStudio 1.2.1335 - macOS 10.12+ (64-bit) Any issues? Let us know! Quick note: it is useful to update your R from now and then. As you progress in your degree, try to update you R every year you are back to dapR. 1.3 Getting Started in RStudio RStudio is what is called an Integrated Development Environment for R. It is dependent on R but separate from it. There are several ways of using R but R Studio is arguably the most popular and convenient. It’s time to open it up and get started! When you first open up RStudio, this is what you should see: There’s a whole lot of text that (1) isn’t very interesting, and (2) generally isn’t that important so you don’t need to pay much attention to it. 1.4 R as an interactive environment RStudio has four panels: Current file (Editor), Console, Environment, and Viewer. We will focus on using those more in the upcoming labs. 1 The Information area (all of the right-hand side of RStudio) shows you useful information about the state of your project. The top panel of the Information side of RStudio contains the Environment and History panes. History contains the history (unsurprising I know) of commands that have been typed into the console. The Environment is virtual storage of all objects you create in R, and contains your list of variables, data frames, strings, etc., which R has been told to save from previous commands. The bottom panel of the Information section contains a number of different panes - Files, Plots, Packages, Help, and Viewer. In other words, you have a file manager (where you can view which files are loaded into your project), a panel to show plots and graphs, a list of packages, help on the functions available within R, and viewer allows you to view local web content. On the left-hand area of RStudio, you have your Current File (Editor) and Console. The “heart of R” is the Console window. This is where instructions are sent to R, and its responses are given. It’s the place to try things out, but don’t want to save. Finally, the Editor is where you write more complicated scripts without having to run each command. When you run such a script file, it gets interpreted by R in a line by line fashion. This means that your data cleaning, processing, visualization, and analysis needs to be written up in sequence otherwise R won’t be able to make sense of your script. There is an important practical distinction between the Console and the Editor: In the Console, the Enter key runs the command. In the Editor, it just adds a new line. The hash (#) marks everything to the right of it as comment. Using comments can be very useful for annotating your code, and it improves the readability too. It will also help you to remember why/what you done months later, when you return to your code that you have inevitably forgotten! 1.5 Check your settings Before we get to some work in R, let us first make sure that everything works the way we want. Some of you may want to change the appearance of your R, adjust fonts or colors so you find it easier to use R on your computer. 1.5.1 Disable automatic saving of your workspace We will need this to make sure that we clear up our working space each time you close you RStudio so you can start afresh each week without piling up staff in your environment. You will see what we mean as we practice. For now, let’s do the following: 1.5.2 Adjust fonts/colours if you like Depending what works better for your vision and experience, there are number of choices to personalise the look of your R. Try these out here: 1.6 Setting Up Your Working Directory Step 1. Create a folder (sub-directory) named dapR1 on your “Desktop” folder Step 2. From RStudio, use the menu to change your working directory under Session &gt; Set Working Directory &gt; Choose Directory Step 3. Choose the directory you’ve just created in Step 1 Step 4: Select File -&gt; New File -&gt; R Script. In the window that appears, click on the disk icon just below Untitled1 and save the blank script as Lab1.R. Make sure that this saves in your DAPR1 folder. For the purpose of this course, we’re going to be using scripts and RMarkdown files (more on that next week), but first, lets have a play around with the console so you can familiarise yourself with it. 1.7 The Console Click on the console, type 1+1, and hit enter 1 + 1 ## [1] 2 It should have hopefully returned the answer 2. Next type 2*1, and hit enter. This should return the sum value. 2*1 ## [1] 2 Now, hit ctrl + uparrow (cmd + uparrow on mac). This is a useful shortcut, and allows you to quickly re-run or edit previous code used in the console. This handy little shortcut isn’t needed in the script, as you’ll be able to easily copy, change, or run code as needed. 1.8 Spacing You can use spaces to make code easier to read, and although R is pretty good at ignoring large gaps, it does have to guess and make the assumption that the spacing was unintended. Try to keep your code neat and tidy! 2 * 1 ## [1] 2 What you can’t do is insert spaces into the middle of a word, or R will get upset with you. 1.9 Typos You’ll also need to be careful to avoid typos, as R will not know that it is producing unintended output. Instead, it will assume that you meant exactly what you typed. For example, suppose you forget to press the shift key when typing the + sign. Your command would then be 20 = 10, as opposed to 20+10. Here is what would happen: 20 = 10 ## Error in 20 = 10: invalid (do_set) left-hand side to assignment And there you have it - your first error message! This happens when what you type doesn’t make any sense to R. It’s a pretty touchy programme, and can’t spot these kinds of simple human mistakes. Sometimes, R will produce the right answer, but to the wrong question, if you are not careful with what you type. Sticking with the same example, suppose your hand slipped and you pressed the “-” key next to the +. R has no way of knowing that you intended to add 10 to 20, not to subtract. This time you’d get: 20 - 10 ## [1] 10 This can be a little more dangerous, especially when you are working on more advanced stuff, as it can become more difficult to spot mistakes in output. The take home message is simple: You must be precise and accurate with what you say to R. It’s mindlessly obedient, and doesn’t have auto correct. Be careful with what you type! 1.10 Unfinishe…. d Sometimes, you might get a little too excited and hit enter when you haven’t actually finished a command. Because R is so obedient, it will just keep waiting. For example, if you type 20 + and then press enter, R will know that you probably want to put another number at the end of that command. and there in the console you should see a blinking cursor on the right of your + sign. This tells you that R is still waiting for you to finish your command, and the + sign is another command prompt. If you now type 10 and press enter, you will get: 1.11 Basic Arithmetic Table 1 lists the operators that correspond to the basic arithmetic: Operation Operator Example Input Example Output Addition + 20 + 10 30 Subtraction - 20 - 10 10 Multiplication * 20 * 10 200 Division / 20 / 10 2 Power ^ 20 ^ 10 1.024e+13 The one important thing to remember when using R to calculate sums is that brackets always come first. One easy way to remember this is to enclose what you want to happen first in brackets e.g. (20/10) * 2. In this example, R would have done the division first anyway, but its always important to make sure that R is doing exactly what you want! 1.12 Using Functions for Calculations As you will have seen above, there are lots of calculations you can do with very basic operators. If you wanted to do more advanced calculations, you need to use functions. Lets run through a few of the simple ones that are handy to use in R. Operation R code Example Input Example Output Square root sqrt( ) sqrt(100) 10 Absolute value abs( ) abs(-100) 100 Round round(x, digits = ) round(12.345, 2) 12.35 Minimum min(...) min(2.21, 2.22) 2.21 Max Maximum max(...) max(2.21, 2.22) 2.22 It’s also useful to note that you can use multiple functions together and combine them if desired. For example: sqrt(100 + abs(-44)) ## [1] 12 R has started out by calculating abs(44), and then simplifies the command to sqrt(100+44). To solve the sum, it then needs to add 100 and 44, before evaluating the sqrt(144), and returning the value of 12. Top Tip: Lets take an example from above, and say that we wanted to round a value. This time, in the console, start typing the name of the function, and then hit the ‘tab’ key. RStudio will display the below window. There are two panels: one gives a list of variables that starts with the letters I’ve typed, and the other tells you what it does. If you have the name of the function written, R will recognise this, and instead pop up with the arguments for that function instead. 1.13 R Scripts Now that you are hopefully familiar with the console, lets try out writing in a script! R will continue to provide the output in the console, and hitting enter won’t make R run any command. You now need to press Ctrl + Enter (Cmd + Enter). R operates on named data structures, and you can create very simple to very complex structures. # In your script, create an object a and assign it the value of 1 a &lt;- 1 # In the above, we would say &quot;the variable (object) a is assigned to 1&quot;, or &quot;a gets 1&quot; # increment a by 1 a + 1 ## [1] 2 # OK, now see what the value of a is a ## [1] 1 So, R returned us output as if it forgot we asked it to do a + 1 and didn’t change its value. That’s because we weren’t clear in what we wanted! The only way to keep this new value is to put it in an object. b &lt;- a + 1 # now let&#39;s see b ## [1] 2 # success! Quick Note: The &lt;- operator is used to ‘point’ to the object receiving the stated value. In most cases = can be used instead but we advise that you stick to &lt;-. You can also make assignments in the other direction too: b + 1 -&gt; c c ## [1] 3 Now, lets create a vector (or string) of numbers. This is a single entity that consists of a list of ordered numbers. If we wanted to create a vector called x, containing five numbers (2, 4, 6, 8, 10), we would use the R command: x &lt;- c(2, 4, 6, 8, 10) In simple words, we have now created a list of the five numbers, and assigned them to the object x. We made use of the c() function, as we were giving R a list of values. Let’s take a look at x. x ## [1] 2 4 6 8 10 You can now use these objects in other commands. For example, lets say you wanted to square each of the values in x, or multiply by b: x^2 ## [1] 4 16 36 64 100 x*b ## [1] 4 8 12 16 20 1.14 Short Example Now that you’ve created your first variables to store some numbers, lets try an example. Say that you have all found that this course was extremely helpful as an introduction to R, and that you wanted to recommend to your friend to buy the materials online. Firstly, I’d want to calculate how many copies I’d sell if we actually turned this into a proper book. Since there are (roughly) 170 students in the whole class, and you all loved it, I’m going to assume 170 sales, and create a variable called sales. So, how do I do this? sales &lt;- 170 Now to work out how much money I’m going to make per book. I need to create another variable called royalty, which we will use to indicate how much money I will get per copy sold. Let’s say I get £5 per copy and book get sold for £25. royalty &lt;- 5 The last thing I want to do is calculate how much money I’ll make from sales in this class. We now need to create our revenue variable, and ask R to print out the total value of revenue. revenue &lt;- sales * royalty revenue ## [1] 850 And there we have it - £850. As far as R thinks, the sales*royalty is the exact same as 170*5. What if at last minute a student decides to buy 10 copies for their friends too? That would mean that we need to update our revenue. The easiest way to do this is to overwrite the value. revenue_simple&lt;- revenue + 50 revenue_simple ## [1] 900 But we can also be smarter: revenue_smarter &lt;- revenue + 10*royalty revenue_smarter ## [1] 900 1.15 Naming variables You might have noticed so far that I’ve used very simple letters or names for my variables, and that they have all been written in lower case. R does have more flexibility in what you can name your variables, but it also has some rules… Variable names cannot include spaces: my revenue is not a valid name, but my_revenue is Variable names are case sensitive: Revenue and revenue are different names Variable names must be started with a letter or a full stop: You couldn’t use something like 1revenue or _revenue There are some reserved keywords that cannot be used to name variables with: These include, if, else, for, in, next, TRUE, FALSE, NULL, NA, NaN, repeat, function, NA_integer_, NA_real_, NA_complex_. Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these. There are a few other things you’d want to consider too, such as using meaningful names, using short names, and using a conventional style for multiword names when necessary. Consistency is key! 1.16 Now over to you You have seen a few basic operations you can do in R. Now it is time to try some yourself. These should be easy-peasy. 1.16.1 Exercise 1 Calculate the product of 155*12 Store the result in the ‘results’ object (check that it now appears in your environment) Now take a square root of the results Save the results in the object ‘final_results’ 1.16.2 Exercise 2 Go back to the revenues example Assuming that our book did not sell that great after all and we only sold 50 copies. Our publisher did not like it so they decrease the royalties that we get from sales so publisher can cover the cost of book production (new royalty is £3) What is the total revenue now? 1.17 Solutions #Exercise 1 155*2 #Store the results results&lt;-155*12 #sqrt sqrt(results) #New object final_results&lt;- sqrt(results) #Exercise 2 #New sales new_sales &lt;- 50 #New royalty new_royalty &lt;- 3 #New revenue new_revenue &lt;- new_sales * new_royalty new_revenue #Your new revenue should be 150 Jessica Ward (https://twitter.com/RLadiesNCL/status/1138812826917724160).↩ "],
["week-2.html", "Chapter 2 Week 2 2.1 Introduction to RMarkdown 2.2 What is R Markdown? 2.3 Getting things ready 2.4 R Markdown basics 2.5 Headings 2.6 Text 2.7 Tables 2.8 Lists 2.9 Rmd documents 2.10 Code chunks 2.11 Common Code Chunks Options 2.12 Including images or links 2.13 Generating documents 2.14 Extra: Open/Save files on Rstudio Cloud (Chrome users)", " Chapter 2 Week 2 2.1 Introduction to RMarkdown Welcome to the second lab of dapR 1. This week, we will introduce R Markdown (Rmd), an extremely versatile and powerful tool for writing reproducible documents of all sorts and formats. We promise you that once you get the hang of it, you will never again want to use your ordinary word processor/text editor for writing reports and notes that are based on data analysis. In fact, all the teaching materials in this course, from the lecture slides to these lab sheets, have been written in Rmd. Apart from Rmd, this week we will also have a bit of a closer look at the little bits of RStudio and walk you through environment, packages, libraries and editing spaces. Once again, do not worry if you do not remember everything at once, we will continue using various Rstudio panels each week and soon enough it will become a habit. 2.2 What is R Markdown? Well, it is a language - or a system - for telling computers how to process and format text. At this point you might be thinking “why on Earth should I be learning this when I can just use the text editor or Word on my computer?!”. That is a good question and the answer to it is that, when it comes to Rmd, its integration with R Studio makes it an incredibly useful tool for writing documents that include the results of a statistical analysis or data visualisations. You can easily write text and include code where you need to. Compared to R scripts that we showed you last week, we have more flexibility here. 2.3 Getting things ready Before you do anything else, open R Studio and install the rmarkdown package if you have not yet done so. This package will enable you to convert files written in Rmd to output of your choice. Task 1: Type exactly the following command into the console and press ↵ Enter: install.packages(&quot;rmarkdown&quot;) Once you have done the step above, you do not need to repeat it again, unless you update your R or work on another computer. Task 2: Download and open the Week2_pratice.Rmd file (you can find it here and on Learn) and have a look what you can see in there. We will walk you through the first steps - there is some practice for you to try as well. 2.4 R Markdown basics Once you have done the step above, you do not need to repeat it again, unless you update your R or work on another computer. 2.5 Headings First, lets look at how you would make headings using #: # Section 1 ## Subsection 1 ### Sub subsection 1 2.6 Text You can also vary the format of your text: *italics* returns italics **bold** returns bold ~~strikethrough~~ returns strikethrough superscript^2 returns superscript^2 subscript^~2~ returns subscript^2 2.7 Tables An example of simple table that can be produced using RMarkdown is presented below. You will get an illustration on how to build one in your practice template. Operation R code Example Input Example Output Square root sqrt( ) sqrt(100) 10 Absolute value abs( ) abs(-100) 100 Round round(x, digits = ) round(12.345, 2) 12.35 Min min(...) min(2.21, 2.22) 2.21 Max max(...) max(2.21, 2.22) 2.22 2.8 Lists We can list items pretty easily. 2.8.1 Unordered Item Item Item 2.8.2 Ordered Item 1 Item 2 Sub-item 2.1 Item 3 And most importantly, we can include chunks of code which will produce the required output when we compile our document all together. Find a button `insert’ on the top right corner of your Rmd editor. Choose ‘R’. You will be able to see a chunk where you can put a comment and an example of an operation (say multiplication - try to vary those as you go). Press the green button on the right in the code chunk. # This is an R code chunk # Here you can write code and R will run it when you generate your document # and display the output below 6 * 7 ## [1] 42 For a quick reference guide to Rmd, see this cool cheat sheet. 2.9 Rmd documents OK, now that you understand simple editing, let’s look at the .Rmd file step-by-step. The first thing to realise is that an .Rmd file is just a plain text file (such as .txt). You could open it in Notepad, MS Word, or OpenOffice and you would basically see the same thing as in R Studio. The only reason for the special .Rmd extension is for R Studio to know to put all the nice colours in to aid readability and offer you options associated with R Markdown, such as the option to actually generate a document from the file. With that out of the way, but let’s scroll all the way up in the .Rmd file. There, you can see this header: --- pdf_document: default title: &quot;Introducing R Markdown&quot; author: &quot;dapR 1 -- Lab 2&quot; output: html_document: default pdf_document: default In our document, we set the title and author and define the output to be html and pdf (we will show you what we mean by that!) Setting your output file to HTML file is equivalent to the format of websites, which is why we can easily put it online like our book. You can also try out to create PDF of even Word from your Rmd file and save it in your folder. For this semester and perhaps, this year - lets stick to HTML output. 2.10 Code chunks Let’s talk a little more about code chunks, since they are the main reason why Rmd is so useful when it comes to reports of statistical analysis. For one, they are great for creating tables and figures. As a basic demonstration, we can create a simple histogram. Again, at this point, you don’t have to worry about understanding the code itself. The important bit is that, once you know how to create fancy plots and tables, you can create them directly in your .Rmd file to put them in your paper/report/presentation. We can first create some data. # create a made up sequence of numbers and pretend they are the ages of our participants age &lt;- c(34, 22, 26, 25, 43,19, 19, 20, 33, 27, 27, 26, 54) We can then try to plot it (say using histogram for now). We will teach you more about plots in week 4. hist(age) # basic quick histogram This feature has a very useful consequence: you can write a document in such a way that, if something about your data or analysis changes, you can simply edit the code in the appropriate chunks, re-generate the output file and all the values will get updated including your plots. Imagine having to redo a table of 40, 50 or 100 numbers – that’s an awfully tedious task and it’s prone to human error. With a proper use of R Markdown you will never have to do it! Imagine how many hours of work that will save you (trust us, it’s a lot). 2.11 Common Code Chunks Options For now, you can stick to simple code chunks without worrying too much about adding extra options. For future reference, the specifications below could be added to your code chunks. Important note: When provided with a template each week sometimes we will add some modifications to your chunks, please do not edit them or delete anything there as this may affect how your final document looks. If you are keen to learn more about what these settings do, see below: name - This allows you to name your code chunks, but is not necessary unless you want to reference them later echo - Whether to display the code chunk or just show the results. echo=FALSE will embed the code in the document, but the reader won’t be able to see it eval - Whether to run the code in the code chunk. eval=FALSE will display the code but not run it warning - Whether to display warning messages in the document message - Whether to display code messages in the document results - Whether and how to display the computation of the results 2.12 Including images or links 2.12.1 Adding links You can add links to your text quite easily, using square brackets and including the webpage link e.g. [here] (LINK). In practice, just remove the space. See our book [here] (https://bookdown.org/animestina/dapr1_labs/) 2.12.2 Adding figures &amp; pictures Include a picture from online or from your working directory (more on the latter later). With link it’s pretty simple: knitr::include_graphics(&quot;https://imgs.xkcd.com/comics/correlation.png&quot;) You can try to add another one in your practice Rmd file as well. 2.13 Generating documents Now that you have an understanding of the basics of Rmd along with some nifty tricks and can read the source file, let’s talk about how to generate output from the .Rmd files. The simplest way of turning the source file into output is by using the ‘Knit’ button at the top left of your Rmd file. The first time you generate a document like this, it can take a while for R to install and run all the tools necessary to produce your output. After a moment, the result should pop out in R Studio’s internal viewer. Take a minute to marvel at your creation! OK, that’s plenty for now! Close the viewer window and check your “Week_2” folder. Therein, you should find a file called “Week2_practice.html”. This is your actual output. Every time you adjust your Rmd file and compile it again, your output file will get updated as well. If you open it, it should appear in your default web browser because HTML files are the stuff websites are made from. That is all we have in store for you for this lab. We suggest you go over what you learnt today to help your newly acquired knowledge settle. We will be using an Rmd template each week and soon enough you will get used to compiling one at the end of each lab. 2.14 Extra: Open/Save files on Rstudio Cloud (Chrome users) If you are using Chrome Books you will find that there is slight difference with respect to how you would load/save files in RStudio. To provide you with an example on how to open and save the file for this week please use the guide below: "],
["week-3.html", "Chapter 3 Week 3 3.1 Vectors, lists, data frames and data types 3.2 Get the package first 3.3 Numeric Data 3.4 Text/Character Data 3.5 Logical Data 3.6 Variable Classes 3.7 Factors 3.8 Lists 3.9 Practice.Rmd Solutions", " Chapter 3 Week 3 3.1 Vectors, lists, data frames and data types In this section we will introduce you to various types of data you can store and create in R. In applied research and psychology in particular, you will often find different types of information in your data and these could be both numerical and text. We will work through a few examples below to give you an overview of various types of objects that store data and will talk about how you can access and work with the information within those. As you saw during your lecture last week there are few key types of variables we can enounter when it comes to storing information. There is a way to specify this in R: When working with continuous/numeric data you will be creating numeric variables. When working with categorical/nominal/ordinal data you will be creating factor variables. When working with variables which store information such as TRUE or FALSE you will be using logical variables. 3.2 Get the package first For today, the key package we will need is tidyverse. Let’s install it first. install.packages(&#39;tidyverse&#39;) Next, we will need to call it from the library: # Load from the library library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.0 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.5 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ─────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 3.3 Numeric Data Let’s imagine that someone wanted to provide summary data for the average temperature each month in Edinburgh. Imagine that they started updating their notes monthly. They started in June and by now they have the following records: The average temperature was 18 degrees in June, 22 in July, 19 in August, and 16 in September. Task: I want to create a variable, called monthly.temp that stores this data. The first number should be 18, the second 22, and so on. We want to use the combine function c() to help us do this. To create our vector, we should write: monthly.temp &lt;- c(18, 22, 19, 16) monthly.temp ## [1] 18 22 19 16 To summarise, we have created a single variable called monthly.temp, and this variable is a vector with four elements. So, now that we have our vector, how do we get information out of it? What if I wanted to know the average temperature for August, for example? Since we started in June, August is the third month, so let’s try: monthly.temp[3] ## [1] 19 Turns out that the numbers I put for August were wrong, and it was actually warmer this August (not 19 but 21!). How can I fix this in my monthly.temp variable? I could make the whole vector again, but that’s a lot of typing and it’s wasteful given that I only need to change one value. We can just tell R to change that one specific value: monthly.temp[3] &lt;- 21 monthly.temp ## [1] 18 22 21 16 You can also ask R to return multiple values at once by indexing. For example, say I wanted to know the temperature between July (the second element) and September (the fourth element). The first way to ask for an element is to simply provide the numeric position of the desired element in the structure (vector, list…) in a set of square brackets [ ] at the end of the object name. I would ask R: monthly.temp[2:4] ## [1] 22 21 16 # This is equivalent to: monthly.temp[c(2, 3, 4)] ## [1] 22 21 16 Notice that the order matters here. If I asked for it in the reverse order, then R would output the data in the reverse too. 3.4 Text/Character Data Although you will mostly be dealing with numeric data, this isn’t always the case. Sometimes, you’ll use text. Let’s create a simple variable to see how its done: greeting &lt;- &quot;hello&quot; greeting ## [1] &quot;hello&quot; It is important to note the use of quotation marks here. This is because R recognises this as a “character”. A character can be a single letter, 'g', but it can equally well be a sentence including punctuation, &quot;Descriptive statistics can be like online dating profiles: technically accurate and yet pretty darn misleading.&quot; Back to our temperature records example, I might want to create a variable that includes the names of the months. To do so, I could tell R: # Create months months &lt;- c(&quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;) In simple terms, you have now created a character vector containing four elements, each of which is the name of a month. Let’s say I wanted to know what the fourth month was. What would I type? # As before, access the fourth element of your vector months[4] ## [1] &quot;September&quot; 3.5 Logical Data A logical element can take one of two values: TRUE or FALSE. Logicals are usually the output of logical operations (anything that can be phrased as a yes/no question e.g. Is x equal to y?). In formal logic, TRUE is represented as 1 and FALSE as 0. This is also the case in R. If we ask R to calculate 2 + 2, it will always give the same answer. 2+2 ## [1] 4 If we want R to judge whether something is a TRUE statement, we have to explicitly ask. For example: 2+2 == 4 ## [1] TRUE By using the equality operator ==, R is being forced to make a TRUE or FALSE judgement. 2+2 == 3 ## [1] FALSE What if we try to force R to believe some fake news (aka incorrect truths)? 2+2 = 3 ## Error in 2 + 2 = 3: target of assignment expands to non-language object R cannot be convinced that easily. It understands that the 2+2 is not a variable (“non-language object”), and it won’t let you change what 2+2 is. In other words, it won’t let you change the ‘definition’ of the value of 2. There are several other logical operators that you can use, some of which are detailed in the table below. Operation R code Example input Example output Less than &lt; 1 &lt; 2 TRUE Greater than &gt; 1 &gt; 2 FALSE Less than or equal to &lt;= 1 &lt;= 2 TRUE Greater than or equal to &gt;= 1 &gt;= 2 FALSE Equal to == 1 == 2 FALSE Not equal to != 1 != 2 TRUE Not ! !(1==1) FALSE Or | (1==1) (1==2) And &amp; (1==1) &amp; (1==2) FALSE Let’s apply some of these logical operators to our vectors. Let’s use our monthly.temp vector, and ask R whether there were any months when the temperature dropped below zero. monthly.temp &lt; 0 ## [1] FALSE FALSE FALSE FALSE I can then store this information in a vector: any.temp &lt;- monthly.temp &lt; 0 any.temp ## [1] FALSE FALSE FALSE FALSE To summarise, we have created a new logical vector called any.temp, whose elements are TRUE only if the corresponding sale is below zero. But this output isn’t very helpful, as a big list of TRUE and FALSE values don’t give much insight into which months the temperature was below zero. We can use logical indexing to ask for the names of the months where temperature was below zero. Ask R: months[ any.temp &lt; 0 ] ## character(0) 3.6 Variable Classes So far, you’ve encountered character, numeric and logical data. It is really important that you remember/know what kind of information each variable stores (and it is essential) that R remembers, because otherwise you could run into some problems. For example, let’s say you create the following variables: x &lt;- 1 y &lt;- 2 Given that we have assigned numbers, let’s check whether they are numeric: is.numeric(x) ## [1] TRUE is.numeric(y) ## [1] TRUE Great, that means that we could proceed with simple sums e.g. multiplication. However, if they contained character data, R would provide you with an error: x &lt;- &quot;blue&quot; y &lt;- &quot;yellow&quot; x*y ## Error in x * y: non-numeric argument to binary operator Yes, R is smart enough to know that you can’t multiply colours. It knows because you’ve used the quotation marks to indicate that the variable contains text. This might seem unhelpful, but it is actually quite useful, especially when working with data. For example, without quotation marks, R would treat 10 as a number and would allow you to do sums with it. With the quotation marks, “5”, it treats it as text. Above, we checked to specifically see whether our x and y variables were stored as numeric variables. But what if you can’t remember what you should be checking for? You could use the class( ) and mode( ) functions instead. The class( ) of the variable tells you the classification, and mode( ) relates to the format of the information. The former is the most useful in most cases. x &lt;- &quot;hello&quot; class(x) ## [1] &quot;character&quot; mode(x) ## [1] &quot;character&quot; y &lt;- TRUE class(y) ## [1] &quot;logical&quot; mode(y) ## [1] &quot;logical&quot; z &lt;- 10 class(z) ## [1] &quot;numeric&quot; mode(z) ## [1] &quot;numeric&quot; 3.7 Factors Let’s get into some more relevant examples for statistics. We have only referred to ‘numeric’ data so far but we commonly make the distinctions between nominal, ordinal, interval, and ratio numeric data. Imagine that we had conducted a study with different treatment conditions. Within our study, all twelve participants completed the same task, but each of the three groups were given different instructions. Let’s first create a variable that tracks which group people were in: group &lt;- c(1,1,1,1,2,2,2,2,3,3,3,3) Now, it wouldn’t make sense to add two to group 1, group 2, and group 3 but let’s try anyway: group + 2 ## [1] 3 3 3 3 4 4 4 4 5 5 5 5 R has now created groups 4 and 5, which don’t exist. But we allowed it to do so, as the values are currently just ordinary numbers. We need to tell R to treat “group” as a factor. We can do this using the as_factor function. group &lt;- as_factor(group) group ## [1] 1 1 1 1 2 2 2 2 3 3 3 3 ## Levels: 1 2 3 This output is a little different from the first lot, but let’s check that it is now a factor: is.factor(group) ## [1] TRUE class(group) ## [1] &quot;factor&quot; Now, let’s try to add two to the group again to see what happens. group + 2 ## Warning in Ops.factor(group, 2): &#39;+&#39; not meaningful for factors ## [1] NA NA NA NA NA NA NA NA NA NA NA NA Great! Now R knows that we’re the ones being stupid! But what if we wanted to assign meaningful labels to the different levels of the factor? Say, for example, we had low, high, and control conditions? We can do it like this: levels(group) &lt;- c(&quot;low&quot;, &quot;high&quot;, &quot;control&quot;) print(group) ## [1] low low low low high high high high control control control ## [12] control ## Levels: low high control Factors are extremely helpful, and they are the main way to represent nominal scales. It is really important that you label them with meaningful names, as they can help when interpreting output. There are lots of other ways that you can assign labels to your levels. 3.8 Lists Lists arrange elements in a collection of vectors or other data structures. In other words, lists are just a collection of variables, that have no constraints on what types of variables can be included. Emma &lt;- list(age = 26, siblings = TRUE, parents = c(&quot;Mike&quot;, &quot;Donna&quot;) ) Here, R has created a list variable called Emma, which contains three different variables - age, siblings, and parents. Let’s have a look at how R stores this list: print(Emma) ## $age ## [1] 26 ## ## $siblings ## [1] TRUE ## ## $parents ## [1] &quot;Mike&quot; &quot;Donna&quot; If you wanted to extract one element of the list, you would use the $ operator: Emma$age ## [1] 26 You can also add new entries to the list, again using the $. For example: Emma$handedness &lt;- &quot;right&quot; print(Emma) ## $age ## [1] 26 ## ## $siblings ## [1] TRUE ## ## $parents ## [1] &quot;Mike&quot; &quot;Donna&quot; ## ## $handedness ## [1] &quot;right&quot; Nice! These are key things we want you to understand for today. Now, let’s load your Week3_Practice.Rmd file from (download here or from Learn) and you can try these for yourself. Try to attempt all exercises and where necessary, go back to the notes above. Note, that you will need to create the data that we will use next week so make sure to complete all the steps. The solutions are available below but do not look at those yet. 3.9 Practice.Rmd Solutions To do this practice, we will use tidyverse. Make sure that you first run the installation code for the package in your console. Remember once you have done it once, there is no need to do it ever again. install.packages(&#39;tidyverse&#39;) # Load packages library(&#39;tidyverse&#39;) 3.9.1 Exercise 1 Create a list with some information about yourself or play around and store something you think can be best described using lists. Check out the example you saw in the tutorial. # Exercise 1 (example answer) Anastasia&lt;- list(favourite_colour = &#39;blue&#39;, married = TRUE , speak_languages = c(&quot;English&quot;, &quot;Russian&quot;) ) 3.9.2 Exercise 2 Create a nominal variable called sex with three groups: male, female, and other, with four individuals in each group. Make sure to level and label your variable appropriately! I provided an example for this one, try to figure out the second one by yourself. # Exercise 2 (example answer) # Create the variable sex &lt;- c(1,1,1,1,2,2,2,2,3,3,3,3) # Transform into factor sex &lt;- as_factor(sex) # Label the levels levels(sex) &lt;- c(&quot;male&quot;, &quot;female&quot;, &quot;other&quot;) print(sex) Create a variable called “group” that you saw in the tutorial. Remember that one to three represent low, high, and control conditions respectively. # Create group (example answer) group &lt;- c(1,2,3,1,2,2,3,3,1,2,3,3) # Tranform into factor group &lt;- as_factor(group) # Label the levels levels(group) &lt;- c(&quot;low&quot;, &quot;high&quot;, &quot;control&quot;) print(group) 3.9.3 Exercise 3 Earlier we created two variables called group and sex. We also have the test scores and ages of these individuals, so let’s record those as well so we can create a full data set that has everything together. My records for age were: 20, 22, 49, 41, 35, 47, 18, 33, 21, 24, 22, 28 My records were scores were: 70, 89, 56, 60, 68, 62, 93, 63, 71, 65, 54, 67 # Exercise 3 (example answer) age &lt;- c(20, 22, 49, 41, 35, 47, 18, 33, 21, 24, 22, 28) score &lt;- c(70, 89, 56, 60, 68, 62, 93, 63, 71, 65, 54, 67) We now have four variables of the same size in our environment - age, sex, group, and score. Each of them are the same size (i.e. vectors with four elements) and the first entry for age (i.e. age[1]) corresponds to the same person for sex[1]. All of these four variables correspond to the same data set, but R doesn’t know this yet, we need to tell it! Now, let’s put everything side by side: # Now let&#39;s put everything together mydata &lt;- tibble(age, sex, group, score) mydata Note that data is now completely self-contained, and if you were to make changes to say, your original age variable stored in a vector, it will not make any changes to age stored in your data frame. When you have large data frames, you might want to check what variables you have stored in there. To do this, you can ask R to return the names of each of the variables using the names() function. # Check for the variable names names(mydata) # Glimpse at your data glimpse(mydata) This gives you a very basic overview of your data, but is a very helpful tool in displaying the breakdown of what is contained in an object. You might want to get some specific data out of your data frame, as opposed to all four columns. You need to be specific in asking R to return you this information. For example, let’s say you want to extract the scores. # Select specific column select(mydata, score) # Note how we specify data first, then the variable we want # Same fo age select(mydata, age) Quick note: Remember the steps you used to create ‘mydata’ as we will use it again next week to build plots and visualisations. "],
["week-4.html", "Chapter 4 Week 4 4.1 Introduction to plots &amp; geoms (ggplot) 4.2 Visualisations 4.3 Get our data sorted 4.4 Numerical data 4.5 Simple one-variable plot using ggplot() 4.6 Categorical variables 4.7 Continuous variables grouped by a categorical one 4.8 Even more advanced 4.9 Practice.Rmd Solutions", " Chapter 4 Week 4 4.1 Introduction to plots &amp; geoms (ggplot) This week we will walk you through both simple and slightly advanced visualisations in R. Visualisations can in fact be a very creative task and you can let yourself go wild with colours and shapes to visualise your data. :) Good visualisations can also be very powerful when it comes to telling a story with your data. Watch out next time you are reading a news article that uses data. Check how they present the data, what types of graphics they use and how easy it is to read the message. Some cool examples of visualisations made in R for the BBC can be found here. Below we will show you quite a few examples of what you can do with R. This week we want to focus mainly on one-variable visualisations using ggplot(). We will leave some extra materials for you to work through in your own time on how to plot two or more variables. You will need those later in your courses so bookmark the page for the future. :) # Load tidyverse library(tidyverse) 4.2 Visualisations Remember that last week we created some data. But what does it look like? Visualising data is one of the most important tasks facing the data analyst or researcher. It’s important to be able to display your data clearly and coherently, as it makes it easier for the reader to understand. Plus, it helps you to understand your data. The best way to learn plots is by practicing building and modifying simple visualisations. This week we will focus on the key plots you will need to visualise a continuous and a categorical variable, plus a plot that allows you to combine the two. 4.3 Get our data sorted Before we get into using specialised graphics, let’s start by drawing a few very simple graphs to get a feel for what it’s like to draw pictures using R. We will need first to have data. Luckily, there are some in-built datasets that we can use for this illustration. We will use diamonds. 4.4 Numerical data # Get diamonds dataset data &lt;- diamonds We can use help to find out more about this dataset. Try to run the code below: ?diamonds You will see some information in your right-hand pane. It is handy to use ? ... for anything you may want clarification on. # Glimpse glimpse(diamonds) ## Observations: 53,940 ## Variables: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30, 0.23, 0.22, … ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very Good, Fair, Ve… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, E, H, J, J, G, … ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, SI1, SI2, SI2, I… ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64.0, 62.8, 60.4, … ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58, 54, 54, 56, 59… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 342, 344, 345, 345… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.25, 3.93, 3.88, … ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.28, 3.90, 3.84, … ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.73, 2.46, 2.33, … We have quite detailed information about each diamond. Take note of the number of observations and the number of variables. Let’s try to visualise some of the key variables. We will focus on price and cut. 4.5 Simple one-variable plot using ggplot() We can start by creating a space in R where we will map our data. Let’s start by specifying the dataset we want and the variable we are interested in plotting. 4.5.1 Continuous variable # Simple histogram plot for variable &#39;price&#39; - baseline ggplot(data = data, aes(x = price)) Note the key components of the specification above: data: where we provide the name of the dataset. aes: where we provide the aesthetics, i.e. the ‘x-scale’ or more precisely, what we are mapping. We will now need to add geometry - this will be the way our data will be mapped to the space we have just defined: geometry: specifies which type of plot we want to use (i.e geom_bar(), geom_histogram()). Some of the key plot specifications we may need are: For one variable: + geom_bar() adds a bar plot geometry to the graph. + geom_histogram() adds a histogram geometry to the graph. + geom_boxplot() adds a boxplot geometry to the graph. + geom_violin() adds a violin plot geometry to the graph. For two variables: + geom_point() adds a point (scatter plot) geometry to the graph (use with two continuous variables). + geom_boxplot() adds a boxplot geometry to the graph (great for plotting a continuous variable which is grouped). # Simple histogram for variable &#39;price&#39; ggplot(data = data,aes(x = price)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Check carefully what you are seeing. We have an x axis (price) and a y axis (count). We can see that we have more diamonds of the price below $10,000 and fewer very expensive ones. From the first look, I feel the plot could be nicer. Perhaps we could edit our axes, provide a title or change the background? Well, we can do all of these things! Let’s start with the axes and the title. # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram() + labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39; ) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can now modify what our histogram looks like. We will work within geom_histogram(). Note how we leave brackets empty when we are going with the default option. By adding specifications we can modify the look of our graph. I will use color and fill arguments. Try varying these to see how they change: The key colours that are available are (there are more if you need!): Let’s pick some which are good for the reader. If you find that certain colours create certain associations with the data you are plotting that could be helpful or confusing - so please be careful about which ones you choose. We are going with classic: # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(color = &quot;black&quot;, fill = &quot;white&quot;) + # I would like to change the fill to white labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39;) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Try to vary the colours (you could make it quite bright): # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(color = &quot;red&quot;, fill = &quot;blue&quot;) + # I would like to change the fill to blue labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39;) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Whilst we are here, we can also add density to our plot to better visualise the underlying distribution of our variable: # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(aes(y = ..density..), color = &quot;brown&quot;, fill = &quot;grey&quot;) + geom_density() + # Note that I have now added aes(y=..density..) labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39;) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can also adjust the line type around our bins in the histogram, note how we can add linetype= argument inside of geom_histogram(). # Simple histogram for variable &#39;price&#39; ggplot(data = data,aes(x = price)) + geom_histogram(aes(y = ..density..), color = &quot;brown&quot;, fill = &quot;grey&quot;, linetype = &quot;dashed&quot;) + geom_density() + # Note that I have now added aes(y = ..density..) labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39; ) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Lastly, to reflect on some new knowledge, we can also add a line to indicate where the mean of the variable is. We will use geom_vline. # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(aes(y = ..density..), color = &quot;brown&quot;, fill = &quot;grey&quot;, linetype = &quot;dashed&quot;) + geom_density() + # Note that I have now added aes(y = ..density..) and geom_density() geom_vline(aes(xintercept = mean(price)), # Note how we provide x intercept using mean(x) linetype = &quot;dashed&quot;)+ # You can modify the line here as well (change size, colour) labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39;) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. I want to make the line a bit bigger and maybe red so it’s easier to see: # Simple histogram for variable &#39;price&#39; ggplot(data = data, aes(x = price)) + geom_histogram(aes(y = ..density..), color = &quot;brown&quot;, fill = &quot;grey&quot;, linetype = &quot;dashed&quot;) + geom_density() + # Note that I have now added aes(y = ..density..) geom_vline(aes(xintercept = mean(price)), # Note how we provide x intercept using mean(x) linetype = &quot;dashed&quot;, color = &#39;red&#39;, size = 2) + # You can modify the line here as well (change size, colour) labs(x = &quot;Price of the diamond ($)&quot;, # Add x axis label title = &#39;Histogram of diamond prices&#39; ) # Add title ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Nice! Now let’s see how we can work with categorical data. 4.6 Categorical variables In the dataset we are working with we have one interesting categorical variable, the cut of the diamond. It has five levels (Fair, Good, Very Good, Premium, Ideal). Check what it looks like: # Check &#39;cut&#39; select(data, cut) ## # A tibble: 53,940 x 1 ## cut ## &lt;ord&gt; ## 1 Ideal ## 2 Premium ## 3 Good ## 4 Premium ## 5 Good ## 6 Very Good ## 7 Very Good ## 8 Very Good ## 9 Fair ## 10 Very Good ## # … with 53,930 more rows One way to visualise the data would be to start with a simple bar plot. Note how I again specified the data, the x axis and the type of plot I am interested in. # Simple bar plot for variable &#39;cut&#39; ggplot(data = data, aes(x = cut)) + geom_bar() Nice! Let’s do some edits as before. I will do them now in one go. Try to breakdown each modification that was added. I added something new here, namely theme_minimal() so take a note how it changes the plot. # Simple bar plot for variable &#39;cut&#39; ggplot(data = data, aes(x = cut)) + geom_bar(color = &quot;grey&quot;, fill = &quot;cornsilk&quot;) + theme_minimal() + #use different theme labs(x = &quot;Quality of the diamond cut&quot;, # Add x axis label title = &#39;Bar plot for the quality of diamonds cut&#39;) # Add title There are a few others you can try as well (try them out): theme_classic() theme_dark() theme_void() 4.7 Continuous variables grouped by a categorical one Sometimes you may want to provide visualisation of continuous variables using groups. In our case, we could check how the price varies by the cut of the diamond. Note, now I will add aes(x = cut, y = price). I have reversed the axis because I am now interested to show how price varies by cut. Note: This is a sneak peak into what you will be working on in Semester 2 - you will have an actual test to compare these groups for statistical differences but for now, let’s carry on with our plots. ;) I will now build a simple boxplot for price only, note that price is now in the argument as y. # Simple boxplot for variable price ggplot(data = data, aes (y = price)) + geom_boxplot() Now let’s provide visualisations where we also include cut: # Simple boxplot for variable price by cut ggplot(data = data, aes(x = cut, y = price)) + geom_boxplot() Quite cool, don’t you think? We can see that the average price is not that different and some diamonds are just very expensive regardless of the quality of the cut. This could be due to many reasons: carat, shape, clarity, etc. Let’s focus on editing the plot above. Here we can do even more editing - adjusting the colours to differentiate the type of diamond when visualising the price. Let’s first add fill to our aes(): # Simple boxplot for variable price by cut ggplot(data = data, aes(x = cut, y = price, fill = cut)) + geom_boxplot() + theme_minimal() + labs(x = &quot;Quality of the diamond cut&quot;, y = &quot;Price of the diamond ($)&quot;, # Add x and y axes labels title = &#39;Box plot of diamonds price by the cut&#39; ) # Add title We can also play with the colours using a palette, some of the common ones are below: You can now see that we can really edit the colours to our liking. Let’s try using the palette ‘Blues’: # Simple boxplot for variable price by cut ggplot(data = data, aes(x = cut, y = price, fill = cut)) + geom_boxplot() + theme_minimal() + scale_fill_brewer(palette = &quot;Blues&quot;) + #note the palette labs(x = &quot;Quality of the diamond cut&quot;, y = &quot;Price of the diamond ($)&quot;, # Add x and y axes label title = &#39;Box plot of diamonds price by the cut&#39; ) # Add title 4.8 Even more advanced We can also provide visualisations of price distribution by cut all in one go. Check this one out, you will need an extra package so make sure to run install.packages('ggridges'): # Load the package after installation library(ggridges) # Now create fancy plot ggplot(data, aes(x = price, y = cut)) + geom_density_ridges(aes(fill = cut)) + # Note how we add mutiple densities scale_fill_brewer(palette = &quot;Blues&quot;) + # We can assign colours using pallette again labs(x = &quot;Price of the diamond ($)&quot;, y = &quot;Quality of the diamond cut&quot;, # Add x and y axes label title = &#39;Distributions of diamonds price by the cut&#39;) #Add title Nice! Now you have seen some of the basics, we want you to try to build your own plots. Download the Week4_practice.Rmd file (download here or from Learn) and use the notes above to help you where needed. 4.9 Practice.Rmd Solutions Before the start, make sure that you have tidyverse loaded. # Load tidyverse library(tidyverse) 4.9.1 Exercise 1 For your first practice we want you to build a plot based on the example you have seen in the tutorial. We will work with diamonds again. # Get diamonds dataset data &lt;- diamonds Can you check which variables we have? # Glimpse at your data glimpse(data) ## Observations: 53,940 ## Variables: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30, 0.23, 0.22, … ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very Good, Fair, Ve… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, E, H, J, J, G, … ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, SI1, SI2, SI2, I… ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64.0, 62.8, 60.4, … ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58, 54, 54, 56, 59… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 342, 344, 345, 345… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.25, 3.93, 3.88, … ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.28, 3.90, 3.84, … ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.73, 2.46, 2.33, … Select variable carat. # Check carat select(data, carat) ## # A tibble: 53,940 x 1 ## carat ## &lt;dbl&gt; ## 1 0.23 ## 2 0.21 ## 3 0.23 ## 4 0.290 ## 5 0.31 ## 6 0.24 ## 7 0.24 ## 8 0.26 ## 9 0.22 ## 10 0.23 ## # … with 53,930 more rows Produce a simple histogram (finish the expression below). # ggplot of carat ggplot(data = data, aes(x = carat)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Once you are happy with what you see above, try to add labels and change colours. # Complete ggplot for carat ggplot(data = data,aes(x = carat)) + geom_histogram(aes(y =..density..), color = &quot;cadetblue&quot;, fill = &quot;bisque&quot;) + geom_density() + labs(x = &quot;Weight of the diamond (carat)&quot;, title = &#39;Histogram of diamond weight (carat)&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If you want to put both plots side by side: # Assign your plots to specific objects # plot1 plot1 &lt;- ggplot(data = data,aes(x = carat)) + geom_histogram() # plot2 plot2 &lt;- ggplot(data = data,aes(x = carat)) + geom_histogram(aes(y = ..density..), color = &quot;cadetblue&quot;, fill = &quot;bisque&quot;) + geom_density() + labs(x = &quot;Weight of the diamond (carat)&quot;, title = &#39;Histogram of diamond weight (carat)&#39;) You can then install.packages('cowplot') and use the following code: # Load package library(cowplot) ## ## ******************************************************** ## Note: As of version 1.0.0, cowplot does not change the ## default ggplot2 theme anymore. To recover the previous ## behavior, execute: ## theme_set(theme_cowplot()) ## ******************************************************** # Set theme theme_set(theme_grey()) # Put plots side by side plot_grid(plot1, plot2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.9.2 Exercise 2 Now I am keen again to look at the categorical variable cut and ideally, I want to plot the differences in weight of the diamond by cut. Let’s first produce a plot for cut, you need to use geom_bar here: # ggplot for cut ggplot(data = data, aes(x = cut)) + geom_bar() Try to make it a little nicer. # ggplot for cut (with labels, title, colours) # Simple bar plot for variable &#39;cut&#39; ggplot(data = data, aes(x = cut)) + geom_bar(color = &quot;grey&quot;, fill = &quot;blue&quot;) + labs(x = &quot;Quality of the diamond cut&quot;, title = &#39;Bar plot for the quality of diamonds cut&#39;) + theme_minimal() Finally, get the visualisation of carat by cut: # ggplot for carat by cut (add labels, title, colours) - I started this one for you: ggplot(data = data, aes(x = cut, y = carat, fill = cut)) + geom_boxplot() + theme_minimal() + scale_fill_brewer(palette=&quot;Pastel2&quot;) + labs(x = &quot;Quality of the diamond cut&quot;, y = &quot;Weight of the diamond (carat)&quot;, title = &#39;Box plot of diamond weight (carat) by the cut&#39;) 4.9.3 Exercise 3 - Advanced Here is the plot which is based on slighly different data. For this task you will need to get the data and explore it yourself. You will then want to work with key variables that are visible on the plot below. Your task is to recreate the plots. # Load data data2 &lt;- iris Check what each variable means: ?iris # Simple plot of sepal length ggplot(data = data2, aes(y = Sepal.Length)) + geom_boxplot() # Sepal length by species ggplot(data = data2, aes(x = Species, y = Sepal.Length)) + geom_boxplot() Note that here we also adjusted the position of the legend and the theme. # Advanced plot ggplot(data = data2, aes(x = Species, y = Sepal.Length)) + geom_boxplot(aes(fill = Species)) + ylab(&quot;Sepal Length&quot;) + ggtitle(&quot;Iris Data Boxplot: Sepal Length by Species&quot;) + scale_fill_brewer(palette=&quot;YlGn&quot;) + theme_dark() # Even more advanced plot library(ggridges) ggplot(data2, aes(x = Sepal.Length, y = Species)) + geom_density_ridges(aes(fill = Species)) + # Note how we add mutiple densities scale_fill_brewer(palette = &quot;YlGn&quot;) + # We can assign colours using pallette again labs(x = &quot;Sepal Length&quot;, y = &quot;Species&quot;, title = &#39;Distributions of sepal length by species&#39;) # Add title ## Picking joint bandwidth of 0.181 "],
["week-5.html", "Chapter 5 Week 5 5.1 Central Tendency and Variability 5.2 Load all the necessary packages 5.3 Descriptive Statistics 5.4 Central tendency &amp; variability 5.5 Mean and median 5.6 Using piping operator for descriptive statistics 5.7 Standard deviation 5.8 Visualise your data 5.9 Practice.Rmd Solutions", " Chapter 5 Week 5 5.1 Central Tendency and Variability The recommended additional reading can be found in Chapter 5 of Navarro Textbook. We have largely summarised the key information from the reading and the lecture notes in this week’s materials so that you can get a grasp of how to quickly produce descriptions of your data in R. By the end of this week, hopefully you will get some intuition about reading data in R and providing visualisations to accompany your descriptive data analysis. 5.2 Load all the necessary packages library(tidyverse) 5.3 Descriptive Statistics When statisticians are asked to describe what their data looks like, they will often start with descriptive statistics as a quick way to inform people about their data or variable distributions. It may sound weird now but as you move through the years of doing statistics you will find that just reporting the mean, median and standard deviation can easily allow you to visualise the data in your mind without the need for any graphics. Descriptive statistics are an essential component of any research paper or report and you will often find them in those of your readings that use empirical evidence. Interestingly, when it comes to news articles, often only the mean or median are reported (e.g. “Median wage in the UK is £29,588”). Without additional information on sample size or other measures of central tendency, these statistics can be quite meaningless and can trick the reader. We will show you why in the practice exercises below. 5.4 Central tendency &amp; variability To illustrate measures of central tendency we will be using quite a common but easy to understand example of flight delays. Knowing the average flight delay is handy when you are making plans for travelling, but without knowing the actual distribution of the delays for a given dataset, we cannot really know exactly what to expect. Here is some data I found online about average flight delays across some of the UK’s biggest airports. 2 5.5 Mean and median We will work with flight delays in Edinburgh, since we are based here! You will know from experience that you rarely have a delay of exactly 15.8 minutes and in reality, delays could easily range from a minute to a few hours. This may depend on the day of the year, the airline, the time of day and many other factors. Consequently, taking the mean at face value is not always the best strategy! I have collected the data for the delay times of flights on Christmas Eve and a randomly selected midweek day during autumn. Let’s see what we have. The data comes in minutes. Read the data in using the command below. We will then explore the data using some descriptive measures. We will try to see if we can tell anything about the distribution of the data from the central tendency measures - we will then visualise it to check how good our intuition was. 5.5.1 Reading data in We will need to read in the data after saving it in our folder. We will do the steps together but if you forgotten here is the quick guide: For RCloud it is the same process really, just make sure you have uploaded your file, then select working directory and read in the file. There is another way to set your working directory. Go to Session -&gt; Set Working Directory -&gt;Choose Directory -&gt;Locate your folder . Then run the following: # Read data in edinburgh_delays &lt;- read.csv(&#39;edinburgh_delays.csv&#39;) Note that data now appears in your Environment. # Check what is inside head(edinburgh_delays) ## delay_time day ## 1 10 christmas eve ## 2 26 christmas eve ## 3 35 christmas eve ## 4 12 christmas eve ## 5 120 christmas eve ## 6 100 christmas eve 5.6 Using piping operator for descriptive statistics 5.6.1 Mean The mean is also known as the average across your observations and is achieved by summing all of the observations together and then dividing by the total number of obervations (N). The equation for the mean is: \\[\\frac{\\sum\\limits_{i = 1}^{N}x_i}{N}\\] Get the means: # Overall mean edinburgh_delays %&gt;% summarise(mean = mean(delay_time)) ## mean ## 1 27.775 # Mean (grouped by day) edinburgh_delays %&gt;% group_by(day) %&gt;% summarise(Mean = mean(delay_time)) ## # A tibble: 2 x 2 ## day Mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 average day 15.3 ## 2 christmas eve 40.2 It seems like the mean delay time on an ‘average day’ is much closer to what the article reported compared to the mean Christmas Eve delay time which is more than double. Let’s explore it a bit more. 5.6.2 Median The median suggests where the centre of the distribution is. If the data is skewed (e.g. remember our diamond prices from last week?), the median is certainly a better measure of central tendency than the mean. edinburgh_delays %&gt;% group_by(day) %&gt;% summarize(Median = median(delay_time)) ## # A tibble: 2 x 2 ## day Median ## &lt;fct&gt; &lt;dbl&gt; ## 1 average day 15.5 ## 2 christmas eve 26 Note the median for Christmas Eve. Or all in one go: edinburgh_delays %&gt;% group_by(day) %&gt;% summarise( median = median(delay_time), mean = mean(delay_time) ) ## # A tibble: 2 x 3 ## day median mean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 average day 15.5 15.3 ## 2 christmas eve 26 40.2 5.7 Standard deviation We can also calculate standard deviation to get an idea of the variability around our means. Note the formula for the overall variance around the mean first: \\[{\\sigma}^2=\\frac{1}{N-1}\\sum_{i=1}^{N} (X_{i} -\\bar{X})^2 \\] The standard deviation is by derivation: \\[{\\sigma}=\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N} (X_{i} -\\bar{X})^2} \\] Let’s see what it looks like numerically: # Describe the flight delays (mean, median, variance, sd) edinburgh_delays %&gt;% group_by(day) %&gt;% summarise( mean = mean(delay_time), median = median(delay_time), variance = var(delay_time), sd = sd(delay_time) ) ## # A tibble: 2 x 5 ## day mean median variance sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 average day 15.3 15.5 14.8 3.85 ## 2 christmas eve 40.2 26 1400. 37.4 We can make some preliminary conclusions about the data. Note the standard deviation for Christmas Eve. We already know there is a large difference between the two means, but standard deviation can tell us about the spread of our data around the mean. There is less variation on an average day than on Christmas Eve. Before we make some plots, note that you can save your summary statistics as an object. # Save descriptives as an object descriptives &lt;- edinburgh_delays %&gt;% group_by(day) %&gt;% summarise( median = median(delay_time), mean = mean(delay_time), variance = var(delay_time), sd = sd(delay_time) ) descriptives ## # A tibble: 2 x 5 ## day median mean variance sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 average day 15.5 15.3 14.8 3.85 ## 2 christmas eve 26 40.2 1400. 37.4 5.8 Visualise your data Now we have gathered all the numerical descriptions, let’s see how good our intuition was about what the data looks like. Let’s try visualising the delays regardless of the day: # Visualise the data using histogram ggplot(data = edinburgh_delays, aes(x = delay_time)) + geom_histogram(aes(y = ..density..), binwidth = 0.7, color = &quot;blue&quot;) + geom_density() + xlab(&quot;Delay time (min)&quot;) + ggtitle(&quot;Distribution of Delays&quot;) We have quite a variation. We also know that there were differences in the means and the standard deviations when we checked the variable by day. Let’s now visulise each day separately. We can do these with a mini %in% in ggplot(). Pretty cool, right? # Visualise the data using a histogram by day (average day) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;average day&quot;)), aes(x = delay_time)) + geom_histogram() + geom_vline(aes(xintercept = mean(delay_time)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color = &#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Average Day)&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Visualise the data using a histogram by day (average day) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;average day&quot;)), aes(x = delay_time)) + geom_histogram(color =&#39;grey&#39;, fill =&#39;cornsilk&#39;) + geom_vline(aes(xintercept = mean(delay_time)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color = &#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Average Day)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Visualise the data using a histogram by day (Christmas Eve) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = delay_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(delay_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Conclusions? Of course, when describing the average delay for the news article it is best to focus on an average day rather than Christmas Eve. From the example above, if we take an average autumn weekday our data matches what the article suggests pretty well. Nevertheless, it tell us little about what to expect during the weekend or a busy public holiday. To gauge the range of delays and the variation around the mean value, we would need statistics for the mean, median and standard deviation to get a better picture about the situation. Graphics will help us even more! As a final one, let’s also add a box plot or ggridges(): # Boxplot to visualise a comparison ggplot(data = edinburgh_delays, aes(y = delay_time, x = day, fill = day)) + geom_boxplot() + labs(x = &#39;Day&#39;, y = &quot;Delay time (min) &quot;, title = &quot;Delays at the Edinburgh Airport &quot;) + theme_minimal() # ggridges library(ggridges) ggplot(data = edinburgh_delays, aes(y = day, x = delay_time, fill = day)) + geom_density_ridges2() + labs(x = &#39;Delay time (min)&#39;, y = &quot;Density&quot;, title = &quot;Delays at the Edinburgh Airport&quot;) + theme_minimal() ## Picking joint bandwidth of 8.38 Note: before jumping to any conclusions, remember that we only have records for 40 flights for each of the days. Edinburgh Airport can see as many as 313 flights a day. Since we picked our data at random we can consider our samples representative (note: a rule of thumb is &gt; 25 but this can vary depending on the phenomena you are trying to describe). Let’s try to explore something else now. Load your practice file for today and work with the income data in a similar fashion to what we just did. 5.9 Practice.Rmd Solutions Make sure to load the packages first: # Load tidyverse library(tidyverse) 5.9.1 Income Distribution Example Let’s look at some data on income collected for the UK in 2017 by ONS. Note the mean and median values and the skew. What can you say about the income distribution in the UK? How representative is the data of the true population? You can read more about income calculations in the ONS report, if curious of course. I decided to collect my own sample of individuals so I collected the data for 115 residents aged 18+ in London. I further grouped them into North or South London. Use the data to study the variation in income. # Get the data income_london &lt;- read.csv(&quot;income_london.csv&quot;) Open your Rmd template (from here or from Learn) and the data (from here or Learn) for this week and attempt the following steps. Work with the data on income to provide information about the income distribution for Londoners. Save an RMarkdown file for this lab, make notes where necessary and replicate the steps you have seen in the worked example. Make sure to visualise the data, describe it and write a few notes with conclusions about what you found. Are there differences based on geography (i.e. North versus South)? Pay attention to the sample size and how representative it is of the true population (i.e. can we use the London mean to represent the UK?) Compile your file in the end to produce the final report. Polish off some chunks to make it all look nicer! Congratulate yourself on the amazing work you have done over past five weeks! # Check what&#39;s inside head(income_london) ## income region ## 1 64352 north ## 2 49667 north ## 3 57265 north ## 4 20106 north ## 5 54457 north ## 6 72379 north # Means income_london %&gt;% summarise(mean = mean(income)) ## mean ## 1 35504.47 # Means by region income_london %&gt;% group_by(region) %&gt;% summarise(mean = mean(income)) ## # A tibble: 2 x 2 ## region mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 north 37348. ## 2 south 33661. # Medians by region income_london %&gt;% group_by(region) %&gt;% summarise(median = median(income)) ## # A tibble: 2 x 2 ## region median ## &lt;fct&gt; &lt;dbl&gt; ## 1 north 35766 ## 2 south 34016. # Means, medians and SDs by region descriptives &lt;- income_london %&gt;% group_by(region) %&gt;% summarise( mean = mean(income), median = median(income), sd = sd(income) ) descriptives ## # A tibble: 2 x 4 ## region mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 north 37348. 35766 14937. ## 2 south 33661. 34016. 10719. 5.9.2 Visualisations 5.9.2.1 Visualise the distribution of income # Visualise the data using a histogram and/or a density plot ggplot(data = income_london, aes(x = income)) + geom_histogram(aes(y = ..density..), color = &#39;grey&#39;, fill = &#39;yellow&#39;) + geom_density() + geom_vline(aes(xintercept = mean(income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Income (£)&quot;, title = &quot;Distribution of Londoners Income&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 5.9.2.2 Visualise the distribution of income by region Now, focus on regions and provide a separate plot for ‘South’ and ‘North’. Feel free to add mean and median lines too. You will note that the central tendency measures may have changed a little but the shape of the data remains almost the same (i.e. approximately normal). # Visualise the data using a histogram by region (South) ggplot(data = subset(income_london, region %in% c(&quot;south&quot;)), aes(x = income)) + geom_histogram(aes(y = ..density..), color = &#39;grey&#39;, fill = &#39;orange&#39;) + geom_density() + geom_vline(aes(xintercept = mean(income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Income (£) &quot;, title = &quot;Distribution of Londoners Income (South)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Visualise the data using a histogram by region (North) ggplot(data = subset(income_london, region %in% c(&quot;north&quot;)), aes(x = income)) + geom_histogram(aes(y = ..density..), color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_density() + geom_vline(aes(xintercept = mean(income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Income (£)&quot;, title = &quot;Distribution of Londoners Income (North)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Boxplot to visualise a comparison ggplot(data = income_london, aes(y = income, x = region, fill = region)) + geom_boxplot() + labs(x = &#39;Region&#39;, y = &quot;Income (£) &quot;, title = &quot;Distribution of Londoners Income by Region&quot;) What can you conclude about regional differences from studying your data? We observed that there are slight differences in the central tendency measures between North and South London, however we cannot say anything more at this point. Next semester, we will show you an appropriate statistical test that you can use to provide a confident answer to such questions. Compare the results you have observed to the graphic we have from ONS. Are there any differences? Can we use the London data to describe the whole population? Finally, praise yourself for all the work so far :) library(praise) praise() ## [1] &quot;You are wonderful!&quot; Original Article ‘The average flight delay at each of the UK’s 25 busiest airports’↩ "],
["week-6.html", "Chapter 6 Week 6 6.1 Plotting Functions and Simple Data Transformation 6.2 Load all the necessary packages 6.3 Part 1: Hand plotting functions 6.4 Basics 6.5 Linear functions 6.6 Practice plotting 6.7 Part 2: Introduction to Data Transformation 6.8 Renaming 6.9 Adding new variables 6.10 Transformation and skewness 6.11 Mean Centering 6.12 Standardisation 6.13 Practice.Rmd Solutions", " Chapter 6 Week 6 Practice.Rmd here or on Learn. We’ll use the same data as last week (you can download it again from here or Learn). 6.1 Plotting Functions and Simple Data Transformation The key reading that you may find useful for this week: Chapter 5 on Data Transformation - R for Data Science by Garett Grolemund and Hadley Wikham 6.2 Load all the necessary packages library(tidyverse) 6.3 Part 1: Hand plotting functions This week we will be focusing on learning more about various functions and the nature of the data you may be dealing with as a psychology researcher. We will also introduce some hand calculations so make sure you have some paper and a pencil with you to practice these. The main aim is to build intution about functions and learn how these can be applied to the data. 6.4 Basics You have seen by now how to plot your data. Thanks to mathematics, each of the plots we presented to you earlier can be described mathematically by formulating the functions that describe the data best. This section will provide a foundation for understanding functions that can be used to describe relationships in your data. 6.5 Linear functions 6.5.1 Tree height example You will find that in statistics most analytical approaches are based on the assumption that a proccess in your data can be described using linear functions. The most common form would be : \\[y=a+bx\\] Please note that you always have two axes (x and y). Your \\(x\\) will always represent an independent process and \\(y\\) will represent the response in some variable due to change in the independent process. In the equation above \\(a\\) and \\(b\\) are constans. This may sound really complicated so here is an example. As the time passes, the tree height is increasing. Here Time will be our \\(x\\) - an independent variable and (Tree height) will be a dependent variable. In other words, Tree height will be our \\(y\\) - it depends on x (Time). ggplot(data = tree, aes(x = x, y = y)) + geom_line(color = &#39;blue&#39;) + geom_point() + xlab(&quot;Time (years)&quot;) + ylab(&quot;Tree Height (cm)&quot;) + ggtitle(&quot;Tree height&quot;) Note that the height is affected by Time but Time is not affected by Tree Height. Hence the relationship. Your choice of what goes where in your graph depends largely on the assumption about the process you are studying. If we were to fit a model to this data the function we would use is: \\[y=a+bx\\] Tree height (y): \\[y = 5+15x\\] Note that 5 is where we start our line or in other words the value of \\(y\\) when \\(x\\) is zero, and 15 is the slope of the line. Interpretation: The tree will grow in height by about 15 cm each year. 6.5.2 Non-linear functions (first order polynomials) Of course there are cases where the change in \\(y\\) will not always be the same for every unit change of \\(x\\). Such can also be described as non-constant change in y as a function of x. For those cases, polynomials are very helpful. Consider the case below: If we have a sequence of \\(x\\) represented by: \\[x=\\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\\\ 6 \\\\ 7 \\\\ 8\\\\ 9\\\\ 10\\\\ \\end{bmatrix} \\] Then for: \\[f(x)=x^2\\] We will have a corresponding \\(f(x)=y\\): \\[f(x)=y=\\begin{bmatrix} 0 \\\\ 1 \\\\ 4 \\\\ 9 \\\\ 16 \\\\ 25\\\\ 36 \\\\ 49 \\\\ 64\\\\ 81\\\\ 100\\\\ \\end{bmatrix} \\] Can you see how it works? Try now by yourself. 6.6 Practice plotting To play a bit with various functions and plotting, consider using both R and a piece of paper and a pencil. Imagine that you have following values for \\(x\\): \\[x=\\begin{bmatrix} 2 \\\\ 4 \\\\ 5 \\\\ 8 \\\\ 12 \\\\ 16\\\\ 18 \\\\ 22 \\\\ \\end{bmatrix} \\] We are keen to provide visualisations for various functions of x that can be expressed via \\(f(x) = y\\) To plot the following function \\[y=x\\] we can frist create a mini data that will have our \\(x\\) and the function we want to apply to the vector of \\(x\\). # Assign x and y data_1 &lt;- tibble(x = c(2,4,5,8,12,16,18,22), y = x) # Plot ggplot(data = data_1, aes(x = x, y = y)) + geom_point() + geom_line() + xlab(&quot;x&quot;) + ylab(&quot;y = x&quot;) + ggtitle(&quot;Plot of y = x&quot;) Note the response in \\(y\\) for each value of \\(x\\). You can check the result in R too by running data. data_1 ## # A tibble: 8 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 ## 2 4 4 ## 3 5 5 ## 4 8 8 ## 5 12 12 ## 6 16 16 ## 7 18 18 ## 8 22 22 Now try with: \\[y=x-3x\\] # Assign x and y data_2 &lt;- tibble(x = c(2,4,5,8,12,16,18,22), y = x-3*x) # Plot ggplot(data = data_2, aes(x = x, y = y)) + geom_point() + geom_line() + xlab(&quot;x&quot;) + ylab(&quot;y = x - 3x&quot;) + ggtitle(&quot;Plot of y = x - 3x&quot;) data_2 ## # A tibble: 8 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 -4 ## 2 4 -8 ## 3 5 -10 ## 4 8 -16 ## 5 12 -24 ## 6 16 -32 ## 7 18 -36 ## 8 22 -44 Now try with: \\[y=(x+3)^2\\] # Assign x and y data_3 &lt;- tibble(x = c(2,4,5,8,12,16,18,22), y = (x + 3)^2) # Plot ggplot(data = data_3, aes(x = x, y = y)) + geom_point() + geom_line() + xlab(&quot;x&quot;) + ylab(&quot;y = (x + 3)^2&quot;) + ggtitle(&quot;Plot of y = (x + 3)^2&quot;) data_3 ## # A tibble: 8 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 25 ## 2 4 49 ## 3 5 64 ## 4 8 121 ## 5 12 225 ## 6 16 361 ## 7 18 441 ## 8 22 625 Finally, try with: \\[y=log(x)\\] # Assign x and y data_4&lt;- tibble(x = c(2,4,5,8,12,16,18,22), y = log(x)) # Plot ggplot(data = data_4, aes(x = x, y = y)) + geom_point() + geom_line() + xlab(&quot;x&quot;) + ylab(&quot;y = log(x)&quot;) + ggtitle(&quot;Plot of y = log(x)&quot;) data_4 ## # A tibble: 8 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0.693 ## 2 4 1.39 ## 3 5 1.61 ## 4 8 2.08 ## 5 12 2.48 ## 6 16 2.77 ## 7 18 2.89 ## 8 22 3.09 6.7 Part 2: Introduction to Data Transformation This week we will walk you through examples of various data transformations you can do using tidyverse. When working with real data you often may be interested to apply various transformations to your variable to change or/and adjust the scale of your data or for many other purposes. We will use the datasets from Week 5 and will focus on understanding mutate() function. We will further discuss how variable transformations can address the skewness in your variable distributions - if this may seem a bit confusing at this stage, bear with us - we will get back to it soon enough. Let’s start by loading tidyverse. # Load tidyverse library(tidyverse) And the data from last week - edinburgh_delays. # Load the data edinburgh_delays &lt;- read.csv(&#39;edinburgh_delays.csv&#39;) Whilst we are here let us provide a quick data decscription: # Quick description edinburgh_delays %&gt;% group_by(day) %&gt;% summarise(mean = mean(delay_time), median = median(delay_time), sd = sd(delay_time)) ## # A tibble: 2 x 4 ## day mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 average day 15.3 15.5 3.85 ## 2 christmas eve 40.2 26 37.4 From last week remember that Christmas Eve delay times were slighly skewed as we had some very extreme delays which have affected the shape of our histogram. # Visualise the data using a histogram by day (average day) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;average day&quot;)), aes(x = delay_time)) + geom_histogram(color =&#39;grey&#39;, fill =&#39;cornsilk&#39;) + geom_vline(aes(xintercept = mean(delay_time)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color = &#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Average Day)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. And for Christmas Eve too: # Visualise the data using a histogram by day (christmas eve) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = delay_time)) + geom_histogram(color =&#39;grey&#39;, fill =&#39;cornsilk&#39;) + geom_vline(aes(xintercept = mean(delay_time)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_time)), color = &#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min)&#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.8 Renaming Now we not only want to specify that delay time is measured in minutes but we also want to add an extra variable that will provide us with an idea of the delay time in hours. # Rename variable time as delay_min edinburgh_delays &lt;-rename(edinburgh_delays, delay_min = delay_time) 6.9 Adding new variables # Change time into hours edinburgh_delays &lt;- mutate(edinburgh_delays, # Note how we use mutate() delay_hours = delay_min / 60) # We want to divide minutes by 60 to get the number of hours 6.10 Transformation and skewness Now, look back at your histograms for both Christmas Eve and Average Day. Whilst the shape of the Average Day distribution may look quite symmetrical, we may want to address the skewness in the Christmas Eve shape. There are two common ways to do this: taking the log() of the variable or taking the sqrt(). Let us see what these will do in action: # Transform using log edinburgh_delays &lt;- mutate(edinburgh_delays, log_time = log(delay_min)) # Transform using sqrt edinburgh_delays &lt;- mutate(edinburgh_delays, sqrt_time = sqrt(delay_min)) # Visualise the data using a histogram by day (Christmas Eve) now with log transformation ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = log_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(log_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(log_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (log(min))&#39;,y = &#39;Frequency&#39;, title = &#39;Log of Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Visualise the data using a histogram by day (Christmas Eve) now with sqrt transformation ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = sqrt_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(sqrt_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(sqrt_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (sqrt(min))&#39;, y = &#39;Frequency&#39;, title = &#39;Sqrt of Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Log is much more effective in dealing with skewness but from these plots it might be hard to tell. Focus on how close the mean and median values are. Let us compare the real values with the log now - we can use a cowplot. Please note how we first assign our plots to objects which we then plot. # Put side-by-side to compare library(cowplot) # Original data plot plot_original &lt;- ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = delay_min)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(delay_min)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(delay_min)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (min) &#39;, y = &#39;Frequency&#39;, title = &#39;Delays at Edinburgh Airport (Christmas Eve) &#39;) + theme_minimal() # Log transformed plot plot_log &lt;- ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = log_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(log_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(log_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time (log(min))&#39;, y = &#39;Frequency&#39;) + theme_minimal() # To put side by side plot_grid(plot_original, plot_log) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.11 Mean Centering Mean centering is one of the most common transformation for a continious variable. To provide transformation in R we can use the following: # Mean centering edinburgh_delays &lt;- mutate(edinburgh_delays, mean_centered_time = delay_min - mean(delay_min)) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = mean_centered_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(mean_centered_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(mean_centered_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time - mean centered&#39;, y = &#39;Frequency&#39;, title = &#39; Mean Centered Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.12 Standardisation Lastly, we can also standardise our variables by calculating the distance between each observation and the mean in standard deviation units using scale(), note that the results are very smiliar to mean centering technique. # Transform using scale() edinburgh_delays &lt;- mutate(edinburgh_delays, stand_time = scale(delay_min)) ggplot(data = subset(edinburgh_delays, day %in% c(&quot;christmas eve&quot;)), aes(x = stand_time)) + geom_histogram(color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + geom_vline(aes(xintercept = mean(stand_time)), color =&#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(stand_time)), color =&#39;blue&#39;, size = 2) + labs(x = &#39;Delay time - standardized&#39;, y = &#39;Frequency&#39;, title = &#39; Standardised Delays at Edinburgh Airport (Christmas Eve)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We will talk more about these as we progress through the course - for now, just take a note of the various transformations that you can apply to your numeric variables. 6.13 Practice.Rmd Solutions For this week practice we want you to work with simple functions and basic data transformations. Part 1: Work with the vector of \\(x\\) and apply the following functions. If we have \\(x_1\\) represented by: \\[x_1=\\begin{bmatrix} -10 \\\\ 1\\\\ 4 \\\\ 19 \\\\ - 6 \\\\ 0 \\\\ 14 \\\\ -8 \\\\ 0\\\\ 12\\\\ 9\\\\ \\end{bmatrix} \\] And \\(x_2\\) represented by: \\[x_2=\\begin{bmatrix} -3 \\\\ 2\\\\ 1 \\\\ 0 \\\\ 1 \\\\ 2\\\\ 3 \\\\ 5 \\\\ -1\\\\ -2\\\\ -3\\\\ \\end{bmatrix} \\] Provide the results for: \\(y_1=x_1^3\\) \\(y_2=3x_1-5x_1\\) \\(y_3=x_1*x_2\\) Note: you can use mutate() to add an extra variables to represent each of \\(y_i\\). Have a look at the solutions for the hints. # Create the data first with x online my_data &lt;- tibble(x1 = c(-10,1,4,19,-6,0,14,-8,0,12,9), x2 = c(-3,2,1,0,1,2,3,5,-1,-2,-3)) We can use mutate() to add extra variables for \\(y\\): my_data &lt;- mutate(my_data, y1 = x1^3, y2=3*x1-5*x1, y3=x1*x2) my_data ## # A tibble: 11 x 5 ## x1 x2 y1 y2 y3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -10 -3 -1000 20 30 ## 2 1 2 1 -2 2 ## 3 4 1 64 -8 4 ## 4 19 0 6859 -38 0 ## 5 -6 1 -216 12 -6 ## 6 0 2 0 0 0 ## 7 14 3 2744 -28 42 ## 8 -8 5 -512 16 -40 ## 9 0 -1 0 0 0 ## 10 12 -2 1728 -24 -24 ## 11 9 -3 729 -18 -27 Nice! Part 2: Let’s practice with the income data which we worked with last week. Load the data in and work in the template. Create an extra variable called income_thousands which will provide information on income in thousands of pounds instead of just pounds. Visualise the distribution of income. Do you think we need to transform the income variable? Try one of the transformations you have seen today and visualise your data post-transformation (hint: try with log() and mean centering) library(tidyverse) Load the data in and work in the template. Provide descriptives by region. # Read data in income_london &lt;- read.csv(&#39;income_london.csv&#39;) # Quick description income_london %&gt;% group_by(region) %&gt;% summarise(mean = mean(income), median = median(income), sd = sd(income)) ## # A tibble: 2 x 4 ## region mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 north 37348. 35766 14937. ## 2 south 33661. 34016. 10719. Create an extra variable called income_thousands which will provide information on income in thousands of pounds instead of pounds. # Create variable income_thousands using mutate() income_london &lt;- mutate(income_london, # Note how we use mutate() income_thousands = income / 1000) # We want to divide income by 1000 Visualise the distribution of income using thousands. Do you think we need to transform the income variable? Try one of the transformations you have seen today and visualise your data. # Distribution of income in thousands ggplot(data = income_london, aes(x = income_thousands)) + geom_histogram( color = &#39;grey&#39;, fill = &#39;yellow&#39;) + geom_vline(aes(xintercept = mean(income_thousands)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(income_thousands)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Income (thousands of pounds)&quot;, title = &quot;Distribution of London Incomes (thousands of pounds)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. In the context of the distribution, we may find that the shape is pretty symmetrical and our data can be described using the properties of normal distribution. To make the scale of income more comparative we can still try to transform. Let’s do the log(income). # Log transform income_london &lt;- mutate(income_london, log_income = log(income)) And quickly visualise: # Distribution of log(income) ggplot(data = income_london, aes(x = log_income)) + geom_histogram( color = &#39;grey&#39;, fill = &#39;brown&#39;) + geom_vline(aes(xintercept = mean(log_income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(log_income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;log(Income)&quot;, title = &quot;Distribution of Londoners log(Income)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Through rescaling we achieved a reverse - now, our distribution is negatively skewed. Try with the mean centering as well. # Mean Centered income_london &lt;- mutate(income_london, mean_centered_income = income-mean(income)) And quickly visualise: # Distribution of mean centered income ggplot(data = income_london, aes(x = mean_centered_income)) + geom_histogram( color = &#39;grey&#39;, fill = &#39;brown&#39;) + geom_vline(aes(xintercept = mean(mean_centered_income)), color = &#39;red&#39;, size = 2) + geom_vline(aes(xintercept = median(mean_centered_income)), color = &#39;blue&#39;, size = 2) + labs(x = &quot;Mean Centered Income&quot;, title = &quot;Distribution of Londoners Income (Mean Centered)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Take a note of the transformation. Why do you think mean centered income here might be a better transformation of the data compared to that of log()? "],
["week-7.html", "Chapter 7 Week 7 7.1 Reading in Data, Merge and More R Practice 7.2 Dataset 1 (.csv) 7.3 Dataset 2 (.txt) 7.4 Dataset 3 (.sav and .dta) 7.5 Merging datasets together 7.6 Sorting and arrange() 7.7 Data description 7.8 Using filter() 7.9 Visualisations 7.10 Save the file in your folder 7.11 Practice.Rmd Solutions", " Chapter 7 Week 7 Practice.Rmd here or on Learn. A zipped folder of all the datasets can be downloaded here or from Learn. Individual datasets are also available: - data_students_1.csv - data_students_2.txt - data_students_3.csv - data_students_3.dta - data_students_3.sav 7.1 Reading in Data, Merge and More R Practice This week we will focus on reading various datasets and merging them together where necessary. This can be useful if you collected data at different times or might be merging datasets from different studies for your analysis. We will also work through a few code examples that can help you to navigate around the data and filter observations, using filter and arrange from tidyverse. You can find more examples in the books here: Chapter 5 on Data Transformation - R for Data Science by Garett Grolemund and Hadley Wikham Chapter on Descriptive Statistics and Data Manipuliation - Modern R with the Tidyverse by Bruno Rodrigues Reading in and merging data of different formats can be tricky if you don’t have R to help you. We will walk you through a fairly simple example today to build your intutition. When you get to Years 3 and 4 you will get to work with your own dissertation data - we hope you’ll find the notes here useful for then. First things first, let’s load tidyverse. library(tidyverse) This week we will work with the datasets that have information on students’ grades for different programmes. We have got three separate datasets which we want to put together for the analysis. The first one has data on programmes for 15 students. The second dataset provides grades for the same students. We then have a separate dataset that has information on another 15 students and their respective programmes and grades. On top of that, each dataset comes in a different format. We have .csv, .txt, .sav (SPSS) and also .dta (STATA) format. Confusing right? Our task for today will be to find an efficient way to create a single dataset that has information for these 30 students that you will then work with in your practice. 7.2 Dataset 1 (.csv) We have student IDs and grades. In terms of observations, we have 15 in total. ID (ID1, ID2, ID3… ID15) grades (1-100) The dataset comes in the format familiar to us, csv, so we know how to read that one in, please note that this week we introduce a tidyverse read_csv() function which can be useful for us when it comes to joining the data. data_students_1 &lt;- read.csv(&#39;data_students_1.csv&#39;) 7.3 Dataset 2 (.txt) We have student IDs and programme. In terms of observations, we have 15 in total. ID (ID1, ID2, ID3… ID15) programme (‘psych’, ‘lang’, and ‘phil’) Here, we have got a .txt format. Not a problem for R: data_students_2 &lt;- read.table(&quot;data_students_2.txt&quot;, header = TRUE) # Note that we add the header TRUE which will read the first line in the file as the column names. 7.4 Dataset 3 (.sav and .dta) We have student IDs, grades and programme. In terms of observations, we have 15 in total but these are different students so we will want to add those with the previous datasets later. ID (ID16… ID30) grades (1-100) programme (‘psych’, ‘lang’, and ‘phil’) We have .dta format here which may look foreign to you as it looks like the data was saved by a different software. To deal with those in R, we can install package foreign and then read directly from the format: library(foreign) data_students_3 &lt;- read.dta(&#39;data_students_3.dta&#39;) You may also note that we have data_students_3.sav. This format comes from very popular software that psychology researchers often use, SPSS. We can open it vis foreign as well: data_students_3 &lt;- read.spss(&quot;data_students_3.sav&quot;, to.data.frame=TRUE) # note the argument for data.frame - if you don&#39;t specify, the data will be loaded as the list 7.5 Merging datasets together Once the data is visible in the environment, we can start attempting to bring them together. There are number of ways to do this. Let us start with the most intuitive one. We can merge datasets 1 and 2 using the ID column. We are lucky to have a unique identifier which can allow us to bring datasets together so we can have our grades and programme all in one dataset. Exploring data first can help: head(data_students_1) ## X ID grades ## 1 1 ID_1 20 ## 2 2 ID_2 35 ## 3 3 ID_3 45 ## 4 4 ID_4 85 ## 5 5 ID_5 70 ## 6 6 ID_6 72 head(data_students_2) ## ID programme ## 1 ID_1 psych ## 2 ID_2 lang ## 3 ID_3 phil ## 4 ID_4 psych ## 5 ID_5 lang ## 6 ID_6 phil We can merge data by ID now and we will use full_join(). # Full join students_grades_prog &lt;- full_join(data_students_1, data_students_2, by = c(&#39;ID&#39;)) # We can specify the unqiue variable we use to match the datasets via the &#39;by =&#39; argument. Quickly check that you got what you wanted: head(students_grades_prog) ## X ID grades programme ## 1 1 ID_1 20 psych ## 2 2 ID_2 35 lang ## 3 3 ID_3 45 phil ## 4 4 ID_4 85 psych ## 5 5 ID_5 70 lang ## 6 6 ID_6 72 phil Nice! Now we can work with this data a bit using some extra functions from tidyverse. 7.6 Sorting and arrange() To check how the data looks when sorted we can use arrange():: # Sort in ascending order (default option) students_grades_prog %&gt;% arrange(grades) ## X ID grades programme ## 1 1 ID_1 20 psych ## 2 2 ID_2 35 lang ## 3 15 ID_15 40 phil ## 4 11 ID_11 44 lang ## 5 3 ID_3 45 phil ## 6 10 ID_10 56 psych ## 7 9 ID_9 58 phil ## 8 8 ID_8 60 lang ## 9 14 ID_14 64 lang ## 10 7 ID_7 65 psych ## 11 12 ID_12 68 phil ## 12 5 ID_5 70 lang ## 13 13 ID_13 70 psych ## 14 6 ID_6 72 phil ## 15 4 ID_4 85 psych # Sort in descending order (add &#39;desc&#39;) students_grades_prog %&gt;% arrange(desc(grades)) ## X ID grades programme ## 1 4 ID_4 85 psych ## 2 6 ID_6 72 phil ## 3 5 ID_5 70 lang ## 4 13 ID_13 70 psych ## 5 12 ID_12 68 phil ## 6 7 ID_7 65 psych ## 7 14 ID_14 64 lang ## 8 8 ID_8 60 lang ## 9 9 ID_9 58 phil ## 10 10 ID_10 56 psych ## 11 3 ID_3 45 phil ## 12 11 ID_11 44 lang ## 13 15 ID_15 40 phil ## 14 2 ID_2 35 lang ## 15 1 ID_1 20 psych We can also sort by other variables: # Sort in ascending order by programme (just remove desc()) students_grades_prog %&gt;% arrange(programme) ## X ID grades programme ## 1 2 ID_2 35 lang ## 2 5 ID_5 70 lang ## 3 8 ID_8 60 lang ## 4 11 ID_11 44 lang ## 5 14 ID_14 64 lang ## 6 3 ID_3 45 phil ## 7 6 ID_6 72 phil ## 8 9 ID_9 58 phil ## 9 12 ID_12 68 phil ## 10 15 ID_15 40 phil ## 11 1 ID_1 20 psych ## 12 4 ID_4 85 psych ## 13 7 ID_7 65 psych ## 14 10 ID_10 56 psych ## 15 13 ID_13 70 psych 7.7 Data description Now, we have a full dataset we can try to do some simple analysis on the data and study the variation in grades across the cohort and by programme. # All grades students_grades_prog %&gt;% summarise(mean = mean(grades), median = median(grades), sd = sd(grades)) ## mean median sd ## 1 56.8 60 17.00084 # Group grades by programme students_grades_prog %&gt;% group_by(programme) %&gt;% summarise(mean = mean(grades), median = median(grades), sd = sd(grades)) ## # A tibble: 3 x 4 ## programme mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 lang 54.6 60 14.6 ## 2 phil 56.6 58 14.0 ## 3 psych 59.2 65 24.3 7.8 Using filter() We can use a handy option filter() to do descriptives only for certain observations in the data. Let’s group descriptives by programme but only look at ‘psych’. # Check programme &#39;psych&#39; students_grades_prog %&gt;% filter(programme == &#39;psych&#39;) %&gt;% summarise(mean_psych = mean(grades), median_psych = median(grades), sd_psych = sd(grades)) ## mean_psych median_psych sd_psych ## 1 59.2 65 24.30432 Try with ‘lang’ and ‘phil’ too. # Check programme &#39;lang&#39; students_grades_prog %&gt;% filter(programme == &#39;lang&#39;) %&gt;% summarise(mean_lang = mean(grades), median_lang = median(grades), sd_lang = sd(grades)) ## mean_lang median_lang sd_lang ## 1 54.6 60 14.58767 # Check programme &#39;phil&#39; students_grades_prog %&gt;% filter(programme == &#39;phil&#39;) %&gt;% summarise(mean_phil = mean(grades), median_phil = median(grades), sd_phil = sd(grades)) ## mean_phil median_phil sd_phil ## 1 56.6 58 13.95708 Where necessary, we can also focus on studying only specific values in our data. For instance, imagine you just wanted to study students who have received grades of 50 and above in ‘psych’. # Check programme &#39;psych&#39;, grades &gt; 50 students_grades_prog %&gt;% filter(programme == &#39;phil&#39;, grades &gt; 50) %&gt;% summarise(mean_phil = mean(grades), median_phil = median(grades), sd_phil = sd(grades)) ## mean_phil median_phil sd_phil ## 1 66 68 7.211103 You will get much higher means now as you removed the grades which were lower. We can also use filter() to check just the for occurence of specific values. For example, what if we focused on only very high grades or very low grades in psych? # Check programme &#39;psych&#39; grades above 70 or below 40 students_grades_prog %&gt;% filter(programme == &#39;psych&#39;) %&gt;% filter(grades &gt; 70 | grades &lt; 40) # Take a note of how we use &#39;|&#39; to specify OR. ## X ID grades programme ## 1 1 ID_1 20 psych ## 2 4 ID_4 85 psych There are two extreme values in our data for our specification but let’s look at all the programmes together as well. # Group programmes and check for extreme values students_grades_prog %&gt;% group_by(programme) %&gt;% filter(grades &gt; 80 | grades &lt; 40) ## # A tibble: 3 x 4 ## # Groups: programme [2] ## X ID grades programme ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; ## 1 1 ID_1 20 psych ## 2 2 ID_2 35 lang ## 3 4 ID_4 85 psych 7.9 Visualisations Lastly, as usual, always visualise your data to gauge what the distribution looks like. # Visualise ggplot(data = students_grades_prog, aes(x = grades)) + geom_histogram(bins = 15, color = &#39;grey&#39;, fill = &#39;blue&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades&#39;) + theme_minimal() What about by programme? Try plotting with subsets. Bear in mind we don’t have that many observations. # Example for psych only ggplot(data = subset(students_grades_prog, programme %in% c(&#39;psych&#39;)), aes(x = grades)) + geom_histogram(bins = 20, color = &#39;grey&#39;, fill = &#39;blue&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades (Psych)&#39;) + theme_minimal() 7.10 Save the file in your folder Before finishing off, we can also write the merged dataset into our folder so it can be saved for the future. Check your folder after you run below. write.csv(students_grades_prog, &#39;student_grades_prog.csv&#39;) # Note how we first specify the object we want to save and then the name of the file including the extension &#39;.csv&#39;. 7.11 Practice.Rmd Solutions First, make sure that all the necessary packages are loaded: library(tidyverse) library(foreign) For your practice work with the same data as in the tutorial. You will need to use filter and arrange and also mutate to answer the questions below. You are also required to provide some simple visualisations for your data to show what is happening in student grades by programme. Here is the breakdown of the tasks we want you to do and the solutions: Read all three datasets in (data_students_1, data_students_2, data_students_3). Since they come in different formats make sure to check your notes from the tutorial. Note that you have data_students_3 in different formats so you can choose which one you want to read in. After you’ve read them in, check what’s inside of each dataset. # Dataset 1 data_students_1 &lt;- read.csv(&#39;data_students_1.csv&#39;) head(data_students_1) ## X ID grades ## 1 1 ID_1 20 ## 2 2 ID_2 35 ## 3 3 ID_3 45 ## 4 4 ID_4 85 ## 5 5 ID_5 70 ## 6 6 ID_6 72 # Dataset 2 data_students_2 &lt;- read.table(&quot;data_students_2.txt&quot;, header = TRUE) head(data_students_2) ## ID programme ## 1 ID_1 psych ## 2 ID_2 lang ## 3 ID_3 phil ## 4 ID_4 psych ## 5 ID_5 lang ## 6 ID_6 phil # Dataset 3 data_students_3 &lt;- read.dta(&#39;data_students_3.dta&#39;) head(data_students_3) ## ID grades programme ## 1 ID_16 40 psych ## 2 ID_17 38 lang ## 3 ID_18 50 phil ## 4 ID_19 80 psych ## 5 ID_20 69 lang ## 6 ID_21 70 phil # Or data_students_3 &lt;- read.csv(&#39;data_students_3.csv&#39;) head(data_students_3) ## X ID grades programme ## 1 1 ID_16 40 psych ## 2 2 ID_17 38 lang ## 3 3 ID_18 50 phil ## 4 4 ID_19 80 psych ## 5 5 ID_20 69 lang ## 6 6 ID_21 70 phil Note that we have IDs 16-30 which means that we have got data for an extra 15 students. We can now add these to the other dataset we have. Let us first merge grades and programme for data_students_1 and data_students_2. Merge datasets together. First merge data_students_1 and data_students_2, then merge the resulting data with data_students_3. Hint: you will need to use full_join(). # Dataset 1 + Dataset 2 students_grades_prog &lt;- full_join(data_students_1, data_students_2, by = c(&#39;ID&#39;)) # We can specify the unqiue variable we use to match the datasets via the &#39;by =&#39; argument. # + Dataset 3 students_grades_prog_all &lt;- full_join(students_grades_prog, data_students_3) # Please note that we do not need to specify a unique identifier here as we just want to match data by columns and R is clever enough to know what to do. head(students_grades_prog_all) ## X ID grades programme ## 1 1 ID_1 20 psych ## 2 2 ID_2 35 lang ## 3 3 ID_3 45 phil ## 4 4 ID_4 85 psych ## 5 5 ID_5 70 lang ## 6 6 ID_6 72 phil Work with the final dataset that has information on all students (30 observations). Provide means, medians and standard deviations for grades in each programme. students_grades_prog_all %&gt;% group_by(programme) %&gt;% summarise(mean = mean(grades), median = median(grades), sd = sd(grades)) ## # A tibble: 3 x 4 ## programme mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lang 54.7 61 13.8 ## 2 phil 56.9 58.5 12.0 ## 3 psych 61 65.5 19.1 Provide a simple visualisaiton for each programme. # Psych ggplot(data = subset(students_grades_prog_all, programme %in% c(&#39;psych&#39;)), aes(x = grades)) + geom_histogram(bins = 20, color = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades (Psych)&#39;) + theme_minimal() # Lang ggplot(data = subset(students_grades_prog_all, programme %in% c(&#39;lang&#39;)), aes(x = grades)) + geom_histogram(bins = 20,color = &#39;grey&#39;, fill = &#39;lightblue&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades (Lang)&#39;) + theme_minimal() # Phil ggplot(data = subset(students_grades_prog_all, programme %in% c(&#39;phil&#39;)), aes(x = grades)) + geom_histogram(bins = 20, color = &#39;grey&#39;, fill = &#39;coral&#39;) + labs(x = &#39;Grades&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Student Grades (Phil)&#39;) + theme_minimal() Now, try to answer the following questions: How many students in the dataset received the grades above 70? # Filter for grades above 70 students_grades_prog_all %&gt;% filter(grades &gt; 70) ## X ID grades programme ## 1 4 ID_4 85 psych ## 2 6 ID_6 72 phil ## 3 4 ID_19 80 psych The answer is three. What is the mean and the median grade for those who got more than 65? # Mean and median for grades above 70 students_grades_prog_all %&gt;% filter(grades &gt; 65) %&gt;% summarise(mean_above_65 = mean(grades), median_above_65 = median(grades)) ## mean_above_65 median_above_65 ## 1 72 70 The answer is 72 and 70. How many students received grades that were between 40 and 50 in philosophy programme? # Phil grades between 40 and 50 students_grades_prog_all %&gt;% filter(programme == &#39;phil&#39;) %&gt;% filter(grades &gt; 40 &amp; grades &lt; 50) # Note that we use &#39;&amp;&#39; to specify that we want grades both less than 50 and more than 40. ## X ID grades programme ## 1 3 ID_3 45 phil ## 2 15 ID_30 42 phil There are two students. Considering only philosophy programme, what were the top three grades in the cohort? # Only phil arranged students_grades_prog_all %&gt;% filter(programme == &#39;phil&#39;) %&gt;% arrange(desc(grades)) ## X ID grades programme ## 1 6 ID_6 72 phil ## 2 6 ID_21 70 phil ## 3 12 ID_12 68 phil ## 4 12 ID_27 65 phil ## 5 9 ID_24 59 phil ## 6 9 ID_9 58 phil ## 7 3 ID_18 50 phil ## 8 3 ID_3 45 phil ## 9 15 ID_30 42 phil ## 10 15 ID_15 40 phil The answer is 72, 70 and 68. Now, for language, what were the three lowest grades in the cohort? # Only lang arranged students_grades_prog_all %&gt;% filter(programme == &#39;lang&#39;) %&gt;% arrange(grades) ## X ID grades programme ## 1 2 ID_2 35 lang ## 2 2 ID_17 38 lang ## 3 11 ID_26 40 lang ## 4 11 ID_11 44 lang ## 5 8 ID_8 60 lang ## 6 8 ID_23 62 lang ## 7 14 ID_14 64 lang ## 8 14 ID_29 65 lang ## 9 5 ID_20 69 lang ## 10 5 ID_5 70 lang It should be 35, 38 and 40. Well done. It may have taken a while to build all of these code chunks but it is an essential part of the practice to keep playing with the code we are showing you. Try to arrange things differently and see what happens. "],
["week-8.html", "Chapter 8 Week 8 8.1 Introduction to Probability 8.2 Example 1 (hot hand) 8.3 Example 2 (coin toss) 8.4 Example 3 (Independent shooter) 8.5 Practice Rmd. Solutions 8.6 Pen and Paper Exercises", " Chapter 8 Week 8 Practice.Rmd here or on Learn. 8.1 Introduction to Probability The essential reading for this week can be found in Navarro’s Chapter 9 This week we will combine learning R with doing some basic probability calculations. We will toss a few coins and we will also try to work out what the probability of certain events happening is, given that we draw the data at random. This could be fun but might look tricky too. The goal is to build an intutition, not to overwhelm you with probability theory. There are few probability rules that you may want to refer to when working with basic probability examples for this week. The probability of an event lies in the interval [0,1] The probability of two events that are mutually exclusive (i.e. cannot happen at the same time) happening together can be found by adding their individual probabilities The probability of two independent events that are happening together can be found by muiltiplying their individual probabilities The sum of the probabilities of all outcomes must equal 1 The key functions we will need for today wil be sample(), tibble(), count(), table() and select(). Make sure to load tidyverse before your start. library(tidyverse) 8.2 Example 1 (hot hand) The example is adapted from the Open Intro Statistics textbook that, by the way, has an excellent overview of basic statistics and R. Check this out here. This example is based on the data of every shot taken by a basketball player, Kobe Bryant, pictured above. His performance against the Orlando Magic in the 2009 NBA Finals earned him quite a reputation - this lead to him being associated with a phenomenon know as ‘hot hand’. If we are keen to study this in more detail one approach would be to construct a simple distribution of his shots and study the data - we then can check that indeed Kobe does have a ‘hot hand’ meaning that he is more likely to score after a successful shot already occured. The data comes from the textbook website and we can easily load it by using the link below: load(url(&quot;http://www.openintro.org/stat/data/kobe.RData&quot;)) head(kobe) ## vs game quarter time description basket ## 1 ORL 1 1 9:47 Kobe Bryant makes 4-foot two point shot H ## 2 ORL 1 1 9:07 Kobe Bryant misses jumper M ## 3 ORL 1 1 8:11 Kobe Bryant misses 7-foot jumper M ## 4 ORL 1 1 7:41 Kobe Bryant makes 16-foot jumper (Derek Fisher assists) H ## 5 ORL 1 1 7:03 Kobe Bryant makes driving layup H ## 6 ORL 1 1 6:01 Kobe Bryant misses jumper M There are 133 observations of Kobe Bryant’s hits and misses. We will focus on the variable basket - it has values H for hit and M for miss. #Count hits/misses kobe %&gt;% count(basket) ## # A tibble: 2 x 2 ## basket n ## &lt;chr&gt; &lt;int&gt; ## 1 H 58 ## 2 M 75 The first sequence looks like this: \\[H M | M | H H M | M | M | M\\] The streak length is the number of consecutive HITs until a MISS e.g. \\(|H M |\\) represents a streak of length 1 whilst \\(|H H M |\\) represents a streak of length 2. A miss on its own will represent a streak of length 0. #Calculate streaks using the function that came with the data (calc=streak) kobe_streak&lt;-tibble(streak=calc_streak(pull(kobe,basket))) #note that we use pull() here to select data, variable #Count streaks kobe_streak %&gt;% count(streak) ## # A tibble: 5 x 2 ## streak n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 39 ## 2 1 24 ## 3 2 6 ## 4 3 6 ## 5 4 1 #Barplot ggplot(data=kobe_streak,aes(x=streak))+geom_bar(fill=&#39;lightblue&#39;) + labs(title=&#39;Distribution of streaks by Kobe Bryant&#39;) +theme_minimal() Now we can see easily what has been happening. How many hits and misses are in a streak of one? What about a streak length of 0? A streak of 0 is actually the most common. Then we have streak of length 1 - a hit (H) followed by a miss (M), almost as common as having |M M|. We have few occurences of streaks of l. Imagine that we actually had the probability assigned. If Kobe had 58 hits out of 133 then: \\[P(shot 1 = H)=0.45\\] If Kobe indeed had a ‘hot hand’ then the probability of getting a hit after a hit, two in a row, should be even higher than getting a single hit: \\[P(shot 2 = H|shot 1 = H)=0.60\\] But if he doesn’t really have a ‘hot hand’ then the probability could also be: \\[P(shot 2 = H|shot 1 = H)=0.45\\] That means that getting a hit then a miss or two hits in a row is totally random. Later, you will study the streaks of Kobe and then compare them to an independent shooter. We will show you an example below of how to generate a simulation of a random event and you can then try to construct your own independent shooter example. Let’s look at coin tossing as an example of a random process. 8.3 Example 2 (coin toss) We can easily generate a toin coss in R. Why coins? Traditionally, we use coin tossing in statistics because the flip of a coin describes events with an equal chance of happening, they are random, and random processes are a great playground for probability learning. #Lets assign some outcome variable outcomes&lt;-c(&quot;heads&quot;, &quot;tails&quot;) #We then will sample the outcomes 100 time with replacement (note: events are independent) sample(outcomes, size = 100, replace = TRUE) ## [1] &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; ## [12] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; ## [23] &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; ## [34] &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; ## [45] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; ## [56] &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; ## [67] &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; ## [78] &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; ## [89] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; ## [100] &quot;heads&quot; #We can also assign the simulation to an object and use count() to see the resulting distribution and the proportion outcomes&lt;-c(&quot;heads&quot;, &quot;tails&quot;) simulation_coin&lt;-tibble(coin_side = sample(outcomes, size = 100, replace = TRUE)) simulation_coin %&gt;% count(coin_side) ## # A tibble: 2 x 2 ## coin_side n ## &lt;chr&gt; &lt;int&gt; ## 1 heads 46 ## 2 tails 54 You can also do the same via table() and use the results to construct a propotions table: simulation_coin %&gt;% select(coin_side) %&gt;% #select the variable to provide counts table() %&gt;% #table how many values are in each category print() %&gt;% #output the results prop.table() # use the output to calculation the proportion ## . ## heads tails ## 46 54 ## . ## heads tails ## 0.46 0.54 Try to vary the size and see how table changes. You’ll note that as your sample size (N) approaches infinity (\\(N -&gt; +∞\\)) you get closer to the true probability of the event. # Try with the small simulation first simulation_coin &lt;- tibble(coin_side = sample(outcomes, size = 10, replace = TRUE)) simulation_coin %&gt;% count(coin_side) ## # A tibble: 2 x 2 ## coin_side n ## &lt;chr&gt; &lt;int&gt; ## 1 heads 5 ## 2 tails 5 # Or increase to a 1000 simulation_coin&lt;-tibble(coin_side=sample(outcomes, size = 1000, replace = TRUE)) simulation_coin %&gt;% count(coin_side) ## # A tibble: 2 x 2 ## coin_side n ## &lt;chr&gt; &lt;int&gt; ## 1 heads 491 ## 2 tails 509 What if you have an unfair coin? By default the previous simulation assumes that we have 50/50 chance to observe ‘heads’ or ‘tails’. You can assign different probability via simulations: # Simulate an unfair coin simulation_coin_unfair &lt;- tibble(coin_side = sample(outcomes, size = 100, replace = TRUE, prob = c(0.2, 0.8))) # Note that the first outcome has probability 0.2 and the second 0.8 simulation_coin_unfair %&gt;% count(coin_side) ## # A tibble: 2 x 2 ## coin_side n ## &lt;chr&gt; &lt;int&gt; ## 1 heads 20 ## 2 tails 80 Lastly, plot them both to compare: # Barplot (fair coin) ggplot(data = simulation_coin, aes(x = coin_side)) + geom_bar(fill = &#39;grey&#39;) + labs(x = &#39;Coin Side&#39;, title = &#39;Distribution of head/tails (fair coin&#39;) + theme_minimal() # Barplot ggplot(data = simulation_coin_unfair, aes(x = coin_side)) + geom_bar(fill = &#39;lightgreen&#39;) + labs(x = &#39;Coin Side&#39;, title = &#39;Distribution of head/tails (unfair coin)&#39;) + theme_minimal() 8.4 Example 3 (Independent shooter) We can now easily simulate a very similar draw for hits and misses we worked with earlier. We can create an independent shooter draw and compare it with the Kobe example. # Assign some outcome variable outcomes_h_m &lt;- c(&quot;H&quot;, &quot;M&quot;) # We then will sample the outcomes 100 times with replacement sample(outcomes_h_m, size = 133, replace = TRUE, prob = c(0.45, 0.55)) ## [1] &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; ## [24] &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; ## [47] &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; ## [70] &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; ## [93] &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; ## [116] &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;M&quot; &quot;M&quot; #Lets save to a tibble #Or increase to a 1000 simulation_h_m&lt;-tibble(h_m=sample(outcomes_h_m, size = 1000, replace = TRUE)) simulation_h_m %&gt;% count(h_m) ## # A tibble: 2 x 2 ## h_m n ## &lt;chr&gt; &lt;int&gt; ## 1 H 516 ## 2 M 484 Also, a proportion: simulation_h_m %&gt;% select(h_m) %&gt;% table() %&gt;% print() %&gt;% prop.table() ## . ## H M ## 516 484 ## . ## H M ## 0.516 0.484 To provide a comparison, let’s plot the distribution of streaks: # Calculate streaks using the function that came with the data (calc = streak) indep_streak&lt;-tibble(h_m_ind=calc_streak(pull(simulation_h_m,h_m))) # Barplot ggplot(data = indep_streak, aes(x = h_m_ind)) + geom_bar(fill = &#39;lightblue&#39;) + labs(title = &#39;Distribution of streaks by an Independent Shooter&#39;) + theme_minimal() 8.5 Practice Rmd. Solutions 8.5.1 Sex of the babies born in the UK Work with the following example to study the probability of males and females being born in the UK. You can read more about the actual report [here] (https://www.gov.uk/government/statistics/sex-ratios-at-birth-in-great-britain-2013-to-2017) and check the methodologies on how people determine whether there is or isn’t evidence for sex selection at birth. There were 3.7 million births registered in Great Britain in this period with a ratio of males to females being 105.4. This also can be interpreted as the odds being 105.4 to 100 for males to females. \\[ \\frac{male}{female} = \\frac{105.4}{100} \\] To convert from odds to probability, divide the odds by one plus the odds. So to convert odds of 105.4/100 to a probability we can use the following: \\[ {\\frac{105.4}{100}}/({\\frac{105.4}{100} +{\\frac{100}{100}}}) = 0.51\\] From the information we were given we can then conclude that there might be a pretty random chance for baby male and baby female being registered and there is no evidence for sex selection at birth. Can you generate a sample of 100 babies that follow this distribution and then provide us with the plot? # Assign some outcome variable outcomes_babies &lt;- c(&quot;M&quot;, &quot;F&quot;) # We then will sample the outcomes 100 times with replacement sample(outcomes_babies, size = 100, replace = TRUE, prob = c(0.51, 0.49)) ## [1] &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ## [24] &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; ## [47] &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; ## [70] &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; ## [93] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; # Or increase to 1000 and put into a tibble simulation_babies&lt;-tibble(baby=sample(outcomes_babies, size = 1000, replace = TRUE, prob = c(0.51,0.49))) simulation_babies%&gt;% count(baby) ## # A tibble: 2 x 2 ## baby n ## &lt;chr&gt; &lt;int&gt; ## 1 F 464 ## 2 M 536 To provide a comparison, let’s plot as well: # Barplot ggplot(data = simulation_babies, aes(x = baby)) + geom_bar(fill = &#39;grey&#39;) + labs(title = &#39;Distribution of sex at birth based on the UK estimates&#39;) + theme_minimal() What if there were differences? Assign the probabilities as 0.3 and 0.7 and see what happens: # Change the probabilities as if not random simulation_babies &lt;- tibble(baby = sample(outcomes_babies, size = 300, replace = TRUE, prob = c(0.30, 0.70))) simulation_babies%&gt;% select(baby) %&gt;% count(baby) ## # A tibble: 2 x 2 ## baby n ## &lt;chr&gt; &lt;int&gt; ## 1 F 218 ## 2 M 82 To provide a comparison, let’s plot as well: # Barplot ggplot(data = simulation_babies, aes(x = baby)) + geom_bar(fill = &#39;grey&#39;) + labs(title = &#39;Distribution sex at birth based on the UK estimates (if not random)&#39;) + theme_minimal() Try to play with it by resimulating your data and plot again. You will note that there is likely to be a switch between the two bars when we have a random event but a stronger tendency towards differences when the probability was pre-determined. 8.6 Pen and Paper Exercises Work with pen and paper on these ones. Try to calculate a few probabilities given the information below - go to your lab notes where necessary. 8.6.1 Boys and girls paradox (Mr. Jones and Mr. Smith) The Boy or Girl paradox, also known as The Two Child Problem, comes from a problem set up in the 1950s that asks two questions about the children of Mr. Jones and Mr. Smith. It was first featured by Martin Gardner in October 1959 in the “Mathematical Games column” in Scientific American. The paradox comes as follows: Mr. Jones has two children. The older child is a girl. What is the probability that both children are girls? Mr. Smith has two children. At least one of them is a boy. What is the probability that both children are boys? How would you calculate these given the information you have just been given? It is a bit tricker then one may think. The answer could easily be 50/50 but it can also vary given how much extra information we know. Whenever we are calculating probabilities for real world events we do need to make certain assumptions. If it is a random event, then: Each child is either male or female. Each child has the same chance of being male as being female (50/50). The sex of each child is independent of the sex of the other - there is no conditional effect. Given this, there are four possible options, and for Mr.Jones we can immediately eliminate the last two: Older Child = Girl, Younger Child = Boy Older Child = Girl, Younger Child = Girl Older Child = Boy, Younger Child = Boy Older Child = Boy, Younger Child = Girl But what about Mr.Smith? We can’t really know the combination so perhaps there could be three options, which will lead to the probability of the second child being a boy being \\(1/3\\) or \\(0.33\\) - can you tell which ones we would keep to calculate these? Older Child = Girl, Younger Child = Boy Older Child = Girl, Younger Child = Girl Older Child = Boy, Younger Child = Boy Older Child = Boy, Younger Child = Girl A very nice article that discussed the paradox was written by Peter Lynch in 2011, titled ’The Two-Child Paradox: Dichotomy and Ambiguity’: quite worth having a look! :) 8.6.2 Rugby captain A rugby team contains 6 Englishmen, 4 Welshmen, 8 Irishmen and 2 Scotsmen. If the captain is chosen at random, find the probability that he is: Scottish Welsh Irish or Scottish Not Scottish 8.6.3 Answers Calculate the size of the team first by adding all up: \\[ 6 + 4 + 8 + 2 = 20 \\] Then you will have: \\(Welsh = \\frac{4}{20}\\) \\(Scottish = \\frac{2}{20}= \\frac{1}{10}\\) \\(Irish | Scottish = (\\frac{8 + 2)}{20} = \\frac{10}{20} = \\frac{1}{2}\\) \\(Not Scottish = \\frac{(6 + 4 + 8)}{20} = \\frac{18}{20}\\) or also (total probability =1 , then \\(1 - \\frac{2}{20} = \\frac{20}{20} - \\frac{2}{20} = \\frac{18}{20}\\) 8.6.4 Defective A batch of ten electronic components contains three that are defective. If two components are selected from the batch, find the probability that: Both are defective Neither is defective At least one is defective 8.6.5 Answers Both are defective The probability that the first component we pick is defective is \\(\\frac{3}{10}\\). We set that defective component, once we determined it, aside and look at the box again. We now have \\(\\frac{2}{9}\\) probability of getting a defective one (since we picked a component, the total has gone down from 10 to 9, and since we picked a defective one, there are now only 2 defective components in the box). To combine probability we multiply: \\[(\\frac{3}{10})*(\\frac{2}{9}) = (\\frac{6}{90}) = \\frac{1}{15}\\] Neither is defective The same process, but we are now looking at the 7 out of 10 non-defective parts. Probability of picking the first working part: \\(\\frac{7}{10}\\). Which leaves \\(\\frac{6}{9}\\) working parts in the box. Then: \\[ (\\frac{7}{10})*(\\frac{6}{9})=(\\frac{7}{10})*(\\frac{2}{3})=(\\frac{14}{30})=\\frac{7}{15}\\] At least one is defective The easiest here is to profit from the rule where \\(P(E) + P(E&#39;) = 1\\) where: \\(P(E)\\) is the probability of event to occur \\(1 - P(E) = P(E&#39;)\\) is the probability of the event not to occur In words: 1 - Probability(No part defective) = Probability(Some part defective) So we can take the total probability of everything (1) and take away the probability that neither is defective. That leaves us with any case of having one or two defective parts: \\[1 - (\\frac{7}{15}) = (\\frac{15}{15}) - (\\frac{7}{15}) = \\frac{8}{15} \\] "],
["week-9.html", "Chapter 9 Week 9 9.1 Introduction to Probability Distributions and Revision 9.2 Discrete example (guessing homework answers) 9.3 Generate a homework attempt 9.4 Changing the probability (TRUE/FALSE) 9.5 Studying the distribution 9.6 Cumulative Probability (Advanced) 9.7 Revision Practice Rmd. Solutions 9.8 Extra Probability Practice", " Chapter 9 Week 9 An .Rmd with extra probability practice can be downloaded here or from Learn. A revision practice .Rmd can be downloaded here or from Learn. Data for this can be found here or on Learn. 9.1 Introduction to Probability Distributions and Revision The essential reading for this week can be found in Navarro’s Chapter 9 This week we will extend the probability content to probability distributions. We have already had a glance at coin flips last week. We will extend on the concept this week by talking more about sequences of the events. Last week we were working towards building your intuition about random and independent events. The beauty of the independent variable is that we can construct distributions of the values the variable can take (e.g. outcomes of the coin toss) which we can then study to evaluate the likelihood of certain events happening (e.g. Kobe and his ‘hot hand’). We will continue today by working with discrete distributions and the task is to sample from the outcomes, visualise, and attempt to determine the likelihood of an event from the distributions. 9.2 Discrete example (guessing homework answers) Imagine that you are working on your weekly homework quiz on Learn which has 10 multiple choice questions with 4 answers and you want to see what is the likelihood of getting the answer right if you randomly pick the answer each time with your eyes closed. Assume that there is only one correct option. We can study the probability of getting all the questions right or, say, of getting only half of questions right, by generating the distribution of the likely outcomes. If you pick an answer at random then the probability of getting it right should be \\(\\frac{1}{4}=0.25\\). Such would be true also for question two, assuming that your answer for question one is independent of question two. We can generate samples for each trial (n = 10 questions) and will check what is the probability of getting one, two, three, and so on questions out of 10 right. We will use the rbinom() function in R to generate the distribution of a number of attempts. We can work with it directly in R using the extensions: rbinom() to generate the distribution (‘r’ for random) for discrete outcomes dbinom() to study the probability of the outcome pbinom() to study the cumulative probability (also can be described as the area under the curve) The key arguments we will use are: x is a vector of numbers. p is a vector of probabilities. n is number of observations. size is the number of trials. prob is the probability of success of each trial. 9.3 Generate a homework attempt # We can generate an attempt for 10 questions rbinom(n = 10, # 10 attempts size = 10, # 10 questions) prob = 0.25) # Probability that one is correct (1/4) ## [1] 2 1 2 3 2 3 4 1 1 1 # We can generate an attempt for 100 questions/trials rbinom(n = 100, # 100 times of 10 size = 10, prob = 0.25) ## [1] 3 1 2 5 4 2 2 3 0 5 1 4 1 3 3 2 2 2 2 2 3 1 1 1 2 4 1 2 1 3 2 1 2 2 3 3 1 3 3 3 0 4 2 2 3 4 ## [47] 4 3 2 3 2 2 2 5 4 5 4 2 2 2 1 4 3 3 3 4 1 2 1 4 1 2 2 3 4 4 2 2 2 1 1 4 3 1 3 5 4 3 2 4 2 0 ## [93] 4 2 5 4 5 2 3 2 Let’s assign it to a tibble so we can calculate the proportions and make a plot. library(tidyverse) homework_guess &lt;- tibble(right_guess = rbinom(n = 100, size = 10, prob = 0.25)) homework_guess %&gt;% count(right_guess) ## # A tibble: 7 x 2 ## right_guess n ## &lt;int&gt; &lt;int&gt; ## 1 0 6 ## 2 1 14 ## 3 2 33 ## 4 3 22 ## 5 4 16 ## 6 5 6 ## 7 6 3 ggplot(data = homework_guess, aes(x = right_guess)) + geom_bar(fill = &#39;lightblue&#39;) + labs(x = &#39;Right Guess&#39;, y = &#39;Count&#39;) + theme_minimal() We can change the scale on the graph to see all the options: ggplot(data = homework_guess, aes(x = right_guess)) + geom_bar(fill = &#39;lightblue&#39;) + xlim(0,10) + #note how we add xlim() labs(x = &#39;Right Guess&#39;, y = &#39;Count&#39;) + theme_minimal() ## Warning: Removed 1 rows containing missing values (geom_bar). What’s more, we can also change y units to probability. ggplot(data = homework_guess, aes(x = right_guess)) + geom_bar(aes(y = (..count..)/sum(..count..)), # we will work with the function of count, hence we use&#39;..&#39; fill = &#39;lightblue&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;Probability&#39;) + theme_minimal() ## Warning: Removed 1 rows containing missing values (geom_bar). 9.4 Changing the probability (TRUE/FALSE) What if the multiple choice had only two choices for the answer (i.e. TRUE or FALSE questions) - the right answer will now have a probability of 0.5 instead. Let’s reflect on that and create a new tibble(): #Homework guess distribution with TRUE/FALSE homework_guess_true_false &lt;- tibble(right_guess = rbinom(n = 100, size = 10, prob = 0.5)) # Note that the probability of getting the right answer has gone up homework_guess_true_false %&gt;% count(right_guess) ## # A tibble: 9 x 2 ## right_guess n ## &lt;int&gt; &lt;int&gt; ## 1 1 3 ## 2 2 4 ## 3 3 9 ## 4 4 22 ## 5 5 23 ## 6 6 24 ## 7 7 7 ## 8 8 6 ## 9 9 2 Your chances are higher! Can you see why? ggplot(data = homework_guess_true_false, aes(x = right_guess)) + geom_bar(fill = &#39;lightgreen&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;Count&#39;) + theme_minimal() We have less options now, hence a higher chance of picking the correct ones. Let’s also plot with probabilities: ggplot(data = homework_guess_true_false, aes(x = right_guess)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = &#39;lightgreen&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;Probability&#39;) + theme_minimal() 9.5 Studying the distribution What’s great about learning probability disitributions is that we can use the distribution above to find exact probability of guessing different numbers of questions correctly. We will use dbinom(). Check what the probability is of getting at least one question right when guessing at random: dbinom(1, # Number of outcomes that we got right size=10, # 10 trials (we have 10 questions) prob=0.25) # Probability of getting the right answer (1/4) ## [1] 0.1877117 Or for two correct answers: dbinom(2, # Number of outcomes that we got right size = 10, # 10 trials (we have 10 questions) prob = 0.25) # Probability of getting the right answer (1/4) ## [1] 0.2815676 Or three: dbinom(3, # Number of outcomes that we got right size = 10, # 10 trials (we have 10 questions) prob = 0.25) # Probability of getting the right answer (1/4) ## [1] 0.2502823 And so on…. What if we want to find the probability of guessing all 10 questions correctly? That’s a magical result! dbinom(10, size = 10, prob = 0.25) ## [1] 9.536743e-07 #Round to 2 d.p. round(dbinom(10, size = 10, prob = 0.25), 2) ## [1] 0 So rare that it is almost zero, so never try to do the quiz just relying on your luck! :) 9.6 Cumulative Probability (Advanced) By adding the above we can also get a cumulative probability, meaning that we can study what would be the chance to get four or less right, meaning that you want to include the chances of getting zero, one, two, and three right as well. Graphically, we really want to analyse the probability mass here, given that all should sum up to one. To know the chances of getting one or two questions right we can sum the probabilities: dbinom(1, size = 10, prob = 0.25) + dbinom(1, size = 10, prob = 0.25) + dbinom(2, size = 10, prob = 0.25) ## [1] 0.656991 A faster way to see all the probabilities at once would be: all_probs &lt;- round(dbinom(x = c(0,1,2,3,4,5,6,7,8,9,10), prob = 0.25, size = 10),2) # Note that we use round to see the values to two decimal places all_probs ## [1] 0.06 0.19 0.28 0.25 0.15 0.06 0.02 0.00 0.00 0.00 0.00 You can then find out what the probability is of getting five or less answers right versus five or more answers right: # Five or less pbinom(q = 5, prob = 0.25, size = 10) ## [1] 0.9802723 Quite high chances :) # Five or more 1 - pbinom(q = 5, prob = 0.25, size = 10 ) ## [1] 0.01972771 Not that much! Can you see what we did? We found the probability of outcomes that are equal or less than five and then substracted it from total. Graphically we can show this as following: knitr::include_graphics(&#39;images/prob.jpg&#39;) There is an exercise for you to try this yourself with the distribution for TRUE/FALSE questions. Make sure that you revise this before coming back to the course in January as it will remind you of where we ended. 9.7 Revision Practice Rmd. Solutions This week’s practice is built around the key material we covered during the past eight weeks. You will need to load the data from Learn and then work with the key variables to provide descriptive statistics and visualisations. There is an extra practice at the end for you to work on the discrete probability distribution example as well. The dataset has information on participants that took part in the memory experiment: ’ IDs, Age, Memory score on three different tasks (Task A, Task B, and Placebo), and data on whether the participant received saw information/text twice. We are trying to explore whether treament (i.e. task) and seeing information twice may affect the memory scores. ID: 1 to 143 Age: 18-51 Memory Score: 1-100 (100 when remembered everything) Task: Task A, Task B, Placebo Saw_twice: Yes/No (if participant saw the text twice) # Load tidyverse library(tidyverse) # Read data in data &lt;- read.csv(&#39;week_9.csv&#39;) # Check what&#39;s inside head(data) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 54 Task A No ## 2 ID2 27 23 Task A Yes ## 3 ID3 25 32 Placebo Yes ## 4 ID4 25 38 Task B No ## 5 ID5 49 43 Task B Yes ## 6 ID6 47 32 Placebo No 9.7.1 Provide descriptive statistics for age and memory score variables There are different ways to do so using what we have learned so far: Look at each variable separately: # First age data %&gt;% summarise(mean = mean(Age), median = median(Age), sd = sd(Age)) ## mean median sd ## 1 34.39583 33.5 9.219331 # Then memory data %&gt;% summarise(mean = mean(Memory_score), median = median(Memory_score), sd = sd(Memory_score)) ## mean median sd ## 1 44.03472 41 20.54516 Or do it all in one go: data %&gt;% summarise(mean_age = mean(Age), mean_memory = mean(Memory_score), median_age = median(Age), median_memory = median (Memory_score), sd_age = sd(Age), sd_memory = sd(Memory_score)) ## mean_age mean_memory median_age median_memory sd_age sd_memory ## 1 34.39583 44.03472 33.5 41 9.219331 20.54516 9.7.2 Descriptives by groups Provide descriptives of the memory scores by task, and by whether someone saw the information on the task twice: # By treatment/task data %&gt;% group_by(Task) %&gt;% #note that we are adding group_by() to differentiate by a variable summarise(mean = mean(Memory_score), median = median(Memory_score), sd = sd(Memory_score)) ## # A tibble: 3 x 4 ## Task mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Placebo 43.2 41 21.9 ## 2 Task A 45.7 41.5 19.8 ## 3 Task B 43.4 42 20.2 # By whether someone saw information on the task twice data %&gt;% group_by(Saw_twice) %&gt;% summarise(mean = mean(Memory_score), median = median(Memory_score), sd = sd(Memory_score)) ## # A tibble: 2 x 4 ## Saw_twice mean median sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 44.1 42.5 20.6 ## 2 Yes 44.0 41 20.6 9.7.3 Visualise Provide distributions of age and memory scores. # Age ggplot(data = data, aes(x = Age)) + geom_histogram(colour = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Age (Years)&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Age&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Memory Scores ggplot(data = data, aes(x = Memory_score)) + geom_histogram(colour = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Memory Score (1-100)&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Memory Scores&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 9.7.4 Visualise a subset What about memory scores only for Task A? # Memory Scores for Task A ggplot(data = subset(data, Task %in% c(&#39;Task A&#39;)), aes(x = Memory_score)) + geom_histogram(colour = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Memory Score (1-100)&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Memory Scores (Task A)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can also use %&gt;% to do the same. Check this out: # Memory Scores for Task A data %&gt;% filter (Task == &#39;Task A&#39;) %&gt;% ggplot(data = ., #note how we replace the data with `.` which will allow us to use the specification above as our input aes(x = Memory_score)) + geom_histogram(colour = &#39;grey&#39;, fill = &#39;cornsilk&#39;) + labs(x = &#39;Memory Score (1-100)&#39;, y = &#39;Frequency&#39;, title = &#39;Histogram of Memory Scores (Task A)&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 9.7.5 Variable by a group What about a plot of Memory Score by Task? ggplot(data = data, aes(x = Task, y = Memory_score, fill = Task)) + geom_boxplot() Anything more advanced? Maybe you could try ggridges? library(ggridges) ggplot(data = data, aes(x = Memory_score, y = Task, fill=Task)) + geom_density_ridges() ## Picking joint bandwidth of 8.01 9.7.6 Create a new variable using mutate() Let’s change memory scores into percentages. How would you add an extra variable that converts scores (1-100) into percentages?? # We can use mutate (try first adding the new variable via piping) data %&gt;% mutate(Memory_score=Memory_score/100) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 0.54 Task A No ## 2 ID2 27 0.23 Task A Yes ## 3 ID3 25 0.32 Placebo Yes ## 4 ID4 25 0.38 Task B No ## 5 ID5 49 0.43 Task B Yes ## 6 ID6 47 0.32 Placebo No ## 7 ID7 28 0.51 Task A No ## 8 ID8 26 0.26 Placebo No ## 9 ID9 28 0.56 Placebo No ## 10 ID10 35 0.39 Task A No ## 11 ID11 48 0.32 Task B Yes ## 12 ID12 26 0.19 Placebo No ## 13 ID13 50 0.46 Task B No ## 14 ID14 22 0.26 Task B Yes ## 15 ID15 21 0.28 Task A No ## 16 ID16 44 0.48 Task A Yes ## 17 ID17 45 0.28 Task B Yes ## 18 ID18 40 0.76 Task A No ## 19 ID19 23 0.62 Placebo No ## 20 ID20 42 0.44 Placebo No ## 21 ID21 20 0.26 Task B Yes ## 22 ID22 34 0.30 Task A Yes ## 23 ID23 48 0.24 Task A No ## 24 ID24 48 0.82 Task B No ## 25 ID25 44 0.29 Task A Yes ## 26 ID26 44 0.47 Task B No ## 27 ID27 25 0.55 Task A No ## 28 ID28 48 0.56 Placebo No ## 29 ID29 31 0.30 Placebo No ## 30 ID30 22 0.44 Task B Yes ## 31 ID31 32 0.66 Task A Yes ## 32 ID32 22 0.12 Placebo No ## 33 ID33 29 1.24 Placebo Yes ## 34 ID34 33 0.59 Task A No ## 35 ID35 38 0.34 Task A No ## 36 ID36 31 0.81 Task A Yes ## 37 ID37 49 0.36 Task B Yes ## 38 ID38 25 0.08 Task B No ## 39 ID39 28 0.66 Task B No ## 40 ID40 28 0.62 Placebo No ## 41 ID41 42 0.13 Task A Yes ## 42 ID42 43 0.84 Task B No ## 43 ID43 47 0.60 Placebo Yes ## 44 ID44 28 0.05 Task B No ## 45 ID45 34 0.46 Task B Yes ## 46 ID46 33 0.72 Placebo No ## 47 ID47 24 0.41 Task A Yes ## 48 ID48 39 0.87 Placebo Yes ## 49 ID49 44 0.56 Task A No ## 50 ID50 49 0.22 Placebo No ## 51 ID51 40 0.94 Task B No ## 52 ID52 22 0.42 Task B Yes ## 53 ID53 32 0.33 Task A No ## 54 ID54 47 0.78 Task B Yes ## 55 ID55 37 0.49 Placebo No ## 56 ID56 51 0.19 Task A Yes ## 57 ID57 36 0.67 Task B No ## 58 ID58 22 0.28 Task B No ## 59 ID59 37 0.31 Task A Yes ## 60 ID60 42 0.38 Task B Yes ## 61 ID61 20 0.53 Task B Yes ## 62 ID62 25 0.29 Task B No ## 63 ID63 29 0.38 Task B Yes ## 64 ID64 44 0.53 Task B No ## 65 ID65 28 0.37 Task B No ## 66 ID66 19 0.77 Placebo No ## 67 ID67 42 0.46 Placebo Yes ## 68 ID68 32 0.14 Task B No ## 69 ID69 42 0.89 Placebo No ## 70 ID70 23 0.53 Placebo Yes ## 71 ID71 46 0.36 Task B Yes ## 72 ID72 35 0.55 Placebo Yes ## 73 ID73 23 0.18 Placebo Yes ## 74 ID74 25 0.31 Placebo No ## 75 ID75 38 0.41 Placebo Yes ## 76 ID76 33 0.42 Placebo Yes ## 77 ID77 24 0.34 Placebo No ## 78 ID78 47 0.80 Task A No ## 79 ID79 37 0.20 Task A Yes ## 80 ID80 40 0.25 Task B Yes ## 81 ID81 19 0.75 Task B Yes ## 82 ID82 39 0.42 Task A No ## 83 ID83 24 0.31 Task B Yes ## 84 ID84 27 0.53 Task B No ## 85 ID85 31 0.35 Task B No ## 86 ID86 41 0.28 Task B No ## 87 ID87 29 0.75 Task A Yes ## 88 ID88 43 0.51 Task B No ## 89 ID89 42 0.24 Task A No ## 90 ID90 31 0.31 Task A No ## 91 ID91 22 0.32 Placebo No ## 92 ID92 47 0.43 Placebo No ## 93 ID93 24 0.28 Placebo Yes ## 94 ID94 33 0.39 Task A Yes ## 95 ID95 47 0.34 Task A No ## 96 ID96 33 0.81 Task A No ## 97 ID97 28 0.41 Task B Yes ## 98 ID98 36 0.15 Task B No ## 99 ID99 44 0.71 Task A No ## 100 ID100 40 0.34 Task A Yes ## 101 ID101 21 0.20 Task A No ## 102 ID102 21 0.54 Task A No ## 103 ID103 40 0.67 Task B No ## 104 ID104 38 0.26 Placebo No ## 105 ID105 28 0.57 Placebo No ## 106 ID106 41 0.31 Placebo No ## 107 ID107 23 0.14 Task B No ## 108 ID108 50 0.65 Task A Yes ## 109 ID109 38 0.54 Task A No ## 110 ID110 40 0.34 Placebo No ## 111 ID111 42 0.50 Task B Yes ## 112 ID112 34 0.54 Task B No ## 113 ID113 32 0.22 Placebo Yes ## 114 ID114 46 0.82 Task A No ## 115 ID115 35 0.58 Task B Yes ## 116 ID116 31 0.30 Task B No ## 117 ID117 39 0.63 Task A No ## 118 ID118 29 0.55 Placebo No ## 119 ID119 20 0.26 Placebo Yes ## 120 ID120 26 0.31 Task A Yes ## 121 ID121 22 0.25 Task B Yes ## 122 ID122 50 0.37 Placebo No ## 123 ID123 34 0.52 Placebo No ## 124 ID124 44 0.46 Task B No ## 125 ID125 45 0.33 Task A No ## 126 ID126 22 0.33 Placebo Yes ## 127 ID127 33 0.37 Placebo No ## 128 ID128 43 0.27 Placebo No ## 129 ID129 26 0.45 Placebo No ## 130 ID130 47 0.59 Task B No ## 131 ID131 32 0.19 Task B No ## 132 ID132 42 0.53 Task A Yes ## 133 ID133 44 0.25 Task A No ## 134 ID134 42 0.03 Placebo No ## 135 ID135 28 0.70 Task B Yes ## 136 ID136 33 0.43 Task B No ## 137 ID137 20 0.19 Placebo No ## 138 ID138 46 0.29 Placebo No ## 139 ID139 41 0.45 Task A Yes ## 140 ID140 23 0.46 Placebo Yes ## 141 ID141 45 0.61 Placebo No ## 142 ID142 25 0.62 Task B No ## 143 ID143 28 0.41 Placebo No ## 144 ID144 32 0.65 Task A Yes We can see a new variable above but if you head your data it won’t appear: head(data) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 54 Task A No ## 2 ID2 27 23 Task A Yes ## 3 ID3 25 32 Placebo Yes ## 4 ID4 25 38 Task B No ## 5 ID5 49 43 Task B Yes ## 6 ID6 47 32 Placebo No That’s because we have not assigned it to a dataset. To do so, we will need to use &lt;- in the following way: new_data &lt;- data %&gt;% mutate(Memory_score=Memory_score/100) Check now: head(new_data) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 0.54 Task A No ## 2 ID2 27 0.23 Task A Yes ## 3 ID3 25 0.32 Placebo Yes ## 4 ID4 25 0.38 Task B No ## 5 ID5 49 0.43 Task B Yes ## 6 ID6 47 0.32 Placebo No 9.7.7 Subset observations using filter() Can you subset only Task A and Placebo from the data? We can use filter and then assign the filtered observations to an object too: data %&gt;% filter(Task == &#39;Task A&#39; | Task== &#39;Placebo&#39;) ## ID Age Memory_score Task Saw_twice ## 1 ID1 21 54 Task A No ## 2 ID2 27 23 Task A Yes ## 3 ID3 25 32 Placebo Yes ## 4 ID6 47 32 Placebo No ## 5 ID7 28 51 Task A No ## 6 ID8 26 26 Placebo No ## 7 ID9 28 56 Placebo No ## 8 ID10 35 39 Task A No ## 9 ID12 26 19 Placebo No ## 10 ID15 21 28 Task A No ## 11 ID16 44 48 Task A Yes ## 12 ID18 40 76 Task A No ## 13 ID19 23 62 Placebo No ## 14 ID20 42 44 Placebo No ## 15 ID22 34 30 Task A Yes ## 16 ID23 48 24 Task A No ## 17 ID25 44 29 Task A Yes ## 18 ID27 25 55 Task A No ## 19 ID28 48 56 Placebo No ## 20 ID29 31 30 Placebo No ## 21 ID31 32 66 Task A Yes ## 22 ID32 22 12 Placebo No ## 23 ID33 29 124 Placebo Yes ## 24 ID34 33 59 Task A No ## 25 ID35 38 34 Task A No ## 26 ID36 31 81 Task A Yes ## 27 ID40 28 62 Placebo No ## 28 ID41 42 13 Task A Yes ## 29 ID43 47 60 Placebo Yes ## 30 ID46 33 72 Placebo No ## 31 ID47 24 41 Task A Yes ## 32 ID48 39 87 Placebo Yes ## 33 ID49 44 56 Task A No ## 34 ID50 49 22 Placebo No ## 35 ID53 32 33 Task A No ## 36 ID55 37 49 Placebo No ## 37 ID56 51 19 Task A Yes ## 38 ID59 37 31 Task A Yes ## 39 ID66 19 77 Placebo No ## 40 ID67 42 46 Placebo Yes ## 41 ID69 42 89 Placebo No ## 42 ID70 23 53 Placebo Yes ## 43 ID72 35 55 Placebo Yes ## 44 ID73 23 18 Placebo Yes ## 45 ID74 25 31 Placebo No ## 46 ID75 38 41 Placebo Yes ## 47 ID76 33 42 Placebo Yes ## 48 ID77 24 34 Placebo No ## 49 ID78 47 80 Task A No ## 50 ID79 37 20 Task A Yes ## 51 ID82 39 42 Task A No ## 52 ID87 29 75 Task A Yes ## 53 ID89 42 24 Task A No ## 54 ID90 31 31 Task A No ## 55 ID91 22 32 Placebo No ## 56 ID92 47 43 Placebo No ## 57 ID93 24 28 Placebo Yes ## 58 ID94 33 39 Task A Yes ## 59 ID95 47 34 Task A No ## 60 ID96 33 81 Task A No ## 61 ID99 44 71 Task A No ## 62 ID100 40 34 Task A Yes ## 63 ID101 21 20 Task A No ## 64 ID102 21 54 Task A No ## 65 ID104 38 26 Placebo No ## 66 ID105 28 57 Placebo No ## 67 ID106 41 31 Placebo No ## 68 ID108 50 65 Task A Yes ## 69 ID109 38 54 Task A No ## 70 ID110 40 34 Placebo No ## 71 ID113 32 22 Placebo Yes ## 72 ID114 46 82 Task A No ## 73 ID117 39 63 Task A No ## 74 ID118 29 55 Placebo No ## 75 ID119 20 26 Placebo Yes ## 76 ID120 26 31 Task A Yes ## 77 ID122 50 37 Placebo No ## 78 ID123 34 52 Placebo No ## 79 ID125 45 33 Task A No ## 80 ID126 22 33 Placebo Yes ## 81 ID127 33 37 Placebo No ## 82 ID128 43 27 Placebo No ## 83 ID129 26 45 Placebo No ## 84 ID132 42 53 Task A Yes ## 85 ID133 44 25 Task A No ## 86 ID134 42 3 Placebo No ## 87 ID137 20 19 Placebo No ## 88 ID138 46 29 Placebo No ## 89 ID139 41 45 Task A Yes ## 90 ID140 23 46 Placebo Yes ## 91 ID141 45 61 Placebo No ## 92 ID143 28 41 Placebo No ## 93 ID144 32 65 Task A Yes Now, put it inside a new dataset, called reduced, you can also specify which variables you may want to keep. reduced_data &lt;- data %&gt;% filter(Task == &#39;Task A&#39; | Task== &#39;Placebo&#39;) %&gt;% select(ID, Age, Memory_score, Task, Saw_twice) 9.7.8 Sort via arrange() We can check the lowest and highest memory scores via sorting in each group: # Task A (lowest) data %&gt;% filter(Task == &quot;Task A&quot;) %&gt;% arrange(Memory_score) ## ID Age Memory_score Task Saw_twice ## 1 ID41 42 13 Task A Yes ## 2 ID56 51 19 Task A Yes ## 3 ID79 37 20 Task A Yes ## 4 ID101 21 20 Task A No ## 5 ID2 27 23 Task A Yes ## 6 ID23 48 24 Task A No ## 7 ID89 42 24 Task A No ## 8 ID133 44 25 Task A No ## 9 ID15 21 28 Task A No ## 10 ID25 44 29 Task A Yes ## 11 ID22 34 30 Task A Yes ## 12 ID59 37 31 Task A Yes ## 13 ID90 31 31 Task A No ## 14 ID120 26 31 Task A Yes ## 15 ID53 32 33 Task A No ## 16 ID125 45 33 Task A No ## 17 ID35 38 34 Task A No ## 18 ID95 47 34 Task A No ## 19 ID100 40 34 Task A Yes ## 20 ID10 35 39 Task A No ## 21 ID94 33 39 Task A Yes ## 22 ID47 24 41 Task A Yes ## 23 ID82 39 42 Task A No ## 24 ID139 41 45 Task A Yes ## 25 ID16 44 48 Task A Yes ## 26 ID7 28 51 Task A No ## 27 ID132 42 53 Task A Yes ## 28 ID1 21 54 Task A No ## 29 ID102 21 54 Task A No ## 30 ID109 38 54 Task A No ## 31 ID27 25 55 Task A No ## 32 ID49 44 56 Task A No ## 33 ID34 33 59 Task A No ## 34 ID117 39 63 Task A No ## 35 ID108 50 65 Task A Yes ## 36 ID144 32 65 Task A Yes ## 37 ID31 32 66 Task A Yes ## 38 ID99 44 71 Task A No ## 39 ID87 29 75 Task A Yes ## 40 ID18 40 76 Task A No ## 41 ID78 47 80 Task A No ## 42 ID36 31 81 Task A Yes ## 43 ID96 33 81 Task A No ## 44 ID114 46 82 Task A No What about the highest in Task B? # Task B (the highest) data %&gt;% filter(Task == &quot;Task B&quot;) %&gt;% arrange(desc(Memory_score)) ## ID Age Memory_score Task Saw_twice ## 1 ID51 40 94 Task B No ## 2 ID42 43 84 Task B No ## 3 ID24 48 82 Task B No ## 4 ID54 47 78 Task B Yes ## 5 ID81 19 75 Task B Yes ## 6 ID135 28 70 Task B Yes ## 7 ID57 36 67 Task B No ## 8 ID103 40 67 Task B No ## 9 ID39 28 66 Task B No ## 10 ID142 25 62 Task B No ## 11 ID130 47 59 Task B No ## 12 ID115 35 58 Task B Yes ## 13 ID112 34 54 Task B No ## 14 ID61 20 53 Task B Yes ## 15 ID64 44 53 Task B No ## 16 ID84 27 53 Task B No ## 17 ID88 43 51 Task B No ## 18 ID111 42 50 Task B Yes ## 19 ID26 44 47 Task B No ## 20 ID13 50 46 Task B No ## 21 ID45 34 46 Task B Yes ## 22 ID124 44 46 Task B No ## 23 ID30 22 44 Task B Yes ## 24 ID5 49 43 Task B Yes ## 25 ID136 33 43 Task B No ## 26 ID52 22 42 Task B Yes ## 27 ID97 28 41 Task B Yes ## 28 ID4 25 38 Task B No ## 29 ID60 42 38 Task B Yes ## 30 ID63 29 38 Task B Yes ## 31 ID65 28 37 Task B No ## 32 ID37 49 36 Task B Yes ## 33 ID71 46 36 Task B Yes ## 34 ID85 31 35 Task B No ## 35 ID11 48 32 Task B Yes ## 36 ID83 24 31 Task B Yes ## 37 ID116 31 30 Task B No ## 38 ID62 25 29 Task B No ## 39 ID17 45 28 Task B Yes ## 40 ID58 22 28 Task B No ## 41 ID86 41 28 Task B No ## 42 ID14 22 26 Task B Yes ## 43 ID21 20 26 Task B Yes ## 44 ID80 40 25 Task B Yes ## 45 ID121 22 25 Task B Yes ## 46 ID131 32 19 Task B No ## 47 ID98 36 15 Task B No ## 48 ID68 32 14 Task B No ## 49 ID107 23 14 Task B No ## 50 ID38 25 8 Task B No ## 51 ID44 28 5 Task B No 9.7.9 Let’s do some specific count() using filter() First check how many people we have in each Task group: data %&gt;% count(Task) ## # A tibble: 3 x 2 ## Task n ## &lt;fct&gt; &lt;int&gt; ## 1 Placebo 49 ## 2 Task A 44 ## 3 Task B 51 Can you show how many people above 40 years of age and saw the information on the task twice in each Task group? data %&gt;% filter(Age &gt;40) %&gt;% filter(Saw_twice == &#39;Yes&#39;) %&gt;% count(Task) ## # A tibble: 3 x 2 ## Task n ## &lt;fct&gt; &lt;int&gt; ## 1 Placebo 2 ## 2 Task A 7 ## 3 Task B 8 For the last one, show how people with the highest memory scores are split by task. Use a memory score threshold of 50 out of 100: data %&gt;% filter(Memory_score &gt; 50) %&gt;% count(Task) ## # A tibble: 3 x 2 ## Task n ## &lt;fct&gt; &lt;int&gt; ## 1 Placebo 16 ## 2 Task A 19 ## 3 Task B 17 Task A has greater queanity of high memory scores. Nicely done! If you got to the end, you have now succefully practiced all the key code and functions we have learnt in previous weeks. Play more if you like for the practice. 9.8 Extra Probability Practice Work with the distribution we created in the tutorial for guessing on homework quizzes. We want to analyse how likely you are to get specific numbers of question right. 9.8.1 TRUE/FALSE questions Work with the TRUE/FALSE example we have seen in the tutorial. What if the multiple choice had only two choices for the answer (i.e. TRUE or FALSE questions)? The right answer will now have a probability of 0.5 if you were to guess at random. Create a tibble() to show this: homework_guess_true_false &lt;- tibble(right_guess = rbinom(n = 100, size = 10, prob = 0.5)) 9.8.2 Count the occurencies homework_guess_true_false %&gt;% count(right_guess) ## # A tibble: 9 x 2 ## right_guess n ## &lt;int&gt; &lt;int&gt; ## 1 1 2 ## 2 2 3 ## 3 3 7 ## 4 4 25 ## 5 5 26 ## 6 6 19 ## 7 7 13 ## 8 8 3 ## 9 9 2 9.8.3 Plot ggplot(data = homework_guess_true_false, aes(x = right_guess)) + geom_bar(fill = &#39;lightgreen&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;count&#39;) + theme_minimal() Plot with y being a probability: ggplot(data = homework_guess_true_false, aes(x = right_guess)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = &#39;lightgreen&#39;) + xlim(0,10) + labs(x = &#39;Right Guess&#39;, y = &#39;Probability&#39;) + theme_minimal() 9.8.4 Use dbinom() to study the probability Check what the probability is of getting only one question right when guessing at random: dbinom(1, size = 10, prob = 0.5) ## [1] 0.009765625 Or for five correct answers: dbinom(5, size = 10, prob = 0.5) ## [1] 0.2460938 Or eight: dbinom(8, size = 10, prob = 0.5) ## [1] 0.04394531 Or put it all together at once (make sure that the probability is 0.5): all_probs &lt;- round(dbinom(x = c(0,1,2,3,4,5,6,7,8,9,10), prob = 0.5, size = 10),2) # Note that we use round to see the values to two decimal places all_probs ## [1] 0.00 0.01 0.04 0.12 0.21 0.25 0.21 0.12 0.04 0.01 0.00 9.8.5 Less than five or more than five? We can also study what the chances are of getting less than five questions right versus more than five questions right in a TRUE/FALSE setting (check your notes online). # Five or less pbinom(q = 5, prob = 0.5, size = 10) ## [1] 0.6230469 # Five or more 1 - pbinom(q = 5, prob = 0.5, size = 10) ## [1] 0.3769531 Better chances compared to when you are doing a quiz with four options! :) "],
["week-10-lab-test.html", "Chapter 10 Week 10: Lab test", " Chapter 10 Week 10: Lab test This week there is a Lab test. "],
["chap-sampling-distributions.html", "Chapter 11 Sampling distributions 11.1 Recap 11.2 Population vs sample 11.3 Population parameter vs sample statistic 11.4 Sampling distributions 11.5 The standard error of a statistic 11.6 The effect of sample size on the sampling distribution 11.7 Take-home message 11.8 Lab: Hollywood movies 11.9 Summary 11.10 Glossary 11.11 References", " Chapter 11 Sampling distributions Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand the difference between a population parameter and a sample statistic. LO2. Understand that a sampling distribution shows how sample statistics vary from sample to sample. LO3. Understand the effect of sample size on the sampling distribution, and how to quantify the variability of a statistic. Reading This week’s reading is Chapter 7 of the book by Chester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman and Hall/CRC, 2019. Freely available online at: https://moderndive.com/ 11.1 Recap Figure 11.1: The data analysis pipeline. In Semester 1 (Weeks 1-10), we started walking through the required steps of a statistical investigation, conveniently summarized in Figure 11.1. Suppose that you are interested in a research question that can be answered by collecting data on some statistical units. Once collected, you (a) import/load the data into R; (b) tidy them so that each column corresponds to a single variable of interest; (c) transform the variables if needed; (d) visualise your data and inspect for unusual values; (e) fit statistical models to the data; and (f) communicate your results and conclusions to the wider community. Note: The inner cycle (c-d-e) might need to be re-iterated a few times. In the first semester we saw: how to import data into R; how to tidy datasets (for example by making sure that some variables are factors); how to transform variables (e.g. standardizing via scale(), log-transforming data, …); how to visualise the data using the ggplot2 package. In this semester we will focus on modelling, and communicating our findings to the wider community. 11.2 Population vs sample Typically, it is either infeasible in terms of time or cost to perform an exhaustive data collection on the entire population of interest (also known as census). This is why, for instance, the UK Office for National Statistics only performs a census every 10 years and the next one will be in 2021. To save time and money, we typically record the variables of interest on a smaller subset of the entire population, also known as a sample. In order to make sure that any conclusions we draw from the sample are generalisable to the wider population, this sample needs to be taken at random. This avoids representation bias, where some units are less represented than others, which would lead to wrong conclusions for the entire population. ► Example Average montly salary in Sweden Suppose you are interested in the average monthly salary of people working in Sweden. Unfortunately, you neither have the time nor the money to go to Sweden and ask each single person their own salary. Hence, you decide to ask some people at random. What are the problems of the following sample selection criteria? Asking 500 random people from Facebook that live in Sweden. Asking 1000 random people from the web that live in Sweden. Calling 200 phone numbers from the phone book. Asking 500 people working near the central bank of Sweden. ► Solution All four criteria lead to samples that are not representative of the entire population. This is because the population units will not have the same probability of being included in the sample: some people will have a higher chance of being included in the sample than others. This leads to sampling bias. Because of this, any conclusion we might make from the sample is not generalisable to the whole population. People that do not have Facebook are not considered in the sample. People that do not use the Internet are not represented. People without a landline phone are not included. People working in this area will very likely have higher salaries than the rest of the country. This sample does not represent fairly people working in farming or other sectors. In Week 8 of Semester 1, we discussed how to obtain sample data from a bigger population of interest; this is known as data collection. We now take the opposite direction as we try to use the information from the sample data to draw conclusions about the entire population. This is summarised in Figure 11.2. Furthermore, we will spend some time assessing how accurate our conclusions about the entire population are. Statistical inference Statistical inference is the process of using the sample data to draw conclusions about the entire population. Figure 11.2: Data collection vs statistical inference Caution: The inferential process assumes that the sample is randomly drawn from the population of interest. Any sample selection method that is biased, i.e. leading to samples which are not representative of the entire population, will mean that the results we obtain from the data in the sample can not be generalised to the entire population. In the average salary of Sweden example, none of the four sample selection strategies consisted of random sampling. They systematically excluded some people from the samples, leading to samples that were not representative of the population. For this reason, the average salary computed on any sample could not have been used to draw conclusions about the average salary in Sweden. 11.3 Population parameter vs sample statistic To clarify whether we are referring to the entire population or a sample, we use the term parameter when referring to a numerical summary of the entire population, and the term statistic for a numerical summary of the sample. Parameter vs statistic A parameter is a number describing some aspect of the population. A statistic is a number that is computed from the data in a sample. ► Example Average price of goods sold by ACME Corporation Suppose you work for a company that is interested in buying ACME Corporation3 and your boss wants to know within the next hour what is the average price of goods sold by that company and how the prices of the goods they sell differ from each other. Since ACME Corporation has such a big mail order catalogue, see Figure 11.3, we will assume that the company sells many products. Furthermore, we only have the catalogue in paper-form and no online list of prices is available. Figure 11.3: Product catalogue of ACME corporation. Identify the population of interest and the population parameters. Can we compute the parameters within the next hour? How would you proceed in estimating the population parameters if you just had time to read through 100 item descriptions? Would you pick the first 100 items or would you pick 100 random page numbers? State which statistics you would use to estimate the population parameters. ► Solution The population of interest is all products sold by ACME Corporation. The population parameters are the mean price \\(\\mu\\) and the standard deviation \\(\\sigma\\). Because the catalogue has so many pages, we can not compute the population parameters within the next hour. We must estimate the population mean and standard deviation from a sample of size \\(n = 100\\). We should choose the items entering the sample at random, to avoid sampling bias. If we were to choose 100 consecutive items, we might end up with a very good estimate of the average price for the category those contecutive items belong to (e.g. gardening). However, this would not be a good estimate of the overall price across the multiple categories of products sold. Our best guess of the population mean would be the sample mean, \\(\\bar{x}\\), and our best guess of the population standard deviation would be the sample standard deviation, denoted \\(s\\) or \\(\\hat{\\sigma}\\). From this example, you can see that the population parameter and the sample statistic generally have the same name. However, these are often written with different symbols to convey with just one letter: what feature they represent; if it is a population quantity or a quantity computed on a sample. The following table summarizes standard notation for some population parameters, typically unknown, and the corresponding “best guesses” computed on a sample. Notation for common parameters and statistics. Population parameter Sample statistic Mean \\(\\mu\\) \\(\\bar{x}\\) or \\(\\hat{\\mu}\\) Standard deviation \\(\\sigma\\) \\(s\\) or \\(\\hat{\\sigma}\\) Proportion \\(p\\) \\(\\hat{p}\\) The greek letter \\(\\mu\\) (mu) is used as a parameter to denote the population mean/average, while \\(\\bar{x}\\) (x-bar) or \\(\\hat{\\mu}\\) (mu-hat) is used as a statistic for the mean computed on a sample. The greek letter \\(\\sigma\\) (sigma) is used as a parameter to denote the population standard deviation, while \\(s\\) or \\(\\hat{\\sigma}\\) (sigma-hat) is used as a statistic for the standard deviation of the collected sample. The letter \\(p\\) is used as a parameter to denote the population proportion, while \\(\\hat{p}\\) (p-hat) is used as a statistic for the sample proportion. ► Example Proportion of UK people aged between 25 and 34 with a Bachelor’s degree or higher The last UK Census, done in 2011, reports that 40% of people aged 25 to 34 years had a degree-level or above qualification. Suppose that in a random sample of \\(n = 200\\) UK residents who are between 25 and 34 years old, 58 of them have a Bachelor’s degree or higher. Using the appropriate notation, state what is the population parameter and what is the sample statistic. ► Solution The population parameter is the proportion of all UK people aged between 25 and 34 years old with a Bachelor’s degree or higher: \\(p = 0.4\\). The sample statistic is the proportion with a Bachelor’s degree or higher for those in the sample: \\(\\hat{p} = 58/200 = 0.29\\). As discussed, it is generally infeasible to be able to know the value of the population parameter exactly. This would require collecting data for the entire population and then computing the required quantity. Instead, we typically select a random sample from the population, and then compute the quantity of interest for the sample data. We then use this sample statistic as a (point) estimate or best guess of the population parameter. 11.4 Sampling distributions A population parameter is typically considered to be a fixed value. A sample statistic, however, varies from sample to sample. This is because a sample statistic, i.e. a quantity computed on a sample, will depend on which units are selected to enter the sample. This can be seen as a downside to sampling, but we must remember that time and resources often prevent us from finding out the true value of a population parameter, whereas it is comparatively easy to collect a sample and compute a statistic. A fundamental question that arises when we estimate an unknown population parameter by a sample statistic is: how accurate do we believe our best guess to be? Remembering that the parameter is fixed, while the statistics varies from sample to sample, we might proceed in answering this question by looking at how the computed statistic varies depending on the sample. ► Example Average yearly salary of American National Football League (NFL) players We will read a file containing the yearly salaries (in millions of dollars) for all players being paid at the start of 2015 by a National Football League (NFL) team. This entire dataset represents the population of all National Football League players in 2015.4 We are interested in the following question: what was the average yearly salary of a NFL player in 2015? In this particular example, we actually do know the population parameter, because we have data on the whole population. We resort to sampling, however, to show how the value of a statistic varies when computed on different samples. Read in the data and state, with appropriate notation, what is the population parameter. Select a random sample of \\(n = 50\\) players and compute the average yearly salary for the players in the sample. How does your statistic compare to the population parameter? Take another sample of size \\(n = 50\\) players and compute the average salary for the new sample. How does it compare with the mean from the previous sample? ► Solution Do not worry if some of these functions are new to you, try to follow along and understand what each does. Question 1: Read in the data and state, with appropriate notation, what is the population parameter. Let us start by loading the data, inspecting the initial rows of the tibble, and checking the dimensions: library(tidyverse) nfl &lt;- read_tsv(&#39;https://edin.ac/2TexAFA&#39;) head(nfl) ## # A tibble: 6 x 5 ## Player Position Team TotalMoney YearlySalary ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aaron Rodgers QB Packers 110 22 ## 2 Russell Wilson QB Seahawks 87.6 21.9 ## 3 Ben Roethlisberger QB Steelers 87.4 21.8 ## 4 Philip Rivers QB Chargers 83.2 20.8 ## 5 Cam Newton QB Panthers 104. 20.8 ## 6 Matt Ryan QB Falcons 104. 20.8 dim(nfl) ## [1] 2099 5 The data comprise five measurements on 2099 units. We have data on the players’ names, positions, team names, total money they received while playing on the NFL (cumulative over multiple years), and their yearly salary. Since we are only interested in the yearly salary of the players, we can select only the relevant columns from the data: nfl &lt;- nfl %&gt;% select(Player, YearlySalary) head(nfl) ## # A tibble: 6 x 2 ## Player YearlySalary ## &lt;chr&gt; &lt;dbl&gt; ## 1 Aaron Rodgers 22 ## 2 Russell Wilson 21.9 ## 3 Ben Roethlisberger 21.8 ## 4 Philip Rivers 20.8 ## 5 Cam Newton 20.8 ## 6 Matt Ryan 20.8 We now calculate the average yearly salary for all players: nfl_mean &lt;- nfl %&gt;% summarise(avg = mean(YearlySalary)) nfl_mean ## # A tibble: 1 x 1 ## avg ## &lt;dbl&gt; ## 1 2.24 The population parameter is the average yearly salary in 2015, \\(\\mu =\\) 2.24. Question 2: Select a random sample of \\(n = 50\\) players and compute the average yearly salary for the players in the sample. How does your statistic compare to the population parameter? In order to randomly sample players, we will use the package moderndive. If you do not have it installed, run the following command: install.packages(&quot;moderndive&quot;) We then load the package and create a sample of size \\(n = 50\\) players: library(moderndive) nfl_sample_1 &lt;- nfl %&gt;% rep_sample_n(size = 50) nfl_sample_1 ## # A tibble: 50 x 3 ## # Groups: replicate [1] ## replicate Player YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Craig Dahl 0.87 ## 2 1 Jarrett Bush 1.75 ## 3 1 Shayne Graham 0.98 ## 4 1 Christian Covington 0.593 ## 5 1 Pat McAfee 2.9 ## 6 1 Ron Parker 5 ## 7 1 Bryan Walters 0.723 ## 8 1 John Greco 2.09 ## 9 1 Trenton Robinson 0.68 ## 10 1 Paul Worrilow 0.496 ## # … with 40 more rows This returns a tibble of size \\(50 \\times 3\\). The second column shows the names of the players included in the sample. The additional first column is equal to one for all cases in the sample. This indicates that the 50 rows all belong to the first sample. You are also invited to interactively scroll through the sample by typing the command View(nfl_sample_1). Let’s now compute the average salary for the players in the sample: nfl_sample_1_mean &lt;- nfl_sample_1 %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_1_mean ## # A tibble: 1 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.10 We can see that the average salary in our sample is \\(\\bar{x} =\\) 2.1 million dollars. The sample mean, 2.1, is close to the population mean, 2.24, even if not exactly the same. We are not surprised of this result: we do not expect the mean of every sample to be exactly equal to the population mean, but we do hope that they are somewhat close. Question 3: Take another sample of size \\(n = 50\\) players and compute the average salary for the new sample. How does it compare with the mean from the previous sample? Let’s take another random sample of size 50 and compute the mean salary: nfl_sample_2 &lt;- nfl %&gt;% rep_sample_n(size = 50) nfl_sample_2 ## # A tibble: 50 x 3 ## # Groups: replicate [1] ## replicate Player YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Trevor Scott 0.73 ## 2 1 Scott Crichton 0.755 ## 3 1 NaVorro Bowman 9.05 ## 4 1 Brian Dixon 0.51 ## 5 1 Kyle Williams 0.745 ## 6 1 Tony Jefferson 0.498 ## 7 1 Anthony Hitchens 0.664 ## 8 1 Cedric Ogbuehi 2.33 ## 9 1 Jordan Poyer 0.555 ## 10 1 Brent Celek 4.88 ## # … with 40 more rows nfl_sample_2_mean &lt;- nfl_sample_2 %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_2_mean ## # A tibble: 1 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.05 The statistic computed on the second sample is \\(\\bar{x} =\\) 2.05. Again, this is similar to the population parameter, \\(\\mu =\\) 2.24. We also note that the mean computed on the second sample (2.05) is different from the mean computed on the first sample (2.1). We could have immediately obtained two samples, each of size \\(n = 50\\) units. This requires that we repeat/replicate two times the activity of sampling 50 units from the entire population. Every replicated sampling of 50 units from the population is done “from scratch”, so that the population is always unchanged from replicate to replicate. This is done in the function rep_sample_n(size = 50) by including the extra argument reps = 2: nfl_samples &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 2) nfl_samples ## # A tibble: 100 x 3 ## # Groups: replicate [2] ## replicate Player YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Andrew Luck 7.65 ## 2 1 Antonio Brown 8.39 ## 3 1 Patrick Chung 2.4 ## 4 1 Chris Williams 3.29 ## 5 1 Chris Lewis-Harris 0.585 ## 6 1 Lance Louis 1.01 ## 7 1 Adam Humphries 0.525 ## 8 1 Wade Keliikipi 0.525 ## 9 1 Dustin Colquitt 3.75 ## 10 1 Tyronne Green 0.745 ## # … with 90 more rows If you explore this tibble, it has \\(50 \\times 2 = 100\\) rows. The column replicate takes value 1 for the first 50 rows, and the value 2 for the next 50 rows. This indicates that the players in rows 1 to 50 are selected to be in the first sample, while the players in rows 51 to 100 are those selected to be in the second sample. We can now compute the mean yearly salary for each of the two samples. The salaries of the 50 players in the first sample (replicate = 1) will be summarised by a single mean value, and the salaries of the 50 players in the second sample (replicate = 2) will be summarised into another mean value. In the end we will have a tibble with just two rows: nfl_sample_means &lt;- nfl_samples %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_means ## # A tibble: 2 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.57 ## 2 2 2.19 We see that both are close to the population parameter which, if we remember, is equal to 2.24. Figure 11.4 a histogram of the two means, with a vertical red line denoting the true population mean: ggplot(nfl_sample_means, aes(x = avg)) + geom_histogram(color = &quot;white&quot;) + geom_vline(xintercept = pull(nfl_mean, avg), color = &quot;red&quot;, size = 1) + labs(x = expression(bar(x))) Figure 11.4: Sample means from two samples of size \\(n = 50\\) with population mean \\(\\mu\\) marked by a red vertical line. Clearly, we can extend the repeated sampling procedure to more than just two samples of size = 50. Let us now obtain 2000 replicated samples, all of size 50, from the same NFL population: nfl_samples &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 2000) nfl_samples ## # A tibble: 100,000 x 3 ## # Groups: replicate [2,000] ## replicate Player YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Matthew Masifilo 0.555 ## 2 1 Kyle Juszczyk 0.615 ## 3 1 Gabe Jackson 0.714 ## 4 1 Akeem Ayers 3 ## 5 1 Andy Mulumba 0.497 ## 6 1 Corey Linsley 0.601 ## 7 1 Billy Turner 0.787 ## 8 1 Bennett Jackson 0.435 ## 9 1 Chase Daniel 3.33 ## 10 1 Scott Lutrus 0.465 ## # … with 99,990 more rows This tibble has \\(50 \\times 2000 = 100,000\\) rows. The first 50 players are part of the 1st sample, the next 50 belong to the 2nd sample, and so on… We can now compute the mean of each of the 2000 samples of size 50, obtaining a tibble of 2000 sample means: nfl_sample_means &lt;- nfl_samples %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_means ## # A tibble: 2,000 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.74 ## 2 2 1.69 ## 3 3 2.44 ## 4 4 2.29 ## 5 5 2.37 ## 6 6 2.63 ## 7 7 2.46 ## 8 8 1.99 ## 9 9 2.29 ## 10 10 2.28 ## # … with 1,990 more rows Let us plot the distribution of the sample mean for 2000 random samples, and superimpose in red the true value of the population parameter \\(\\mu\\): ggplot(nfl_sample_means, aes(x = avg)) + geom_histogram(color = &quot;white&quot;) + geom_vline(xintercept = pull(nfl_mean, avg), color = &quot;red&quot;, size = 1) + labs(x = expression(bar(x))) Figure 11.5: Sampling distribution of the mean for \\(n = 50\\) with population mean \\(\\mu\\) marked by a red vertical line. Figure 11.5 shows the values of the sample mean computed from sample to sample. Hence, it shows the variability of the sample mean induced by sampling variation. Such a plot is fundamental in statistical inference, and it is called sampling distribution. Sampling distribution The sampling distribution of a statistic shows the distribution of the statistic for different samples of the same size from the same population. Clearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, … This requires the following steps: Obtaining multiple samples, all of the same size, from the same population; For each sample, calculate the value of the statistic; Plot the distribution of the computed statistics. In Figure 11.5, the distribution of sample means is centred around the population mean \\(\\mu\\) and has a shape that is similar to a bell. This can be generalised further: Centre and shape of a sampling distribution Centre: If samples are randomly selected, the sampling distribution will be centred around the population parameter. (Accuracy) Shape: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will be symmetric and bell-shaped. (Central Limit Theorem) 11.5 The standard error of a statistic The variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be. In Semester 1 you saw how to summarize the variability of data using the standard deviation: sd(). So, the variability of a statistic can be quantified by calculating the standard deviation of its sampling distribution. Technically, this is not different from an ordinary standard deviation as the code and formula is the same. However, the spread of a sample statistic is so fundamental that it has its own name: the standard error of the statistic. In other words, we use: standard deviation to denote the variability among the values in a particular sample; standard error to denote the variability of the statistics computed on many samples. Standard error The standard error of a statistic, denoted \\(SE\\), is the standard deviation of its sampling distribution. The standard error measures the typical error when estimating the population parameter with the sample statistic. You can think of the SE as a “typical distance” of a sample statistic from the population parameter. 11.6 The effect of sample size on the sampling distribution It is of interest to see how the sampling distribution of the mean salary, shown in Figure 11.5 for a sample size \\(n = 50\\), changes with the sample size. In the following code chunk we compute the sampling distribution of the mean for samples of size \\(n = 50\\), \\(n = 100\\), and \\(n = 500\\), always repeating the sampling from the population 2000 times: nfl_sample_means_n_50 &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 2000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_means_n_100 &lt;- nfl %&gt;% rep_sample_n(size = 100, reps = 2000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_means_n_500 &lt;- nfl %&gt;% rep_sample_n(size = 500, reps = 2000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) We now combine the datasets for different sample sizes into a unique tibble, adding a column with the sample size, and then plot the distributions for different sample sizes. Remember that mutate() takes a tibble and adds or changes a column. The function bind_rows() takes multiple tibbles and stacks them under each other. nfl_sample_means_vary_n &lt;- bind_rows( nfl_sample_means_n_50 %&gt;% mutate(n = 50), nfl_sample_means_n_100 %&gt;% mutate(n = 100), nfl_sample_means_n_500 %&gt;% mutate(n = 500) ) ggplot(nfl_sample_means_vary_n, aes(x = avg)) + geom_histogram(color = &quot;white&quot;) + geom_vline(xintercept = pull(nfl_mean, avg), color = &quot;red&quot;, size = 1) + facet_grid(rows = vars(n), labeller = label_both) + labs(x = expression(bar(x)), title = &quot;Distribution of the sample mean for samples of size 50, 100, 500&quot;) Figure 11.6: Three sampling distributions of the mean, with population mean \\(\\mu\\) marked by a red vertical line. Figure 11.6 shows that as the sample size increases, the variability of the sampling distributions decreases, hence the standard error of the statistic decreases as the sample size increases. We can create a tibble that shows, for each sample size, the standard error of the sample mean: nfl_sample_means_vary_n %&gt;% group_by(n) %&gt;% summarise(SE = sd(avg)) ## # A tibble: 3 x 2 ## n SE ## &lt;dbl&gt; &lt;dbl&gt; ## 1 50 0.421 ## 2 100 0.299 ## 3 500 0.121 As we discussed, the tibble shows that as the sample size \\(n\\) increases, the standard error \\(SE\\) decreases. The larger the sample size, the lower the typical error of our estimate will be, and for every sample we will obtain a calculated statistic that is more similar to the population parameter. 11.7 Take-home message You might be wondering: why did we take repeated samples of size \\(n\\) from the population when, in practice, we can only afford to take one? This is a good question. We have taken multiple samples to show how the estimation error varies with the sample size. We saw that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more precise statistics, i.e. the estimates are more concentrated around the true parameter value. This teaches us that, when we have to design a study, it is better to obtain just one sample with size \\(n\\) as large as we can afford. ► Example What would the sampling distribution look like if we could afford to take samples as big as the entire population, i.e. of size \\(n = N\\)? ► Solution If we could afford to census the entire population, then we would find the exact value of the parameter. By taking repeated samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one. To summarize: We have high precision when the estimates are less variable, and this happens for a large sample size. We have high accuracy when our sampling strategy is not biased, and this happens when we do random sampling. High accuracy means that our sampling distribution is centred on the true parameter value. These points are visually displayed in Figure 11.7, where the population parameter is represented as the centre of the target: Figure 11.7: Comparing accuracy and precision. (From www.moderndive.com) 11.8 Lab: Hollywood movies The following code chunk reads in data from over 900 Hollywood movies produced between 2007 and 2013. Consider it as the entire population of movies produced in Hollywood in that time period. hollywood &lt;- read_tsv(&#39;https://edin.ac/2N9yHms&#39;) hollywood ## # A tibble: 970 x 16 ## Movie LeadStudio RottenTomatoes AudienceScore Story Genre TheatersOpenWeek OpeningWeekend ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Spid… Sony 61 54 Meta… Acti… 4252 151. ## 2 Shre… Paramount 42 57 Quest Anim… 4122 122. ## 3 Tran… Paramount 57 89 Mons… Acti… 4011 70.5 ## 4 Pira… Disney 45 74 Resc… Acti… 4362 115. ## 5 Harr… Warner Br… 78 82 Quest Adve… 4285 77.1 ## 6 I Am… Warner Br… 69 69 Quest Thri… 3606 77.2 ## 7 The … Universal 93 91 Purs… Thri… 3660 69.3 ## 8 Nati… Disney 31 72 The … Thri… 3832 44.8 ## 9 Alvi… Fox 26 73 Come… Anim… 3475 44.3 ## 10 300 Warner Br… 60 90 Sacr… Acti… 3103 70.9 ## # … with 960 more rows, and 8 more variables: BOAvgOpenWeekend &lt;dbl&gt;, DomesticGross &lt;dbl&gt;, ## # ForeignGross &lt;dbl&gt;, WorldGross &lt;dbl&gt;, Budget &lt;dbl&gt;, Profitability &lt;dbl&gt;, OpenProfit &lt;dbl&gt;, ## # Year &lt;dbl&gt; Among the recorded variables, three will be of interest: Movie; Genre; Budget. ► Question Extracting relevant variables Extract from the hollywood tibble the three variables of interest (Movie, Genre, Budget) and keep the movies for which we have all information (no missing entries). ► Solution We can extract variables using the function select(), while to keep the rows for which we have all measurements we use na.omit(): hollywood &lt;- hollywood %&gt;% select(Movie, Genre, Budget) %&gt;% na.omit hollywood ## # A tibble: 665 x 3 ## Movie Genre Budget ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Spider-Man 3 Action 258 ## 2 Shrek the Third Animation 160 ## 3 Transformers Action 150 ## 4 Pirates of the Caribbean: At World&#39;s End Action 300 ## 5 Harry Potter and the Order of the Phoenix Adventure 150 ## 6 I Am Legend Thriller 150 ## 7 The Bourne Ultimatum Thriller 110 ## 8 National Treasure: Book of Secrets Thriller 130 ## 9 Alvin and the Chipmunks Animation 70 ## 10 300 Action 65 ## # … with 655 more rows ► Question Proportion of comedy movies What is the population proportion of comedy movies? What is an estimate of the proportion of comedy movies using a sample of size 20? Using the appropriate notation, report your results in one or two sentences. ► Solution prop_comedy &lt;- hollywood %&gt;% summarise(prop = mean(Genre == &quot;Comedy&quot;)) %&gt;% pull(prop) prop_comedy ## [1] 0.2541353 sample_prop_comedy &lt;- hollywood %&gt;% rep_sample_n(size = 20) %&gt;% summarise(prop = mean(Genre == &quot;Comedy&quot;)) %&gt;% pull(prop) sample_prop_comedy ## [1] 0.2 The population proportion of comedy movies is \\(p =\\) 0.25, while the proportion of comedy movies in the sample is \\(\\hat{p} =\\) 0.2. ► Question Sampling distributions What is a sampling distribution? ► Solution The sampling distribution is the distribution of the values that a statistic takes on different samples of the same size and from the same population. ► Question Sampling distribution of the proportion Compute the sampling distribution of the proportion of comedy movies for sample size \\(n = 20\\), using 1000 replicates. Is it centred at the population value? ► Solution sample_props &lt;- hollywood %&gt;% rep_sample_n(size = 20, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(prop = mean(Genre == &#39;Comedy&#39;)) ggplot(sample_props, aes(x = prop)) + geom_histogram(color = &#39;white&#39;) + geom_vline(xintercept = prop_comedy, color = &#39;red&#39;, size = 1) + labs(x = expression(hat(p))) Figure 11.8: Sampling distribution of the proportion for \\(n = 20\\) with population parameter \\(p\\) marked by a red vertical line. Yes, Figure 11.8 shows that the distribution is almost bell-shaped and centred at the population parameter. ► Question Standard error Using the replicated samples from the previous question, what is the standard error of the sample proportion of comedy movies? ► Solution The standard error of the sample proportion is simply the standard deviation of the distribution of sample proportions for many samples. Since we have already computed the proportions for 1000 samples in the previous question, we just have to compute their variability using the standard deviation: se_prop &lt;- sample_props %&gt;% summarise(SE = sd(prop)) %&gt;% pull(SE) se_prop ## [1] 0.09566467 The standard error of the sample proportion for sample size \\(n = 20\\), based on 1000 replicated samples, is \\(SE(\\hat{p}) =\\) 0.1. ► Question The effect of sample size on the standard error of the sample proportion How does the sample size affect the standard error of the sample proportion? Compute the sampling distribution for the proportion of comedy movies using samples of size \\(n = 20\\), \\(n = 50\\), \\(n = 200\\) respectively and 1000 replicated samples. ► Solution sample_props_20 &lt;- hollywood %&gt;% rep_sample_n(size = 20, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(prop = mean(Genre == &#39;Comedy&#39;)) sample_props_50 &lt;- hollywood %&gt;% rep_sample_n(size = 50, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(prop = mean(Genre == &#39;Comedy&#39;)) sample_props_200 &lt;- hollywood %&gt;% rep_sample_n(size = 200, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(prop = mean(Genre == &#39;Comedy&#39;)) sample_props_vary_n &lt;- bind_rows( sample_props_20 %&gt;% mutate(n = 20), sample_props_50 %&gt;% mutate(n = 50), sample_props_200 %&gt;% mutate(n = 200) ) ggplot(sample_props_vary_n, aes(x = prop)) + geom_histogram(color = &#39;white&#39;) + geom_vline(xintercept = prop_comedy, color = &#39;red&#39;, size = 1) + facet_grid(rows = vars(n), labeller = label_both) + labs(x = expression(hat(p)), title = &quot;Sampling distribution of proportion for samples of size 20, 50, 200&quot;) Figure 11.9: Three sampling distributions of the proportion, with population parameter \\(p\\) marked by a red vertical line. From Figure 11.9 we can see that, as the sample size increases, the standard error of the sample proportion decreases. Increasing the sample size, the spread of the statistic values is reduced. ► Question Comparing the budget for action and comedy movies What is the population average budget (in millions of dollars) allocated for making action vs comedy movies? And the standard deviation? ► Solution budgets &lt;- hollywood %&gt;% filter(Genre == &#39;Action&#39; | Genre == &#39;Comedy&#39;) %&gt;% group_by(Genre) %&gt;% summarise(avg_budget = mean(Budget), sd_budget = sd(Budget)) budgets ## # A tibble: 2 x 3 ## Genre avg_budget sd_budget ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Action 85.9 64.0 ## 2 Comedy 36.9 26.7 From the above tibble we see that action movies have been allocated a higher budget (\\(\\mu_{Action} =\\) 85.9) than comedy movies (\\(\\mu_{Comedy} =\\) 36.9). At the same time, action movies have a higher variability of budgets around the mean value (\\(\\sigma_{Action} =\\) 64 vs \\(\\sigma_{Comedy} =\\) 26.7). ► Question (Optional) Estimate the average difference in budget for action vs comedy movies Estimate the average difference in budget for action vs comedy movies, using two samples of size 100. How does it compare to the population difference? Hint: Filter the hollywood tibble to keep only action movies. Then, obtain a sample of 100 action movies and estimate the average budget. Repeat the same steps for comedy movies. ► Solution sample_budget_action &lt;- hollywood %&gt;% filter(Genre == &#39;Action&#39;) %&gt;% rep_sample_n(size = 100) %&gt;% summarise(avg_budget = mean(Budget)) sample_budget_action ## # A tibble: 1 x 2 ## replicate avg_budget ## &lt;int&gt; &lt;dbl&gt; ## 1 1 87.5 sample_budget_comedy &lt;- hollywood %&gt;% filter(Genre == &#39;Comedy&#39;) %&gt;% rep_sample_n(size = 100) %&gt;% summarise(avg_budget = mean(Budget)) sample_budget_comedy ## # A tibble: 1 x 2 ## replicate avg_budget ## &lt;int&gt; &lt;dbl&gt; ## 1 1 36.6 est_diff &lt;- pull(sample_budget_action, avg_budget) - pull(sample_budget_comedy, avg_budget) est_diff ## [1] 50.869 The estimated difference in mean budget is \\(\\bar{x}_{Action} - \\bar{x}_{Comedy} =\\) 87.5 - 36.6 = 50.9. The population difference in budget can be calculated from the previous question, and is equal to \\(\\mu_{Action} - \\mu_{Comedy} =\\) 85.9 - 36.9 = 49. The two values are not exactly equal, as we have estimated the parameter with a sample of size 100. 11.9 Summary In Section 11.3 we have reviewed the difference between a population parameter and a statistic computed on a sample [LO1]. Because of the sampling criteria, which select units at random, those included in the sample vary when the sampling procedure is repeated. The distribution of the values that a sample statistics takes on the different samples is called sampling distribution, and has been defined in Section 11.4 [LO2]. The spread of a statistic gives an idea of how close our estimate of the unknown population parameter is to the population value. Hence, lower variability means a better guess. We quantified the variability of a statistic with its standard error, defined as the standard deviation of the statistic’s sampling distribution, and in Section 11.6 we saw that as the sample size increases, the standard error decreases [LO3]. 11.10 Glossary Statistical inference. The process of drawing conclusions about the population from the data collected in a sample. Population. The entire collection of units of interest. Sample. A subset of the entire population. Random sample. A subset of the entire population, picked at random, so that any conclusion made from the sample data can be generalised to the entire population. Representation bias. Happens when some units of the population are systematically underrepresented in samples. Generalisability. When information from the sample can be used to draw conclusions about the entire population. This is only possible if the sampling procedure leads to samples that are representative of the entire population (such as those drawn at random). Parameter. A fixed but typically unknown quantity describing the population. Statistic. A quantity computed on a sample. Sampling distribution. The distribution of the values that a statistic takes on different samples of the same size and from the same population. Standard error. The standard error of a statistic is the standard deviation of the sampling distribution of the statistic. 11.11 References Ismay, C., &amp; Kim, A. Y. (2019). Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. CRC Press. Freely available online at: https://moderndive.com/ Lock, R. Lock Morgan, K., Lock, E., &amp; Lock, D. (2013) Statistics: Unlocking the power of data. Wiley, Hoboken. You might remember it from the cartoon Wile E. Coyote and the Road Runner.↩ Of course a population might change over time, as people can enter or leave at any time, so you might wonder why did we say that a population parameter is fixed? Because of the large number of units in the entire population, it is reasonable to assume that the addition of comparatively few units to the entirety leads to a negligible change in the population parameter.↩ "],
["bootstrapping-and-confidence-intervals.html", "Chapter 12 Bootstrapping and Confidence Intervals 12.1 Recap 12.2 From Sampling to Resampling 12.3 Getting to the bootstrap distribution 12.4 More generally… 12.5 Confidence Intervals 12.6 Summary 12.7 Take-home message 12.8 Lab 12.9 Glossary 12.10 References", " Chapter 12 Bootstrapping and Confidence Intervals Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand how bootstrap resampling with replacement can be used to approximate a sampling distribution. LO2. Understand how the bootstrap distribution can be used to derive a range of highly plausible values (a confidence interval). Reading This week’s reading is Chapter 8 of the book by Chester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman and Hall/CRC, 2019. Freely available online at: https://moderndive.com/ 12.1 Recap Last week, we learnt about how we can take a statistic from a sample to draw conclusions about a parameter of the population from which the sample is drawn. We then focused on how these sample statistics will vary from sample to sample. We saw that by taking lots of samples we could create a sampling distribution for a statistic, allowing us to quantify the variation in the sample statistics due to sampling. Specifically, we learnt that the standard deviation of the sampling distribution is known as the standard error. Finally, we saw how the size of our samples influences the sampling variation, with bigger samples leading to narrower sampling distributions, and more precise estimates. This week, we are going to continue to think about how we can quantify sampling variation, but specifically when we have only a single sample (which is often the case in real life). We will also see how, just as we can take a sample in order to calculate a single point estimate of a population parameter, we can use sampling variation to construct a range of plausible values which, in the case of uncertain estimates, might be more meaningful than a single value. 12.2 From Sampling to Resampling We mentioned last week that we often have neither the time nor the resources to collect data from the entire population (a census). It is also often infeasible to get many samples of size \\(n\\) in order to get an idea of how accurate our estimate of the population parameter is. How can we study the variability of our sample statistic with only one sample? It turns out that we can mimick the act of sampling \\(n\\) units from the population, by resampling with replacement \\(n\\) units from our original sample of \\(n\\) units. This is broadly known as bootstrapping. Bootstrapping Bootstrap definition: Random sampling with replacement from the original sample, using the same sample size. ► The NFL Example Think back to last week. We saw a dataset of all the National Football League players at the beginning of 2015, along with their yearly salaries. This was our population (in real life we often don’t have data on the entire population). We took multiple samples of 50 players in order to study how the mean salaries of those samples varied (this allowed us to determine the accuracy of using a mean from a sample of 50 players, \\(\\bar{x}\\), as an estimate of the population parameter, \\(\\mu\\)). In fact, we took 2000 samples of 50 players, which is not at all feasible in practice. Now let’s imagine we only collect one sample of 50 players. We can approximate the sampling distribution of \\(\\bar{x}\\) (the mean salary of our sample) by bootstrapping. To do this, we: collect a sample of 50 players; compute the mean salary of the sample; take a random sample with replacement of 50 players from our original sample (this is known as a resample), and compute the mean of the resample; re-do step three many times. Think What do we mean by “with replacement”, and why is it necessary? ► Answer “With replacement” simply means that as we take our sample, we replace the first item before we choose the second.. and so on. If we resampled our 50 player sample without replacement, we would simply end up with the same 50 players, and therefore the same mean! ► Solution Take a sample. We have a sample of 50 players and their salaries, which we can read in to R as follows: library(tidyverse) library(moderndive) nfl_sample &lt;- read_csv(&quot;https://edin.ac/2NATDCQ&quot;) nfl_sample ## # A tibble: 50 x 5 ## Player Position Team TotalMoney YearlySalary ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Marcus Williams CB Jets 1.53 0.51 ## 2 Antonio Smith 34DE Broncos 2 2 ## 3 Dion Jordan 43DE Dolphins 20.6 5.14 ## 4 Carl Davis 34DT Ravens 2.95 0.738 ## 5 Jason Hatcher 34DE Redskins 27.5 6.88 ## 6 Caushaud Lyons 43DE Steelers 1.58 0.528 ## 7 Thomas Rawls RB Seahawks 1.59 0.53 ## 8 Michael Griffin S Titans 35 7 ## 9 Winston Guy S Colts 1.42 0.71 ## 10 Kevin White WR Bears 16.6 4.14 ## # … with 40 more rows Compute the mean salary of our sample: nfl_sample %&gt;% summarise(avg_salary = mean(YearlySalary)) ## # A tibble: 1 x 1 ## avg_salary ## &lt;dbl&gt; ## 1 2.11 Sample our original sample, with replacement, and compute the mean: nfl_resample1 &lt;- nfl_sample %&gt;% rep_sample_n(size = 50, replace = TRUE) nfl_resample1 ## # A tibble: 50 x 6 ## # Groups: replicate [1] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Justin Britt RT Seahawks 3.55 0.864 ## 2 1 Zach Kerr 43DT Colts 1.54 0.512 ## 3 1 Michael Harris LT Vikings 1.54 1.54 ## 4 1 Andre Caldwell WR Broncos 2.7 1.35 ## 5 1 Kyle Brindza K Buccaneers 1.58 0.528 ## 6 1 Brett Kern P Titans 15 3 ## 7 1 Aaron Donald 43DT Rams 10.1 2.53 ## 8 1 Aaron Williams S Bills 26.0 6.50 ## 9 1 Trevor Robinson C Chargers 1.7 0.85 ## 10 1 Josh Mitchell CB Colts 1.58 0.526 ## # … with 40 more rows nfl_resample1 %&gt;% summarise(avg_salary = mean(YearlySalary)) ## # A tibble: 1 x 2 ## replicate avg_salary ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.34 and again.. nfl_resample2 &lt;- nfl_sample %&gt;% rep_sample_n(size = 50, replace = TRUE) nfl_resample2 ## # A tibble: 50 x 6 ## # Groups: replicate [1] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Kelvin Beachum RT Steelers 2.15 0.536 ## 2 1 DJ Williams ILB Bears 1.5 1.5 ## 3 1 Marqueston Huff S Titans 2.65 0.664 ## 4 1 Jason Hatcher 34DE Redskins 27.5 6.88 ## 5 1 Brett Kern P Titans 15 3 ## 6 1 Fletcher Cox 34DE Eagles 10.2 2.56 ## 7 1 Breno Giacomini RT Jets 18 4.5 ## 8 1 Kevin White WR Bears 16.6 4.14 ## 9 1 DJ Williams ILB Bears 1.5 1.5 ## 10 1 Khalif Barnes RT Raiders 1.5 1.5 ## # … with 40 more rows nfl_resample2 %&gt;% summarise(avg_salary = mean(YearlySalary)) ## # A tibble: 1 x 2 ## replicate avg_salary ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1.71 and so on… sample statistic Original Sample 2.11496 Resample 1 2.34474 Resample 2 1.71444 12.3 Getting to the bootstrap distribution Key point If we resample with replacement from our original sample enough times, then the distribution of all the means of these resamples begins to approximate the sampling distribution. We can speed up this process by getting R to take many resamples for us, in the same way that last week we asked it to take many samples from a population. nfl_2000resamples &lt;- nfl_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 2000) The above line of code takes 2000 samples of size 50, by sampling with replacement from our original sample of size 50. \\(2000 \\times 50 = 100,000\\), so this results in a tibble with \\(100,000\\) rows. nfl_2000resamples ## # A tibble: 100,000 x 6 ## # Groups: replicate [2,000] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Josh Mitchell CB Colts 1.58 0.526 ## 2 1 Justin Britt RT Seahawks 3.55 0.864 ## 3 1 Kevin White WR Bears 16.6 4.14 ## 4 1 Stephen Hill WR Panthers 0.595 0.595 ## 5 1 Aaron Williams S Bills 26.0 6.50 ## 6 1 Marqueston Huff S Titans 2.65 0.664 ## 7 1 Dion Jordan 43DE Dolphins 20.6 5.14 ## 8 1 David Bass 34OLB Titans 1.26 0.63 ## 9 1 Khalif Barnes RT Raiders 1.5 1.5 ## 10 1 Trai Turner G Panthers 2.79 0.698 ## # … with 99,990 more rows We can compute the mean of each of the 2000 resamples drawn from the original sample (just like last week when we computed the mean of each of 2000 samples drawn from the population). nfl_resample_means &lt;- nfl_2000resamples %&gt;% group_by(replicate) %&gt;% summarise(avg_salary = mean(YearlySalary)) and we can plot them: ggplot(nfl_resample_means, aes(x = avg_salary)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;resample mean&quot;) ► Question Where do you think that this histogram is centred? The mean salary of the population (\\(\\mu\\)). The mean salary of the original sample (\\(\\bar{x}\\)). Somewhere else. ► Solution The distribution of means of resamples will be centred around the mean of our original sample, which was 2.11. nfl_resample_means %&gt;% summarise(mean_of_means = mean(avg_salary)) ## # A tibble: 1 x 1 ## mean_of_means ## &lt;dbl&gt; ## 1 2.11 ► Question Last week we looked at the standard error (the standard deviation of the sampling distribution). We have seen how the boostrap distribution is an approximation of the sampling distribution. TRUE or FALSE: The standard deviation of the bootstrap distribution is an approximation of the standard error of \\(\\bar{x}\\). ► Solution TRUE! 12.4 More generally… Now let’s think more generally about what we did there… We were interested in estimating some unknown parameter of a population. We had a sample of size \\(n\\) drawn at random from the population. We took lots of resamples (of size \\(n\\)) from our original sample, and calculated a statistic for each one. We then visualised the distribution of those statistics. The tool below will help to conceptualise these steps: The big blue distribution at the top: The population. The vertical blue line: The population parameter \\(\\mu\\). The yellow sample button: Takes a sample from the population (note you can change the sample \\(n\\)). The green resample button: Samples with replacement from the original sample (the yellow one), and calculates the mean (which is then dropped into the bottom panel). The bottom panel: The distribution of resample means - the bootstrap distribution! Spend 10 minutes changing things such as the sample size. If you have any questions about what is happening, then please ask either now or during the lab. source: https://web.archive.org/web/20160807193104/http://wise1.cgu.edu/vis/bootstrap/ 12.5 Confidence Intervals Take a look again at the bootstrap distribution we constructed (Figure 12.1, below). Figure 12.1: Bootstrap resampling distribution based on 2000 resamples ► Question Roughly, between what two values do most of the resample means lie? ► Solution “Most” is very vague. Just eyeballing, most of the distribution lies between 1.5 and 2.7. ggplot(nfl_resample_means, aes(x = avg_salary)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;resample mean&quot;) + geom_vline(xintercept = c(1.5, 2.7)) Confidence intervals simply answer more exactly where “most” sample means lie - they give us a range of plausible values for our population parameter. To construct a confidence interval, we need two things: a confidence level; a measure of sampling variability. We have the latter, in the form of our bootstrap distribution. The confidence level, instead, needs to be set by us. For instance, we might ask between which values the middle 95% (or 90%, or 80%, etc.) of our distribution falls. In other words, the confidence level is the “success rate”: the proportion of all samples whose intervals contain the true parameter. How exactly do we interpret a confidence interval? If we were to do this whole process over and over again: take a random sample of size \\(n\\); sample with replacement from that sample; construct a 95% confidence interval. Then about 95% of the confidence intervals we created would contain the population mean. So if we did this 100 times, we would expect about five of our 95% confidence intervals to not contain the true population mean. And if we had been constructing 80% confidence intervals instead, we would expect roughly 80 of them to contain the population mean. 12.5.1 Calculating confidence intervals using a bootstrap standard error We can construct confidence intervals using the standard error. However, we can not compute standard errors from just one sample, so we need to estimate the standard error of a statistic using bootstrap. We also use the following rules of thumb: If the distribution is symettric and bell-shaped… 68% of values will lie within 1 standard deviation of the mean. 95% of values will lie within 1.96 standard deviations of the mean. 99.7% of values will lie within 3 standard deviations of the mean. We have our sample mean, and we can calculate the standard deviation of our bootstrap distribution (to approximate the standard error of the sample mean). We therefore have all the information we need to calculate, for instance, a 95% confidence interval - it is simply \\(1.96 \\times \\text{standard error}\\) above and below our mean. Formally, we can write this 95% interval as: \\(\\text{Statistic} \\pm 1.96 \\times SE\\) And in R… # Recall that our original sample mean was 2.11496 original_sample_mean &lt;- nfl_sample %&gt;% summarise(avg = mean(YearlySalary)) %&gt;% pull(avg) original_sample_mean ## [1] 2.11496 nfl_resample_means %&gt;% summarise( est_SE = sd(avg_salary), ci_lower = original_sample_mean - (1.96 * est_SE), ci_upper = original_sample_mean + (1.96 * est_SE) ) %&gt;% select(ci_lower, ci_upper) ## # A tibble: 1 x 2 ## ci_lower ci_upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.52 2.71 12.6 Summary Let’s recap what we’ve done today: We started with a sample from a population. We calculated a statistic from our sample to estimate a parameter in our population. We used bootstrapping (random sampling with replacement from our original sample) to estimate the standard error of the statistic. We constructed a range of plausible values (a confidence interval) by combining the sample statistic and the bootstrap estimate of the standard error of the statistic. We constructed our bootstrap distribution using code like below: bootstrap_distribution &lt;- nfl_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 2000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) and we used the standard deviation of our bootstrap resample means… bootstrap_distribution %&gt;% summarise(est_SE = sd(avg)) ## # A tibble: 1 x 1 ## est_SE ## &lt;dbl&gt; ## 1 0.290 to calculate some 95% confidence intervals using the formula: \\(\\bar{x} \\pm 1.96 \\times SE\\) which became: \\(2.11 \\pm 1.96 \\times 0.29\\) giving us a confidence interval of \\([1.54, 2.68]\\). Stop and think What we did today entailed specifying what variable we were interested in, generating replicates, calculating the statistic for each replicate, and finally, we visualised the distribution. This is an important framework for understanding how to estimate sampling variation to evaluate the accuracy of our statistical inferences. The steps for this are visualised in Figure 12.2 below. Figure 12.2: Pipeline of bootstrapping-based inference Source: https://moderndive.com/8-confidence-intervals.html 12.7 Take-home message Using just one sample, it is possible to quantify estimation error by taking repeated resamples with replacement from our original sample. We can use this to construct ranges of plausible values of the parameter we are estimating. This teaches us a standardised way of reporting uncertainty in our estimates. 12.8 Lab Exercise 1: Hollywood Movies The following code chunk reads in a sample of the Hollywood movies data we saw last week. hollywood_sample &lt;- read_tsv(&#39;https://edin.ac/2N9yHms&#39;) %&gt;% # read the data select(Movie, Genre, RottenTomatoes) %&gt;% # selects relevant variables drop_na %&gt;% # removes all the NAs sample_n(size=25) # takes our sample ► Question This week, we’re interested in the average Rotten Tomatoes rating for all Hollywood movies between 2007 and 2013. What is our best estimate of this with the data we just read in? ► Solution \\(\\bar{x}\\), the mean Rotten Tomatoes rating for our sample. ► Question Calculate the sample statistic. ► Solution hollywood_sample %&gt;% summarise(avg_rating = mean(RottenTomatoes)) ## # A tibble: 1 x 1 ## avg_rating ## &lt;dbl&gt; ## 1 52.2 ► Question Generate 1000 bootstrap resamples to create the bootstrap distribution. ► Solution hwood_bs_distribution &lt;- hollywood_sample %&gt;% rep_sample_n(25, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(avg_rating = mean(RottenTomatoes)) hwood_bs_distribution ## # A tibble: 1,000 x 2 ## replicate avg_rating ## &lt;int&gt; &lt;dbl&gt; ## 1 1 58.5 ## 2 2 51.1 ## 3 3 47.5 ## 4 4 53.1 ## 5 5 56.8 ## 6 6 50.4 ## 7 7 52.2 ## 8 8 44.7 ## 9 9 57.7 ## 10 10 56.9 ## # … with 990 more rows ► Question Estimate the standard error of the sample statistic from your bootstrap distribution. ► Solution hwood_bs_distribution %&gt;% summarise(estimated_SE = sd(avg_rating)) ## # A tibble: 1 x 1 ## estimated_SE ## &lt;dbl&gt; ## 1 6.35 ► Question Compute the 95% confidence intervals around our estimate of the average Rotten Tomatoes rating, and plot the bootstrap distribution and the confidence interval. ► Solution hwood_samplemean &lt;- hollywood_sample %&gt;% summarise(avg_rating = mean(RottenTomatoes)) %&gt;% pull(avg_rating) hwood_se &lt;- hwood_bs_distribution %&gt;% summarise(estimated_SE = sd(avg_rating)) %&gt;% pull(estimated_SE) hwood_ci_lower &lt;- hwood_samplemean - 1.96 * hwood_se hwood_ci_upper &lt;- hwood_samplemean + 1.96 * hwood_se ggplot(hwood_bs_distribution, aes(x=avg_rating)) + geom_histogram() + geom_vline(xintercept = c(hwood_ci_lower, hwood_ci_upper)) + labs(x = &quot;bootstrap avg rating&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ► Extra Go back to the top where we read in the data, and change the sample you are collecting from 25 to 50. Run the previous tasks again - how has the confidence interval changed? Exercise 2: NFL Players ► Question Look back to last week. What was the population mean yearly salary for all NFL players at the beginning of 2015? ► Solution 2.238 (million dollers!) ► A bigger Question A researcher lives in Boston. They want to estimate salaries of NFL players, and in 2015 they go around and ask 50 players about their yearly salaries. The code below reads in the sample they collected. nflboston &lt;- read_csv(&quot;https://edin.ac/35QVPwp&quot;) Compute the sample mean, and calculate 99% confidence intervals via bootstrap standard error ► Solution nflb_samplemean &lt;- nflboston %&gt;% summarise(avg_salary = mean(YearlySalary)) %&gt;% pull(avg_salary) nflb_bs_distribution &lt;- nflboston %&gt;% rep_sample_n(50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(avg_salary = mean(YearlySalary)) nflb_se &lt;- nflb_bs_distribution %&gt;% summarise(estimated_SE = sd(avg_salary)) %&gt;% pull(estimated_SE) nflb_ci_lower &lt;- nflb_samplemean - 3 * nflb_se nflb_ci_upper &lt;- nflb_samplemean + 3 * nflb_se ggplot(nflb_bs_distribution, aes(x=avg_salary)) + geom_histogram() + geom_vline(xintercept = c(nflb_ci_lower, nflb_ci_upper)) + labs(x = &quot;bootstrap avg salary&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ► Question This confidence does not include the population mean. Why not? Hint: Look at your data, and think about what you know about how it was collected - why might this not be a good sample? ► Solution The researcher, living in Boston, seems to have sampled a lot of players from the New England Patriots (a local team). The key thing here is that the statistical inference we are making (that the sample mean is an estimate of the population mean) assumes that the sample is an unbiased representation. In this case it is not a truly random sample! 12.9 Glossary Population. The entire collection of units of interest. Sample. A subset of the entire population. Parameter. A fixed but typically unknown quantity describing the population. Statistic. A quantity computed on a sample. Sampling distribution. The distribution of the values that a statistic takes on different samples of the same size and from the same population. Standard error. The standard error of a statistic is the standard deviation of the sampling distribution of the statistic. Resample. To sample again from your original sample Bootstrapping. Repeated random sampling with replacement Bootstrap distribution. The distribution of statistics calculated on random resamples. Approximates the sampling distribution of the sample statistic. Confidence interval (CI). A range of plausible values around an estimate (e.g., a sample statistic), taking into account uncertainty in the statistic (e.g., sampling variability) Confidence level. The percentage of confidence intervals which will contain the true population parameter in the long run (i.e., if you sampled the population and constructed confidence intervals many times over). The proportion of all samples whose intervals contain the true parameter. 12.10 References Ismay, C., &amp; Kim, A. Y. (2019). Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. CRC Press. Freely available online at: https://moderndive.com/ "],
["chap-hyp-test.html", "Chapter 13 Testing hypotheses 13.1 Recap 13.2 Walkthrough 13.3 Summary 13.4 Lab 13.5 Glossary 13.6 References", " Chapter 13 Testing hypotheses Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand null and alternative hypotheses, and how to specify them for a given research question. LO2. Understand how to obtain a null distribution via simulation. LO3. Understand statistical significance and how to calculate p-values from null distributions. 13.1 Recap In Week 11 we saw that a population is described by parameters, and samples are described by statistics. We typically can not measure the entire population, so a population parameter needs to be estimated by a sample statistic. We quantified the uncertainty in our estimate by the standard error (SE) of the sample statistic. This is defined as the standard deviation of the sampling distribution. The latter shows the values of the sample statistic when computed on many samples of the same size from the same population. In practice, however, we are not able to quantify the SE by taking repeated samples from the population, as due to money or time constraints we only have one sample of size \\(n\\). In Week 12 we learned to approximate the sampling distribution by bootstrapping. This approach repeatedly samples with replacement from the observed sample, using the same sample size. The standard deviation of the bootstrap distribution is an estimate of the SE of the statistic. Using the statistic from the observed sample, and the estimated SE from the bootstrap distribution, we learned how to give a range of plausible values for the population parameter. This week we will learn how to use the sample data to answer a wide range of questions about a population. 13.2 Walkthrough Research question of the walkthrough Is there such a thing as extrasensory perception (ESP), also known as the “sixth sense”? Extrasensory perception (ESP) According to Wikipedia, “Extrasensory perception or ESP, also called the sixth sense, includes claimed reception of information not gained through the recognized physical senses, but sensed with the mind.” A famous test for ESP involves a set of cards, shown in Figure 13.1, known as Zener cards: Figure 13.1: Zener cards (Source: https://en.wikipedia.org/wiki/Extrasensory_perception) There are five different cards, one for each of the following symbols: circle cross wavy lines square star Subjects randomly draw a card from these five, and telepathically communicate the chosen card to another subject who then tries to guess the symbol. No visual or auditory clues are allowed. Example 1: ESP experimental results In an experiment conducted on a class of \\(n = 98\\) students, 25 of them correctly guessed their partner’s card. This is equivalent to saying that the observed sample proportion of correct guesses is \\(\\hat{p} = 25/98 = 0.26\\). ► Question Is the sample proportion of correct guesses high enough to provide evidence of ESP? ► Solution We are not sure yet. We still have to introduce the statistical framework that, assuming no ESP, allows us to determine either: if the observed sample statistic is highly unlikely to happen due to sampling variability, or if it would be very likely to see such a value by sampling variation. As we will see, this framework is called hypothesis testing and involves using a statistical test. Example 2: In-class activity Everyone in the class randomly picks one of the five symbols in Figure 13.1. Draw the symbol on a bit of paper without showing it to anyone! Pair up with someone. Telepathically communicate your symbol to your partner, who has to guess it without any visual or auditory clues. Switch roles. Did your partner guess your card correctly? Keep track of the sample size \\(n\\) of the in-class experiment, and the sample statistic \\(\\hat{p}\\). We will use these two values later on to see if we have evidence of ESP in this class. The random chance model ► Question If there was no such thing as ESP, what proportion of guesses do you expect to be correct? ► Solution If there is no ESP, because there are five cards, each person has a chance of correctly guessing the symbol equal to \\(1/5 = 0.2\\). This is similar to asking: what’s the chance of correctly guessing the face of a die? If we had to randomly guess, since there are 6 faces, we have a probability of \\(1/6\\). The only difference is that instead of six faces, here we only have five, hence the \\(1/5\\) chance. We use the chance-alone model to help us decide whether: the observed proportion could easily happen by sampling variation if we are randomly guessing the symbol, or the underlying process is something else, such as having extrasensory perception. The chance model for the ESP example is specified by the probability of correctly guessing the symbol if we were to guess at random: \\(p = 1/5\\). We call this probability the null hypothesis, and it represents what we would expect if there was no ESP (“no effect”). We denote the null hypothesis by: \\[ H_0 : \\ p = \\frac{1}{5} \\] The alternative explanation The research question of interest (i.e. if ESP exists) is captured by the alternative hypothesis, which contradicts the null hypothesis. We always put the research claim of interest in the alternative hypothesis, rather than the null one. The null and the alternative hypotheses are competing claims about the population parameter and we can not have the same value of the parameter included in both the null and the alternative. In the ESP example, we would have evidence of ESP if the chance of correctly guessing the symbol was greater than when randomly guessing it: \\[ H_1 : p &gt; \\frac{1}{5} \\] Statistical test As we saw in Week 11, sample statistics vary from sample to sample. Even if the population proportion was really equal to 1/5, not every random sample would have a sample proportion exactly equal to 1/5. The key question is then: how do we decide in a principled way if a sample proportion is sufficiently above 1/5 to suggest presence of ESP? In Example 1 we obtained a sample proportion of 25/98 = 0.26. Is 0.26 sufficiently higher than 0.20 to suggest presence of ESP? The set of principles that allows us to make an informed claim under uncertainty about the population is called statistical hypothesis testing. Statistical test A statistical test is a method that uses the data collected on a sample to assess a claim about the population. Understanding statistical evidence ► Question If \\(\\hat{p}\\) denotes the proportion of correctly guessed symbols in an ESP experiment, which of the following sample proportions provide the strongest evidence in favour of ESP? \\(\\hat{p} = 0\\) \\(\\hat{p} = 1/5\\) \\(\\hat{p} = 1/2\\) \\(\\hat{p} = 3/4\\) ► Solution The correct answer is (d) as \\(\\hat{p} = 3/4 = 0.75\\) is the highest proportion of correctly guessed symbols. This example shows us that we assess a claim about a population parameter by quantifying the statistical evidence that the observed statistic gives against the null hypothesis and in favour of the alternative hypothesis. Statistical hypotheses We perform statistical tests on two competing hypotheses about a population parameter: Null hypothesis \\(H_0\\): typically a claim about “no effect” or “no difference between groups”; Alternative hypothesis \\(H_1\\): the claim we seek evidence for. The null hypothesis is typically a very specific claim about the population parameter, while the alternative hypothesis is a broader statement. To visually help you identify the null vs the alternative, typically: \\(H_0\\) includes an \\(=\\) sign \\(H_1\\) includes \\(&gt;\\), \\(&lt;\\) or \\(\\neq\\) If \\(H_1\\) contains either \\(&gt;\\) or \\(&lt;\\), we say that the alternative hypothesis is one-sided. If \\(H_1\\) contains \\(\\neq\\), we say that the alternative hypothesis is two-sided. Putting this together, in the ESP investigation we have two competing hypotheses: \\[ H_0 : \\ p = \\frac{1}{5} \\\\ H_1 : \\ p &gt; \\frac{1}{5} \\] Statistical significance Now that we have defined a framework to assess two competing hypotheses about a population parameter, how do we quantify the evidence that the sample data bring in favour of the alternative hypothesis? We do this by quantifying how unusual it is to obtain a statistic as extreme or more extreme than the observed statistic, if the null hypothesis is true. If it is very unusual, we have significant evidence against the null hypothesis. Statistical significance Assuming the null hypothesis to be true, we say that the sample results are statistically significant if it is unlikely that by random chance alone we obtain a statistic that is as extreme as the observed sample statistic, or more extreme (in the direction specified by the alternative hypothesis). Hypothesis testing basically boils down to quantifying how likely or unlikely it is to observe the sample statistic if the null hypothesis is true. Two decisions are possible: If the sample data are statistically significant, we reject \\(H_0\\) as we have enough evidence against \\(H_0\\) and in favour of \\(H_1\\). If the sample data are not statistically significant, we do not reject \\(H_0\\) as we do not have convincing evidence against \\(H_0\\). This has an analogy in law. One of the fundamental legal principles is the presumption of innocence, which says that a person is considered innocent until proven guilty, and that the evidence must be beyond reasonable doubt. The null hypothesis (\\(H_0\\)), corresponding to no effect, is that the defendant is innocent. The alternative hypothesis (\\(H_1\\)) is that the defendant is guilty. The court presumes \\(H_0\\) to be true (the defendant is innocent) unless the prosecutor can provide strong evidence that the defendant is guilty beyond a reasonable doubt. The burden of proof is on the prosecutor (in our case, the data) to convince the court that the defendant is guilty. Similarly, we retain \\(H_0\\) unless there is strong evidence to reject \\(H_0\\). Recall that: \\(p\\) = proportion of correct guesses of the symbol \\(H_0 : \\ p = 1/5\\) vs \\(H_1 : \\ p &gt; 1/5\\) Scenario 1: statistically significant results If results are statistically significant: The sample proportion \\(\\hat{p}\\) of correct guesses is unlikely to occur by random chance alone. Recall that by random chance we mean if ESP does not exist and thus \\(p = 1/5\\). The sample data provide evidence that the population proportion of correct guesses is higher than 1/5, meaning that we have evidence of ESP. Scenario 2: not statistically significant results If results are not statistically significant: The sample proportion \\(\\hat{p}\\) of correct guesses could easily happen by random chance alone. Recall that by random chance we mean if ESP does not exist and thus \\(p = 1/5\\). The sample data do not provide enough evidence to conclude that \\(p &gt; 1/5\\) or that ESP exists. Null distribution In the previous section, we said that if the observed statistic is very unusual, we have significant evidence against the null hypothesis. But how do we quantify if a value is unusual? The key idea is to look at the sampling distribution of the statistic if \\(H_0\\) were true. We do this by generating many samples, assuming the null hypothesis to be true, and computing the statistic on each of the generated samples. The distribution of these statistics is called the null distribution. Null distribution The null distribution is the sampling distribution of the statistic assuming the null hypothesis to be true. Centre: The null distribution is centred at the value of the population parameter specified in the null hypothesis. We will now compute the null distribution for Example 1 on ESP. One experiment is a sequence of \\(n = 98\\) guesses (resulting in a sequence of S’s and F’s, where S = success and F = failure) and this represents one sample. The required steps are: Obtain many random samples of size \\(n = 98\\), assuming a probability of success equal to 1/5 Compute the proportion of successes for each sample Plot the distribution of proportions Load the required packages: library(tidyverse) library(moderndive) Create a tibble with the possible outcome of one play. Either: your partner correctly guesses your card (S = success), or your partner does not correctly guess your card (F = failure). Note: Make sure the column name in the tibble is vals and that it is a factor. outcomes &lt;- tibble(vals = factor(c(&#39;S&#39;, &#39;F&#39;))) outcomes ## # A tibble: 2 x 1 ## vals ## &lt;fct&gt; ## 1 S ## 2 F Assign the probabilities of each possible outcome, remembering that the probabilities must sum to one: Probability of success (S) = probability of correctly guessing the symbol: \\(p = 1/5\\) Probability of failure (F) is the complement to one: \\(1 - p = 1 - 1/5 = 4/5\\). prob &lt;- c(1/5, 4/5) Generate 1,000 samples of size \\(n = 98\\) with the probabilities specified by the null hypothesis: samples &lt;- rep_sample_n(outcomes, size = 98, replace = TRUE, reps = 1000, prob = prob) samples ## # A tibble: 98,000 x 2 ## # Groups: replicate [1,000] ## replicate vals ## &lt;int&gt; &lt;fct&gt; ## 1 1 F ## 2 1 F ## 3 1 F ## 4 1 S ## 5 1 F ## 6 1 S ## 7 1 S ## 8 1 F ## 9 1 F ## 10 1 F ## # … with 97,990 more rows Note that the tibble has \\(98,000\\) rows. Each sample of size \\(n = 98\\) represents a sequence of 98 plays where each play can either be S (your partner correctly guessed your card) or F (your partner didn’t guess correctly). Hence, each sample is a sequence of S’s and F’s. Because we have 1,000 of such samples, the total number of rows will be \\(98 \\times 1,000 = 98,000\\). We can now compute the proportion of successes in each sample: null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;S&#39;) / n()) null_distribution ## # A tibble: 1,000 x 2 ## replicate prop ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.163 ## 2 2 0.184 ## 3 3 0.143 ## 4 4 0.235 ## 5 5 0.224 ## 6 6 0.214 ## 7 7 0.204 ## 8 8 0.184 ## 9 9 0.224 ## 10 10 0.224 ## # … with 990 more rows Let us now plot the 1,000 sample proportions obtained assuming the null hypothesis to be true. This plot shows the null distribution, and each dot represents one of the 1000 proportions from the above tibble: ggplot(null_distribution, aes(x = prop)) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = &#39;white&#39;, stackratio = 0.5) + labs(x = expr(hat(p))) Where does the sample statistic from Example 1 lie in the null distribution? Let’s add a red vertical line showing the value of the observed statistic, \\(\\hat{p} = 0.26\\): ggplot(null_distribution, aes(x = prop)) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = &#39;white&#39;, stackratio = 0.5) + geom_vline(xintercept = 0.26, color = &#39;tomato1&#39;, size = 1) + labs(x = expr(hat(p))) Do we have a high chance of observing the statistic \\(\\hat{p} = 0.26\\) under the null hypothesis of random guessing? Different researchers might reach different conclusions by looking at this plot. Rather than taking a decision by visually inspecting the plot, we want to find a generically applicable tool that would make different researchers all reach to the same conclusion. P-value Recall now the definition of statistical significance. We have evidence against the null hypothesis if it would be very unusual to obtain statistics as extreme or more extreme than the observed statistic in the null distribution. To summarise: in order to measure how unusual the observed statistic is under the null hypothesis, we need to generate many statistics under the null hypothesis (null distribution) and see what’s the proportion of the generated statistics that are as extreme as, or more extreme than, the observed statistic. The proportion of statistics in the null distribution as extreme or more extreme than the observed statistic is known as the p-value. The smaller the p-value, the higher the statistical evidence against the null hypothesis. P-value The p-value represents the chance of obtaining a statistic as extreme or more extreme than the observed one, if the null hypothesis were true. In Example 1 on ESP, we can identify the statistics in the null distribution that are as extreme or more extreme than the observed one (\\(\\hat{p} = 0.26\\)) by colour-coding them: ggplot(null_distribution, aes(x = prop, fill = (prop &gt;= 0.26))) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + labs(x = expr(hat(p)), fill = expr(hat(p) &gt;= 0.26)) + theme(legend.position=&quot;top&quot;) We now calculate the proportion of statistics in the null distribution which are greater than or equal 0.26. You can either do that by eye, counting the red dots and dividing them by the total number of dots (1,000) or, more quickly: pvalue &lt;- null_distribution %&gt;% summarise(pvalue = sum(prop &gt;= 0.26) / n()) pvalue ## # A tibble: 1 x 1 ## pvalue ## &lt;dbl&gt; ## 1 0.07 Interpretation of the p-value: If we were to randomly guess the card symbol in Example 1, the chance of getting a proportion as high as 0.26 is 0.07. The p-value needs to be calculated in the direction specified by the alternative hypothesis: The p-value of a one-sided hypothesis is the proportion in tail specified by \\(H_1\\). The p-value of a two-sided hypothesis is twice the proportion in the smallest tail, i.e. the tail with the smallest count. In the example below, the smallest tail (the tail with the lowest count) is the right tail. So if we were to perform an hypothesis test for the two-sided alternative \\(p \\neq 1/5\\), we would compute it as twice the proportion of statistics greater than or equal to the observed statistic (twice the proportion in the right tail): Note: If you were to calculate the p-value as twice the proportion in the bigger tail, you would obtain a p-value greater than 1. But remember, a probability must always be between 0 and 1! You can calculate a p-value via randomization for any statistic: Simulate many samples assuming \\(H_0\\) to be true. Compute the statistic on each of the simulated samples. Compute the proportion of simulated statistics as extreme or more extreme than the observed statistic, in the direction specified by the alternative hypothesis. Making a formal decision The smaller the p-value, the greater the evidence that the data provide against the null hypothesis \\(H_0\\). ► Question Which of the following p-values gives the strongest evidence against \\(H_0\\)? 0.005 0.1 0.35 0.92 ► Solution The correct answer is (a) as it is the smallest p-value. To summarise, the outcome of a statistical test is either: The p-value is small: we reject the null hypothesis; the observed sample statistic would be extreme in the null distribution; the results are statistically significant; the test concludes that we have enough evidence in favour of \\(H_1\\). The p-value is not small: we do not reject the null hypothesis; the observed sample statistic would not be extreme in the null distribution; the results are not statistically significant; we do not have sufficient evidence to reject \\(H_0\\). How small should the p-value be? This is set by the researcher before seeing any data. We decide if a p-value is small or not by specifying a threshold below which a p-value is deemed to be small. Significance level The significance level \\(\\alpha\\) is the threshold below which we deem a p-value small enough to reject the null hypothesis. For a given significance level \\(\\alpha\\), specified before collecting any data: if the p-value \\(\\leq \\alpha\\) we reject \\(H_0\\), if the p-value \\(&gt; \\alpha\\) we do not reject \\(H_0\\). Typically, “default” choices for \\(\\alpha\\) are \\(0.05\\) or \\(0.01\\). Caution: Never accept \\(H_0\\). “Do not reject \\(H_0\\)” is not the same as “accept \\(H_0\\)”. Not having sufficient evidence against \\(H_0\\) does not mean having evidence for \\(H_0\\). The following table summarizes in words the strength of evidence that the sample results bring in favour of the alternative hypothesis for different p-values: p-value strength of evidence 0.1 \\(&lt;\\) p-value not much evidence against null hypothesis 0.05 \\(&lt;\\) p-value \\(\\leq\\) 0.1 moderate evidence against the null hypothesis 0.01 \\(&lt;\\) p-value \\(\\leq\\) 0.05 strong evidence against the null hypothesis p-value \\(\\leq\\) 0.01 very strong evidence against the null hypothesis We can now wrap up Example 1 on ESP, and conclude that at a significance level of 0.05, we do not reject the null hypothesis as the p-value 0.07 is greater than \\(\\alpha = 0.05\\). Hence, the sample data do not show significant evidence for ESP. An alternative to the p-value as a measure of the strength of evidence is given by the standardized statistic. This measures how many standard deviations away from the mean of the null distribution the observed statistic is. A standardized statistic is typically denoted by \\(z\\), and can be calculated as: \\[ \\textrm{standardized statistic} = z = \\frac{\\textrm{statistic} - \\textrm{mean of null distribution}}{\\textrm{standard deviation of null distribution}} \\] We interpret a standardized statistic according to the following table: standardized statistic strength of evidence between −1.5 and 1.5 little or no evidence against the null hypothesis below −1.5 or above 1.5 moderate evidence against the null hypothesis below −2 or above 2 strong evidence against the null hypothesis below −3 or above 3 very strong evidence against the null hypothesis To compute the standardized statistic for Example 1 on ESP, we first need to find the standard deviation of the null distribution: null_distribution %&gt;% summarise(sd = sd(prop)) ## # A tibble: 1 x 1 ## sd ## &lt;dbl&gt; ## 1 0.0404 The standardized statistic is then: \\[ \\textrm{standardized statistic} = z = \\frac{0.26 - 0.20}{0.04} = 1.5 \\] If we interpreted the standardized statistic, rather than the p-value, for Example 1 on ESP, we would have reached the same conclusion. As the observed statistic (0.26) is only 1.5 standard deviations above the mean of the null distribution (0.20), the sample data bring little or no evidence against the null hypothesis, thus we do not reject \\(H_0\\). This idealized version of a histogram shows how far from the mean of the null distribution each standardized statistic value is: Recall from Week 12 that within 2 standard deviations around the mean lie approximately 95% of all values. Similarly, within 3 standard deviations around the mean lie 99.7% of all values. Note: The p-value is a probability, so it’s always between 0 and 1. A standardized statistic, instead, can be positive or negative. 13.3 Summary Today we have learned to assess how much evidence the sample data bring against the null hypothesis and in favour of the alternative hypothesis. The null hypothesis, denoted \\(H_0\\), is a claim about a population parameter that is initially assumed to be true. It typically represents “no effect” or “no difference between groups”. The alternative hypothesis, denoted \\(H_1\\), is the claim we seek evidence for. We assessed the strength of evidence against \\(H_0\\) following these steps: Generate many samples assuming the null hypothesis to be true; Obtain the null distribution by computing the statistic on each of the generated samples; Compute the p-value as the proportion of statistics in the null distribution as extreme or more extreme than the observed statistic, in the direction specified by the alternative hypothesis. The sample size and the observed statistic in the ESP experiment are: size &lt;- 98 observed_prop &lt;- 0.26 We obtained the null distribution with the following code: outcomes &lt;- tibble(vals = factor(c(&#39;S&#39;, &#39;F&#39;))) prob &lt;- c(1/5, 4/5) reps &lt;- 1000 null_distribution &lt;- outcomes %&gt;% rep_sample_n(size = size, replace = TRUE, reps = reps, prob = prob) %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;S&#39;) / n()) The following code chunk plots the null distribution and shows in red the statistics that are as extreme or more extreme than the observed statistic: ggplot(null_distribution, aes(x = prop, fill = (prop &gt;= observed_prop))) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + labs(x = expr(hat(p)), fill = expr(hat(p) &gt;= !!observed_prop)) + theme(legend.position=&quot;top&quot;) We compute the p-value as the proportion of statistics in the null distribution which are as extreme or more extreme than the observed statistic: pvalue &lt;- null_distribution %&gt;% summarise(pvalue = sum(prop &gt;= observed_prop) / n()) pvalue 13.4 Lab Research question of the lab Has the average body temperature for healthy humans changed from the long-thought 37 °C? In today’s lab we will investigate the average body temperature for healthy humans. You might probably be thinking that the average is about 37 °C, and this is what most people would answer as this has been taken as granted for many years. However, could it be possible that the average body temperature for healthy humans has changed over time? Perhaps this could be due to the climate change? We will use data5 comprising measurements on body temperature and pulse rate for a sample of \\(n = 50\\) healthy subjects. Using the concepts learned today, the question boils down to: do the sample data provide significant evidence (at a 5% level) that the average body temperature is really different from the long-thought 37 °C? Required packages Before attempting to answer the following questions, make sure to run the following code chunk, which assumes that you have already installed the packages tidyverse and moderndive. If you have not installed them yet, type install.packages(&quot;tidyverse&quot;) and install.packages(&quot;moderndive&quot;) in the R console. Load the tidyverse and moderndive packages: library(tidyverse) library(moderndive) Importing the data Let’s start by loading the data using the function read_tsv to read tab separated values: bodytemp &lt;- read_tsv(&#39;https://edin.ac/2vkjSaw&#39;) bodytemp ## # A tibble: 50 x 2 ## BodyTemp Pulse ## &lt;dbl&gt; &lt;dbl&gt; ## 1 36.4 69 ## 2 37.4 77 ## 3 37.2 75 ## 4 37.1 84 ## 5 36.7 71 ## 6 37.2 76 ## 7 37.2 81 ## 8 36.6 77 ## 9 36 75 ## 10 37.2 81 ## # … with 40 more rows Data inspection ► Question 1 What are the names of the variables in the dataset? What are the dimensions of the tibble? Are there any missing values? Hint: Use the function anyNA() to check if there are any Not Available (NA) entries. ► Solution names(bodytemp) ## [1] &quot;BodyTemp&quot; &quot;Pulse&quot; The variable names are BodyTemp and Pulse. dim(bodytemp) ## [1] 50 2 The tibble has 50 rows and two columns. anyNA(bodytemp) ## [1] FALSE The tibble does not appear to have missing values. Sample average and standard deviation ► Question 2 What is the average temperature in the sample and the standard deviation? ► Solution sample_stats &lt;- bodytemp %&gt;% summarise(avg_temp = mean(BodyTemp), sd_temp = sd(BodyTemp)) sample_stats ## # A tibble: 1 x 2 ## avg_temp sd_temp ## &lt;dbl&gt; &lt;dbl&gt; ## 1 36.8 0.425 The average body temperature in the sample is \\(\\bar{x} =\\) 36.81 °C and the standard deviation is \\(s =\\) 0.43 °C. Null and alternative hypothesis ► Question 3 State the null and alternative hypothesis for the research question of this lab. ► Solution \\[ H_0: \\ \\mu = 37\\ {}^\\circ\\mathrm{C} \\\\ H_1: \\ \\mu \\neq 37\\ {}^\\circ\\mathrm{C} \\] Bootstrap distribution and null distribution In order to calculate a p-value, we need to obtain the sampling distribution of the mean assuming that the null hypothesis is true (null distribution). However, we only have one sample, so we must approximate the sampling distribution with a bootstrap distribution showing how the sample mean varies from bootstrap sample to bootstrap sample. The required steps are: Obtain a bootstrap distribution (this is centred at the sample mean \\(\\bar{x} =\\) 36.81) Shift the bootstrap distribution to be centred at the parameter value specified in the null hypothesis (\\(\\mu =\\) 37). This is the null distribution. Note: Why did we shift the bootstrap distribution to obtain the null distribution? Because the null distribution must be centred at the value of the population parameter specified in the null hypothesis. ► Question 4 Compute the bootstrap distribution of the mean using 10,000 repeated samples. Plot the bootstrap distribution using a histogram. ► Solution The following code chunk computes the bootstrap distribution using 10,000 repeated samples: bootstrap_distribution &lt;- bodytemp %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 10000) %&gt;% group_by(replicate) %&gt;% summarise(avg_temp = mean(BodyTemp)) bootstrap_distribution ## # A tibble: 10,000 x 2 ## replicate avg_temp ## &lt;int&gt; &lt;dbl&gt; ## 1 1 36.7 ## 2 2 36.8 ## 3 3 36.8 ## 4 4 36.7 ## 5 5 36.9 ## 6 6 36.8 ## 7 7 36.8 ## 8 8 36.8 ## 9 9 36.9 ## 10 10 36.8 ## # … with 9,990 more rows Let’s plot the bootstrap distribution as a histogram, rather than a dotplot: ggplot(bootstrap_distribution, aes(x = avg_temp)) + geom_histogram(color = &#39;white&#39;) + labs(x = expr(bar(x))) The bootstrap distribution is centred around the sample mean: bootstrap_distribution %&gt;% summarise(avg = mean(avg_temp)) ## # A tibble: 1 x 1 ## avg ## &lt;dbl&gt; ## 1 36.8 ► Question 5 Compute the null distribution, which needs to be centred at the value of the parameter specified by the null hypothesis. Plot the null distribution using a histogram. Hint: Since you already have the bootstrap distribution, which is centred at the sample mean, you can shift the original sample by adding to each body temperature the difference between the parameter value in the null hypothesis and the sample mean. ► Solution The null distribution needs to be centred at the value of the parameter specified by the null hypothesis. The bootstrap distribution we computed earlier is centred at the sample mean. Recall that: \\[ H_0: \\ \\mu = 37\\ {}^\\circ\\mathrm{C} \\\\ H_1: \\ \\mu \\neq 37\\ {}^\\circ\\mathrm{C} \\] We could shift the bootstrap distribution to have mean 37 °C by adding 37 - 36.81 °C = 0.19 °C to each body temperature in the sample. Let us calculate the shift: null_hypothesis &lt;- 37 observed_mean &lt;- bodytemp %&gt;% summarise(avg_temp = mean(BodyTemp)) %&gt;% pull(avg_temp) observed_mean ## [1] 36.81111 shift &lt;- null_hypothesis - observed_mean shift ## [1] 0.1888889 Shift the original sample data values: bodytemp_shifted &lt;- bodytemp %&gt;% mutate(BodyTemp = BodyTemp + shift) null_distribution &lt;- bodytemp_shifted %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 10000) %&gt;% group_by(replicate) %&gt;% summarise(avg_temp = mean(BodyTemp)) null_distribution ## # A tibble: 10,000 x 2 ## replicate avg_temp ## &lt;int&gt; &lt;dbl&gt; ## 1 1 37.1 ## 2 2 37.1 ## 3 3 37.0 ## 4 4 36.9 ## 5 5 37.0 ## 6 6 37.1 ## 7 7 37.1 ## 8 8 36.9 ## 9 9 36.9 ## 10 10 37.1 ## # … with 9,990 more rows The null distribution is centred at the parameter value specified in the null: null_distribution %&gt;% summarise(avg = mean(avg_temp)) ## # A tibble: 1 x 1 ## avg ## &lt;dbl&gt; ## 1 37.0 We can visualise the null distribution using a histogram: ggplot(null_distribution, aes(x = avg_temp)) + geom_histogram(color = &#39;white&#39;) + labs(x = expr(bar(x))) Visualise the observed statistic on the null distribution ► Question 6 Show a histogram of the null distribution, superimposing a vertical red line displaying the observed statistic. ► Solution ggplot(null_distribution, aes(x = avg_temp)) + geom_histogram(color = &#39;white&#39;) + geom_vline(xintercept = observed_mean, color = &#39;tomato1&#39;, size = 1) + labs(x = expr(bar(x))) P-value ► Question 7 Calculate the proportion of means as extreme as the observed statistic in the null distribution. Hint: As this is a double-sided alternative hypothesis, remember to use 2 * p-value of the smallest tail. ► Solution This is a two-sided alternative, hence the p-value is two times the proportion in the smallest tail. Let’s plot the smallest tail: ggplot(null_distribution, aes(x = avg_temp, fill = (avg_temp &lt;= observed_mean))) + geom_histogram(color = &#39;black&#39;, binwidth = 0.01, size = 0.1) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + labs(x = expr(bar(x)), fill = expr(bar(x) &lt;= !!round(observed_mean, 2))) null_distribution %&gt;% summarise(count_smaller = sum(avg_temp &lt;= observed_mean)) ## # A tibble: 1 x 1 ## count_smaller ## &lt;int&gt; ## 1 7 Out of the 10,000 means in the null distribution, 7 of them are lower than the observed sample statistic (36.81). The proportion of means as low as the observed mean (36.81) in the null distribution is then: null_distribution %&gt;% summarise(prop_smaller = sum(avg_temp &lt;= observed_mean) / n()) ## # A tibble: 1 x 1 ## prop_smaller ## &lt;dbl&gt; ## 1 0.0007 Since this test has a double-sided alternative, we must double this proportion in order to obtain the p-value. This represents the proportion of means in the null distribution as extreme as the observed one: pvalue &lt;- null_distribution %&gt;% summarise(pvalue = 2 * sum(avg_temp &lt;= observed_mean) / n()) pvalue ## # A tibble: 1 x 1 ## pvalue ## &lt;dbl&gt; ## 1 0.0014 The p-value is equal to 0.0014. Interpreting the p-value ► Question 8 Using the p-value found in the previous question, how would you answer the research question of the lab? ► Solution At a significance level of 5%, the very small p-value we have found, 0.0014 &lt; 0.05, gives very strong evidence against the null hypothesis that the average body temperature for healthy humans is 37 °C. However, it is worth noting the difference between statistical significance and practical significance. Even if the sample results lead to convincingly rejecting the null hypothesis, assuming the average body temperature for healthy humans to be closer to 36.8 °C rather than 37 °C has a very minimal impact in practice. More generally, with large sample sizes a small difference might turn out to be statistically significant. However, this does not mean that the difference will be of practical importance to decision-makers. 13.5 Glossary Alternative hypothesis. A claim about the population parameter for which we seek evidence for. It contradicts the null hypothesis. Statistical test. A set of principles for measuring the strength of evidence against a null hypothesis about the parameter of interest. Null hypothesis. A claim about the population parameter typically involving “no effect” or “no difference between groups”. Null distribution. The values of the statistic that we expect to obtain by sampling variation if the null hypothesis is true. In other words, the sampling distribution of the statistic assuming the null hypothesis to be true. P-value. The probability of obtaining a value of the statistic at least as extreme as the observed statistic when the null hypothesis is true. Significance level. A threshold used as a criterion for deciding when a p-value is small enough to provide convincing evidence against the null hypothesis. It is typically denoted by \\(\\alpha\\) and common values are 0.01 or 0.05. Statistically significant. Unlikely to occur just by random chance. 13.6 References Lock, R. H., Lock, P. F., Morgan, K. L., Lock, E. F., &amp; Lock, D. F. (2013). Statistics: Unlocking the power of data. John Wiley &amp; Sons. Tintle, N., Chance, B. L., Cobb, G. W., Rossman, A. J., Roy, S., Swanson, T., &amp; VanderStoep, J. (2015). Introduction to statistical investigations. New York: Wiley. Shoemaker, A. L. (1996). What’s Normal: Temperature, Gender and Heartrate. Journal of Statistics Education, 4(2), 4.↩ "],
["chap-typeerror.html", "Chapter 14 Type I, Type II errors and Power 14.1 Overview of last week 14.2 Walkthrough 14.3 Summary 14.4 Lab 14.5 Extra Exercises 14.6 Glossary 14.7 References", " Chapter 14 Type I, Type II errors and Power Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. It contains the code for the worked examples should you wish to follow along, and blank spaces for your answers to the questions in the second hour. Learning outcomes LO1. Revision of hypothesis testing LO2. Understand Type I and Type II errors LO3. Introduce statistical power 14.1 Overview of last week Last week we learned to frame a research question in terms of null and alternative hypotheses about parameters. We related the null hypothesis (\\(H_0\\)) to a ‘random chance model’ - i.e., if differences or effects that we observe are actually just due to sampling variation. We generated a distribution for the null hypothesis (the null distribution) which reflected how much sample statistics would vary due to chance if the null hypothesis is true (i.e., if there really is no difference/effect). We did this by simulating lots of samples and computing the statistic on each of the samples. We then compared the observed statistic to the null distribution we had generated. This is equivalent to asking how likely it would be to get our observed statistic if the null hypothesis was actually true. We learned that the p-value is the proportion of simulated sample statistics in our null distribution which were as or more extreme than our observed statistic. Finally, we thought about how we might make a formal decision about whether or not to reject the null hypothesis based on our p-value. This was based on a pre-specified level (known as \\(\\alpha\\)), below which we would consider p-values to be statistically significant. This week, we will recap this process of hypothesis testing, before thinking about the ways in which this method might lead to error. 14.2 Walkthrough Example 1: Coin flip Research Question &amp; Hypotheses Is our coin biased? Null hypothesis: We’re just as likely to get heads as tails when we flip the coin. We will denote by \\(p\\) the probability of ‘heads’. \\[H_0: p = 0.5\\] Alternative hypothesis: We’re more likely to see either heads or tails when we flip the coin. \\[H_1: p \\neq 0.5\\] Data collection We flip the coin 90 times, and it lands on heads 55 times. \\[H, T, H, H, T, T, T, H, H, H, H, T, H, H, T, T, H, T, H, H,...\\] Analysis Steps Calculate the sample statistic. Generate the null distribution. Calculate the probability of seeing our statistic (or one which is farther away from the null) if the null were true (this is the p-value). Compare the p-value to our pre-specified \\(\\alpha\\)-level (for this example we will use the conventional 0.05). 1. Calculate the sample statistic, \\(\\hat{p}\\) p_hat &lt;- 55/90 p_hat ## [1] 0.6111111 2. Generate the null distribution Remember that the null distribution is what we would expect if the null hypothesis were true - it is how much the statistics computed from samples of size \\(n\\) would vary if the null is true. In our case, this quantifies how much our statistic (the proportion of heads) in a sample of size 90 would vary if the true probability of the coin landing on heads were 1/2. # Specify our possible outcomes and their probabilities under the null outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(1/2, 1/2) # Generate samples under the null samples &lt;- rep_sample_n(outcomes, size = 90, replace = TRUE, reps = 1000, prob = prob) # Calculate the statistics for each sample to create the null distribution null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) Now we can plot our null distribution: ggplot(null_distribution, aes(x = prop)) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = &#39;white&#39;, stackratio = 0.5) + labs(x = expr(hat(p))) And plot the observed statistic on top, like we did last week: ggplot(null_distribution, aes(x = prop, fill = (prop &gt;= 0.61))) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + geom_vline(xintercept = 0.61, color = &#39;tomato1&#39;, size = 1) + labs(x = expr(hat(p)), fill = expr(hat(p) &gt;= 0.61)) 3. Calculate our p-value How surprising is 55 heads in 90 coin flips? We can compare it against the null distribution. ► Question What is our p-value? The proportion of statistics in the null distribution which are \\(\\geq0.61\\). The proportion of statistics in the null distribution which are \\(\\geq0.61\\) or \\(\\leq0.39\\). Two times the proportion of statistics in the null distribution which are \\(\\geq0.61\\). ► Solution If we perform an hypothesis test for the two-sided alternative \\(p \\neq 0.5\\), this is the proportion of statistics at least as extreme in either direction, so we’re looking at both tails of the distribution (i.e., answer B above). However, we can assume the distribution to be symmetric, so we can simply multiply the proportion of the null distribution in one tail by two (i.e., answer C above). Both answers (b) and (c) are correct. As you can see, both methods return very similar estimates of the p-values, with the estimates getting closer and closer as the number of simulated samples increases. pvalue &lt;- null_distribution %&gt;% summarise( pvalue_bothtails = sum(prop &lt;= 0.39 | prop&gt;=0.61) / n(), pvalue_2righttail = 2 * sum(prop&gt;=0.61) / n() ) pvalue ## # A tibble: 1 x 2 ## pvalue_bothtails pvalue_2righttail ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.04 0.044 ► Question At a significance level of \\(\\alpha = 0.05\\), what is our formal decision about the hypotheses? Not enough evidence to reject \\(H_0\\).(\\(H_0:\\) We’re just as likely to get heads as tails when we flip the coin.) Evidence against \\(H_0\\). ► Solution According to the p-value previously found, 0.04 \\(\\leq\\) 0.05, we have reason to reject the null hypothesis that our coin will land on heads and tails equally often. An analogy in law Might we have made a mistake here? Is 55 out of 90 coin flips surprising enough for us to reject the hypothesis that the coin is fair? Last week we discussed an analogy in law, in which a person on trial is presumed innocent until proven guilty. Similarly, we presume \\(H_0\\) to be true until there is strong evidence to reject it. How strong must the evidence be? How do we avoid wrongly convicting an innocent person? (i.e., wrongly rejecting an hypothesis which is actually true?) Two different types of errors Person is innocent\\(H_0\\) is True Person is guilty\\(H_0\\) is False Verdict = Innocent Test fails to reject \\(H_0\\) Correct decision Criminal goes free Type II Error \\(\\beta\\) Verdict = Guilty Test rejects \\(H_0\\) Wrongful conviction Type I Error \\(\\alpha\\) Correct decision Type I errors If the null hypothesis is true, then the sampling distribution of our statistic follows the null distribution which we constructed above, and we will reject (incorrectly) any observed statistic which has a corresponding p-value of less than or equal to our \\(\\alpha\\) level (e.g., the 0.05 we set earlier). So the probability of making this error is equal to the \\(\\alpha\\) level which we set. In other words, when the null hypothesis is true, 0.05 (or 5%) of the random samples we could take would result in us rejecting it. ► Question If there are 100 researchers, each testing a fair coin at \\(\\alpha = 0.05\\), how many of them will incorrectly reject their null hypothesis that the coin is equally likely to land on heads or tails (\\(p = 0.5\\)). ► Solution We would expect 0.05 (5%) of the 100 researchers’ random samples (sequence of coin flips) to have a proportion of heads extreme enough for them to reject the null hypothesis. We would expect 5 researchers to make a Type I Error. Type II Errors and Power The other kind of error we might make is a Type II Error, and is denoted by \\(\\beta\\). This happens when \\(H_0\\) is false, but we do not have enough evidence to reject it. In our table, the columns specify the possible states of the world (\\(H_0\\) is either True or False). In each of the possible states of the world, there are two potential outcomes of conducting a statistical test (reject \\(H_0\\) or don’t reject \\(H_0\\)). We have seen that: If \\(H_0\\) is true, then the probability of incorrectly rejecting \\(H_0\\) is \\(\\alpha\\) (often set at 0.05), and the probability of correctly retaining (not rejecting) \\(H_0\\) is 0.95. If \\(H_0\\) is false, then the probability of incorrectly failing to reject \\(H_0\\) is \\(\\beta\\), and the probability of correctly rejecting \\(H_0\\) is \\(1-\\beta\\). This is known as the statistical power of our test. \\(H_0\\) is True \\(H_0\\) is False Test doesn’t reject \\(H_0\\) Correct\\(1 - \\alpha\\) Type II Error \\(\\beta\\) Test rejects \\(H_0\\) Type I Error \\(\\alpha\\) CorrectPower \\(1 - \\beta\\) Power of a statistical test The power of a statistical test (\\(1-\\beta\\)) is the probability of rejecting the null hypothesis when the alternative hypothesis is true. In other words, the power of a test is the probability that the test finds an effect if there is an effect to be found. Example 2: A biased coin I have a trick coin which is weighted so that it lands on heads 60% of the time (rather than the usual 50% for a normal fair coin). Oh no! Tom has noticed that whenever we flip a coin, I always call heads, and I often seem to win. He accuses me of cheating by using a trick coin which is biased to land on heads! I make him an offer: he can flip the coin 50 times in order to decide whether or not it really is a trick coin. ► Question Tom’s null hypothesis is that he is just as likely to get heads as tails when he flips the coin. \\[H_0: p = 0.5\\] Where \\(p\\) is the probability of ‘heads’, what is his alternative hypothesis? \\(H_1: p \\neq 0.5\\) \\(H_1: p &gt; 0.5\\) \\(H_1: p &lt; 0.5\\) ► Solution Given that Tom thinks the coin is biased towards landing on heads, we will use (b) as his alternative hypothesis: \\(H_1: p &gt; 0.5\\). To calculate the power of Tom’s 50 flips to detect whether or not I’m using a trick coin we need to: Generate the null distribution for Tom’s test. Calculate the critical value (the minimum number of heads in 50 flips which would lead Tom to reject his null hypothesis). Calculate the actual probability of seeing a statistic larger than the critical value, given that we know the true bias of the coin. This is the statistical power of Tom’s test. 1. Generate Tom’s null distribution In Tom’s 50 coin flips, at a significance level of 0.05, what proportion of heads would lead him to the conclusion that the coin is a trick coin? The first step is to generate the null distribution: # Specify our possible outcomes and their probabilities under the null outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.5, 0.5) # Generate samples under the null samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # Calculate the statistics for each sample to create the null distribution null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) 2. Calculating Tom’s critical value: Now that we have generated the null distribution which Tom will use to test his observed statistic, we need to work out the values at which he will reject the null. In other words, we need to work out where the top 5% of the null distribution is. Note that we are only looking at the top 5% because Tom thinks the coin is biased towards heads. So his alternative hypothesis (\\(H_1\\)) is \\(p &gt; 0.5\\), and he will reject \\(H_0\\) if his observed statistic falls in the top end of the distribution. To do this, we’re going to use a new function called quantile(), which will give us the value for which a given percentage of the distribution of simulated sample statistics is to the left (e.g., smaller). crit_val &lt;- null_distribution %&gt;% summarise(crit95 = quantile(prop, 0.95)) %&gt;% pull(crit95) crit_val ## 95% ## 0.62 If Tom flips the coin 50 times, under the null hypothesis, he would need to get a sample statistic (\\(\\hat{p}\\)) of greater than or equal to 0.62. 3. Calculating Tom’s power Things we know so far: The coin is rigged to land on heads on 60% of flips - the true probability of heads is 0.6. If 62% or more of Tom’s 50 coin flips come up heads, then he will reject his null hypothesis (that the coin is fair). What’s the probability that Tom’s 50 flips will come up with 62% or more heads using the biased coin? In other words, what is the power of his test? We can do this by generating the sampling distribution for when the coin is biased towards heads 60% of the time (which we know is actually true). outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.6, 0.4) samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # Calculate the statistics for each sample to create the true distribution true_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) tom_power &lt;- true_distribution %&gt;% summarise( prob_crit = sum(prop &gt;= crit_val) / n() ) tom_power ## # A tibble: 1 x 1 ## prob_crit ## &lt;dbl&gt; ## 1 0.463 Tom’s 50 flips has 46.3% power. What does this mean? It means that he has a 0.463 probability of concluding that my biased coin is biased! It might help if we plotted the two distributions which we just generated. We’ll start with Tom’s null distribution (proportions of heads in 1000 simulated samples of size 50, assuming the probability of heads = 0.5). It’s a histogram, with the dotplots on top just to show their equivalence. This shows the critical value and region for Tom’s one-tailed test at \\(\\alpha = 0.05\\). Finally, let’s add to that the true distribution which we generated based on our knowledge that the coin is actually biased towards landing on heads 60% of the time. Here it is (in green): Think about what this shows. For a coin which is biased towards landing on heads 60% of the time, more often than not, the proportion of heads in 50 flips will fall below 0.62 (which is Tom’s critical value - i.e., the minimum value which would result in him rejecting the null hypothesis). In fact, the probability of 50 flips of the trick coin resulting in a proportion of heads large enough for Tom to reject the null is about 46.3%. This is the proportion of the true sampling distribution (green, above) which is equal to or greater than the critical value (black vertical line). Overlapping distributions We’ve just seem a visualisation of two different possible sampling distributions, one reflecting the null hypothesis, and one reflecting an alternative (in this case, it reflected the true bias of the coin). It helps to think of the statistical power of a test in terms of the overlap of two such distributions. ► Question How will our choice of \\(\\alpha\\) influence our statistical power? (Try explaining it in terms of the two distributions we saw above) ► Solution A lower \\(\\alpha\\) means that we need stronger evidence to reject the null hypothesis. We have less power when we have a lower \\(\\alpha\\) because the critical value is further from the center of the null distribution (i.e., more extreme). Less of the alternative distribution will be equal or greater than a more extreme critical value. ► Question How will our sample size influence our statistical power? (Try explaining it in terms of the two distributions we saw above) ► Solution A bigger sample means a smaller standard error (see Week 11). This would make both sampling distributions (that under the null hypothesis, and that under an alternative hypothesis) narrower, and therefore overlap less, leading to more power. ► Question How would it influence our statistical power if we assumed the coin was actually rigged to land 70% of the time on heads? (Try explaining it in terms of the two distributions we saw above) ► Solution When there is greater difference between the null distribution and an hypothesised distribution, the two distributions will overlap less, leading to more power. A nice visualisation of power can be found here: https://rpsychologist.com/d3/NHST/ Generalising it In our example, we knew the true bias of the coin. But Tom didn’t! What Tom could do, is calculate the power of his test assuming a given value for \\(p\\). He might have thought to himself: “Hmm, that coin seems to land on heads about 3/4 of the time. If that is true, I want to know what the probability of me being able to correctly reject the null hypothesis is when I flip the coin 50 times”. Had Tom used the R code above he might have called his distribution hypothesised_distribution rather than true_distribution! When we conduct NHST (Null Hypothesis Significance Testing), we set \\(\\alpha\\). In setting \\(\\alpha\\), we define a critical region under the null distribution. The critical value is the value of the statistic which defines the start of this region. Any statistic more extreme than this will result in rejecting the null hypothesis. If the null hypothesis is false, the probability that we reject the null hypothesis depends on a) our \\(\\alpha\\) level, b) how far away from the null distribution the assumed state of the world is, and c) our sample size. This means that, for an assumed effect size (i.e., difference from the null hypothesis) - or set of effect sizes - we can compute the power of a test for a given sample size. We can then do useful things such as reframe this to find out what the minimum sample size is that would be required to achieve a given level of power to detect a given effect size. 14.3 Summary Today, we have recapped what we learned last week about hypothesis testing, and introduced some key ideas. We learned that when we reject a true null hypothesis because we happen (by random chance) to have a sample statistic which is unlikely under the null, we commit a Type I Error. We saw how the probability of this happening is equal to our \\(\\alpha\\) level. We also learned that failing to reject the null hypothesis when it is actually false, is known as a Type II Error, and the probability of this depends on how far away from the null parameter the true parameter is (i.e., effect size), as well as how big our sample is. We simulated different distributions under different hypotheses, using the code below: # Our possible outcomes outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) # Our probabilities (reflecting the hypothesis) prob &lt;- c(0.5, 0.5) # Our simulated samples of size 50 samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # Our distribution distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) We calculated the critical value (for a one-sided test) for a null distribution by using the quantile() function, to find the values at which 95% of the distribution of simulated sample statistics are to the left. crit_val &lt;- null_distribution %&gt;% summarise(crit95 = quantile(prop, 0.95)) %&gt;% pull(crit95) For a two-sided test at \\(\\alpha = 0.05\\), we can simply extend this to ask for the values where 2.5% of the distribution is to the left, and where 97.5% of the distribution is to the left (so 2.5% is to the right). In doing so, we divide our critical region in to the two tails of the distribution. null_distribution %&gt;% summarise( crit025 = quantile(prop, 0.025), crit975 = quantile(prop, 0.975) ) ## # A tibble: 1 x 2 ## crit025 crit975 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.36 0.64 We can find the proportion of a different theorised distribution which fell beyond a critical value, giving our power: another_distribution %&gt;% summarise( prob_crit = sum(prop &gt;= crit_val) / n() ) 14.4 Lab 14.4.1 Exercise 1: Rock Paper Scissors! If you haven’t ever played Rock, Paper, Scissors before, then play it now against someone nearby! A paper in 2009 found that novice players tend to avoid choosing scissors. We are going to test this. Research Question Are novice players of Rock, Paper, Scissors biased to choosing scissors less often? ► Question 1 Write out your null and alternative hypotheses. Is our alternative hypothesis one-sided or two-sided? ► Solution Null hypothesis: Novice players are equally likely to choose Scissors as they are to choose Rock or Paper. Alternative hypothesis: Novice players are less likely to choose Scissors as they are to choose Rock or Paper. Where \\(p\\) denotes the proportion of games in which players choose scissors: \\[H_0: p = 1/3\\] \\[H_1: p &lt; 1/3\\] Our alternative hypothesis is one-sided, because we are suggesting that the proportion of games in which players choose scissors is less than 1/3. ► Question 2 Assuming that novice players actually choose Scissors only 1/5 of the time (\\(p = 1/5\\)), calculate the power to detect this if we conduct a test (at \\(\\alpha = 0.05\\)) based on 75 games of Rock, Paper, Scissors with novice players. ► Solution We need to generate our null distribution: outcomes &lt;- tibble(vals = factor(c(&#39;Rock&#39;,&#39;Paper&#39;,&#39;Scissors&#39;))) prob &lt;- c(1/3, 1/3, 1/3) # GENERATE THE NULL samples &lt;- rep_sample_n(outcomes, size = 75, replace = TRUE, reps = 1000, prob = prob) null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Scissors&#39;) / n()) And calculate the critical value (note that because the alternative is \\(p &lt; 1/3\\), we are looking for the bottom 5%, rather the top 5%): # FIND CRITICAL VALUE crit_val &lt;- null_distribution %&gt;% summarise(crit95 = quantile(prop, 0.05)) %&gt;% pull(crit95) Construct an alternative distribution: # GENERATE ALTERNATIVE outcomes &lt;- tibble(vals = factor(c(&#39;Rock&#39;,&#39;Paper&#39;,&#39;Scissors&#39;))) prob &lt;- c(0.4, 0.4, 0.2) samples &lt;- rep_sample_n(outcomes, size = 75, replace = TRUE, reps = 1000, prob = prob) ## Joining, by = &quot;vals&quot; ## Warning: Column `vals` joining character vector and factor, coercing into character vector hypothesised_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Scissors&#39;) / n()) And calculate power. # CALCULATE POWER hypothesised_distribution %&gt;% summarise( prob_crit = sum(prop &lt;= crit_val) / n() ) ## # A tibble: 1 x 1 ## prob_crit ## &lt;dbl&gt; ## 1 0.898 ► Question 3 Calculate the power if we were to conduct the same test at \\(\\alpha = 0.01\\) instead. ► Solution We would have a different critical value: crit_val &lt;- null_distribution %&gt;% summarise(crit95 = quantile(prop, 0.01)) %&gt;% pull(crit95) And our power would therefore change accordingly: # CALCULATE POWER hypothesised_distribution %&gt;% summarise( prob_crit = sum(prop &lt;= crit_val) / n() ) ## # A tibble: 1 x 1 ## prob_crit ## &lt;dbl&gt; ## 1 0.661 14.4.2 Exercise 2: Calculating power for a different coin. ► Question 4 If my trick coin was actually weighted so that it landed on heads 65% of the time, what would the power of Tom’s test (50 flips) be? ► Solution We already know the critical value at which Tom will reject the null hypothesis based on his 50 flips. We need to: Generate the theoretical distribution of \\(\\hat{p} = 0.65\\). #Remember that Tom&#39;s critical value is: crit_val &lt;- 0.62 outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.65, 0.35) samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) theoretical_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) Calculate what proportion of this distribution is greater than his critical value. This will be the power. theoretical_distribution %&gt;% summarise( prob_crit = sum(prop &gt;= crit_val) / n() ) ## # A tibble: 1 x 1 ## prob_crit ## &lt;dbl&gt; ## 1 0.724 14.5 Extra Exercises 14.5.1 Exercise 3: Sample size (number of flips) and power ► Question 5 Assuming the coin to be biased towards landing on heads 60% of the time, calculate the power of Tom’s statistical test for when he flips the coin 75 times, 100 times, and 200 times. Which sample size should Tom use if he wants at least 80% power? ► Solution We can run this chunk changing the nr_flips value each time, to calculate power at different sample sizes. nr_flips &lt;- 50 outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.5, 0.5) # GENERATE THE NULL samples &lt;- rep_sample_n(outcomes, size =nr_flips, replace = TRUE, reps = 1000, prob = prob) null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) # FIND CRITICAL VALUE crit_val &lt;- null_distribution %&gt;% summarise(crit95 = quantile(prop, 0.95)) %&gt;% pull(crit95) # GENERATE THEORISED ALTERNATIVE outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.6, 0.4) samples &lt;- rep_sample_n(outcomes, size = nr_flips, replace = TRUE, reps = 1000, prob = prob) hypothesised_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) # CALCULATE POWER hypothesised_distribution %&gt;% summarise( prob_crit = sum(prop &gt;= crit_val) / n() ) ## # A tibble: 1 x 1 ## prob_crit ## &lt;dbl&gt; ## 1 0.445 ► Question 6 List the three things which affect the power of a statistical test. ► Solution Alpha level Sample size Effect size A thought experiment There are 20 researchers. Each researcher has a perfectly balanced/fair coin. Each researcher conducts a statistical test at \\(\\alpha = 0.05\\) to evaluate whether their coin is fair (lands on heads equally as often as it lands on tails). How many of the researchers’ tests would we expect to result in a Type I Error? Remember: The probability of making a Type I error is the probability of getting an unlikely sample statistic simply due to chance sampling variation (i.e., we just happen to get a random sample with an unlikely statistic). ► Solution This is similar to asking “What is the probability of observing at least one significant result due to chance sampling variation alone?”. We can work this out.. For one researcher, if their null hypothesis is true, the probability that they get a significant result is 0.05, and the probability that they get a non-significant result (p-value \\(&gt; 0.05\\)) is 0.95. If there are 20 researchers, the probability of them all getting non-significant results when their null hypotheses are all true is \\(0.95^{20} = 0.358\\). This means that the probability of the opposite - at least one of them gets a significant result even though all their nulls are true - is \\(1 - 0.358 = 0.642\\). 14.6 Glossary Type I Error. Rejecting the null hypothesis when it is actually true (false positive). Its probability is denoted by \\(\\alpha\\). Type II Error. Failing to reject a null hypothesis that is actually false (false negative). Its probability is denoted by \\(\\beta\\). Power (\\(1 - \\beta\\)). The probability of (correctly) rejecting the null hypothesis when it is false. Critical Region. The area of the null distribution in which an observed sample statistic would lead to rejecting the null hypothesis. The area of the critical region corresponds to \\(\\alpha\\)% of the null distribution. Critical value. The value of the statistic which defines the start of the critical region. Any statistic more extreme than this will result in rejecting the null hypothesis. Effect size. The distance from the value of the parameter under the null hypothesis. When we calculate power, we often do so for an assumed effect size. When we take a sample and calculate an observed statistic, the distance from that observed statistic to the parameter under the null is our observed effect size. 14.7 References Eyler, D., Shalla, Z., Doumaux, A., &amp; McDevitt, T. (2009). Winning at Rock-Paper-Scissors. The College Mathematics Journal, 40(2), 125-128. Tintle, N., Chance, B. L., Cobb, G. W., Rossman, A. J., Roy, S., Swanson, T., &amp; VanderStoep, J. (2015). Introduction to statistical investigations. New York: Wiley. "],
["chap-normal.html", "Chapter 15 Normal distribution &amp; probability 15.1 Recap 15.2 Walkthrough 15.3 Summary 15.4 Lab 15.5 Glossary", " Chapter 15 Normal distribution &amp; probability Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. Please attempt the lab questions as they are not just a mere copy of the walkthrough code using a different dataset. They consolidate key points that were covered in the lecture, but not in the walkthrough. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand the normal distribution. LO2. Compute probabilities using density functions. LO3. Understand the link between simulation-based standard errors and theory-based standard errors. 15.1 Recap Flow-chart revising how to perform hypothesis testing using the p-value method. Flow-chart revising how to perform hypothesis testing using the critical value or rejection region method). 15.2 Walkthrough 15.2.1 From histograms to normal curves In the past four weeks, we plotted many histograms and dotplots. These plots helped us understand the distribution of a variable or a statistic. Distribution The distribution of a variable shows how frequently different values of the variable occur. The graph showing the distribution of a variable shows us where the values are centred, how the values vary, and gives some information about where a typical value might fall. It can also alert you to the presence of outliers (unexpected observations). Pretty much all of the distributions we saw were symmetric and bell-shaped. This typically is the case when the sample size is large enough. Figure 15.1 shows some of the distributions we plotted in the past four weeks, with a red curve superimposed on top. Note how the red curve closely approximates the histograms. NFL example: sampling distribution of the mean for sample size \\(n = 50\\); NFL example: sampling distribution of the mean for sample size \\(n = 500\\); Coin example: sampling distribution of the proportion of heads in \\(n = 50\\) tosses with probability of heads \\(p = 0.3\\); Coin example: sampling distribution of the proportion of heads in \\(n = 50\\) tosses with probability of heads \\(p = 0.7\\). Figure 15.1: Different distributions with their corresponding normal curves shown in red. The symmetric and bell-shaped red curve is known as the normal curve and is a mathematical model (i.e. an equation or graph) which is used to describe reality (see Figure 15.2). Figure 15.2: Normal curve. Normal distribution A continuous variable is said to be a normally distributed, or to have a normal probability distribution, if its distribution has the shape of a normal curve (i.e. it is symmetric and bell-shaped). Key question How can we make sure that the normal curve can adapt to any symmetric and bell-shaped histogram? Every histogram can be centred at a different value and have a different spread. We can specify the normal curve appropriate for each histogram by specifying where it should be centred at, and what should its spread be. Hence, the normal curve depends on two quantities called the parameters of the normal distribution (see Figure 15.3): the mean \\(\\mu\\), specifying the centre of the distribution; the standard deviation \\(\\sigma\\), specifying the spread of the distribution. ## Warning: `mapping` is not used by stat_function() ## Warning: `mapping` is not used by stat_function() ## Warning: `mapping` is not used by stat_function() ## Warning: `mapping` is not used by stat_function() Figure 15.3: Normal curves for different means and standard deviations. ► Question What happens when we vary the mean and the standard deviation parameters? ► Solution By varying the mean only, we can model histograms that are centred at different values but have the same spread. By varying the standard deviation only, we can model histograms with different spreads but that have the same mean. By varying both the mean and the standard deviation we can model any symmetric and bell-shaped histogram. Notation We will write that a random variable \\(X\\) follows a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) as: \\[X \\sim N(\\mu, \\sigma)\\] 15.2.2 The relation between area and probability Consider panel (a) in Figure 15.1. Each bar in the histogram shows the proportion of all the generated statistics falling in a given interval. For example, the proportion of sample means between 2.2 and 2.3 is equal to 0.09. This proportion can be interpreted as a probability. So, the proportion of means between 2.2 and 2.3 has two possible interpretations: the proportion of all samples with a mean between 2.2 and 2.3 is 9%; there is a 9% probability that a randomly selected sample from the population has a mean between 2.2 and 2.3. We will now discuss the uniform distribution to understand the relation between area and probability. Suppose that you are performing an experiment to study the time until loss of attention. You are currently monitoring a subject from a starting point, coded as time = 0, until up to 30 minutes later. The subject is equally likely to lose attention at any point across the 30 minutes. They are equally likely to lose attention after 3-4 minutes as they are likely after 25-26 minutes, or after 6-10 minutes as they are after 20-24 minutes. More generally, the subject is equally likely to lose attention in intervals of time that have the same length. Such a random variable is said to follow a uniform distribution. For discrete random variables, i.e. random variables whose possible values are only whole numbers without decimals, we compute probabilities as the number of favourable cases divided by the total number of possibilities. The probability of seeing an even face when throwing a die is obtained by dividing the number of ways the event “even face” can occur (three even faces in a die: ⚁, ⚃, ⚅) by the total number of possibilities (six total faces in a die: ⚀, ⚁, ⚂, ⚃, ⚄, ⚅): \\[ P(\\textrm{even face}) = \\frac{n_\\textrm{even faces}}{n_\\textrm{faces}} = \\frac{3}{6} = 0.5 \\] ► Question Think about continuous random variables, such as time until loss of attention or reaction times. For continuous random variables, can we compute probabilities in the same way? ► Solution The answer is no. A continuous random variable can take an infinite number of values. For this reason, the probability of observing a specific value is zero. For example, the probability of your subject losing focus after exactly 3.9483627298 minutes is zero. There is only one particular way to observe 3.9483627298, and there is an infinite number of values between 0 and 30. Key point For continuous random variables, we can only compute the probability of intervals of values. In order to find probabilities for continuous random variables, we use probability density functions. Probability density function A probability density function is an equation used to compute the probabilities for continuous random variables. It must satisfy the following properties: the total area under the graph of the equation must be equal to one, equivalent to the fact that the total probability must be one; the graph of the equation must be greater than or equal to zero for all possible values. For discrete random variables, property 1 is similar to saying that the sum of all probabilities must be equal to 100%. Property 2 is similar to saying that probabilities can not be negative. Figure 15.4: Uniform distribution between 0 and 30 minutes. Figure 15.4 shows the uniform distribution for the loss of attention example. Let’s analyse it in more detail. Each value between 0 and 30 minutes is equally likely, so the graph is a rectangle. The width of the rectangle is 30 as the possible values of the random variable are 0 to 30. Property 1 of density functions states that the total area under the graph must be 1. We will use this relationship to find what the height of the rectangle should be. Recalling that the area of a rectangle is given by the width times height, \\[ \\begin{aligned} a &amp;= w \\times h \\\\ 1 &amp;= 30 \\times h \\end{aligned} \\] we find that the density is given by the height: \\[ h = \\frac{1}{30} = \\textrm{density} \\] How can we use this density function to calculate probabilities? In order to find the probability of your subject losing attention between 10 and 20 minutes after the start of the experiment, we must calculate the shaded area in Figure 15.5. The width of the shaded region is \\(20-10 = 10\\). The height of the shaded region is \\(1/30\\). The area between 10 and 20 is \\(10 \\times \\frac{1}{30} = \\frac{1}{3}\\). So, the probability of the subject losing attention between 10 and 20 minutes from the beginning of the study is \\(1/3\\). Figure 15.5: Probability of losing attention after 10 to 20 minutes. ► Question How would you calculate the probability of the subject losing focus either in the first 10 minutes or the last 10 minutes? In other words, between 0-10 minutes and 20-30 minutes? ► Solution Since the total area of the rectangle is 1 (the total probability must be 1), we can compute this area as 1 - the shaded area. The probability of losing focus between 0-10 minutes or 20-30 minutes is \\(1 - \\frac{1}{3} = \\frac{2}{3}\\). We have introduced the uniform distribution to link the concepts of probability and area under a curve. Now, strong in what we have learned, we will move towards a more widely used model for continuous random variables: the normal distribution, which we have previously introduced. 15.2.3 Probabilities for normally distributed random variables We used a rectangle to find the probability of a uniformly distributed variable being in a given range. However, only a few continuous random variables follow such a graph. More common is to see random variables whose distribution is symmetric and bell-shaped. This is the case for aptitude-test scores, the birth weights of newborns, and so on… We previously learned that if a random variable has a histogram which is symmetric and bell-shaped, we say that the variable follows a normal distribution. Consider the following sample of 10,000 people, for which we have recorded their score on a cognitive test. Let’s load the data and inspect them: library(tidyverse) # Load the sample cognitive &lt;- read_csv(&#39;https://edin.ac/39jPFa5&#39;, col_names = TRUE) # Display the first six rows head(cognitive) ## # A tibble: 6 x 2 ## SubjectID CognitiveScore ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 90.6 ## 2 2 103. ## 3 3 87.5 ## 4 4 124. ## 5 5 105. ## 6 6 87.7 # Check the dimensions of the tibble dim(cognitive) ## [1] 10000 2 We have 10,000 rows (subjects) and two columns (measured variables). Let’s plot the distribution of the variable of interest, CognitiveScore, as a histogram. However, on the y-axis, instead of the absolute counts of the values we ask for the density or proportion. This makes it easier to compare across samples of different sizes, as the range of the y-axis won’t change with the sample size. We plot the density by specifying y = stat(density) in the aes() function: ggplot(cognitive, aes(x = CognitiveScore, y = stat(density))) + geom_histogram(color = &#39;white&#39;) We can see that the distribution of cognitive scores is symmetric, centres approximately around 100, and is bell-shaped. We can therefore try to fit a normal curve to it. We must specify: where the normal curve should be centred at. This should be the centre of the histogram/data; how spread out or narrow the normal curve should be. This should be the spread of the histogram, i.e. the standard deviation of the data. Centre: sample_mean &lt;- cognitive %&gt;% pull(CognitiveScore) %&gt;% mean() sample_mean ## [1] 99.9019 Spread: sample_sd &lt;- cognitive %&gt;% pull(CognitiveScore) %&gt;% sd() sample_sd ## [1] 15.18534 Hence the normal curve should be centred at the mean of the data, 99.9 and have standard deviation 15.19. The equation of the normal density curve is provided by the following function: dnorm(x, mean, sd) where: x represents the value of the random variable mean the mean of the normal curve sd the standard deviation of the curve Let’s add the red curve on top of the histogram using stat_function(). It takes as arguments: fun: the function to plot args: parameters of the function to be plotted plotting specifications ggplot(cognitive) + geom_histogram(aes(x = CognitiveScore, y = stat(density)), color = &#39;white&#39;) + stat_function(fun = dnorm, args = list(mean = sample_mean, sd = sample_sd), color = &#39;red&#39;, size = 2) We can now calculate the probability that a randomly selected individual has a cognitive score between 120 and 130 as the area under the normal curve between 120 and 130: We compute areas for normal curves using the function pnorm(x, mean, sd). This function computes the area to the left of x in a normal curve centred at mean and having standard deviation sd. 1. Area to the left of a value \\(x\\) 2. Area between the values \\(x_l\\) and \\(x_u\\) 3. Area to the right of \\(x\\) The probability that a randomly selected individual has a cognitive score between 120 and 130 is the area under the normal curve between 120 and 130: theory_prob &lt;- pnorm(130, mean = sample_mean, sd = sample_sd) - pnorm(120, mean = sample_mean, sd = sample_sd) theory_prob ## [1] 0.06909445 Let’s compare that with the proportion obtained from the histogram: sample_prob &lt;- cognitive %&gt;% summarise(prop = sum(CognitiveScore &gt;= 120 &amp; CognitiveScore &lt;= 130) / n()) sample_prob ## # A tibble: 1 x 1 ## prop ## &lt;dbl&gt; ## 1 0.0705 As we can see the theoretical probability and the sample estimate, rounded to two decimal places, are similar: Simulation-based Theory-based 0.07 0.07 There is a 7% chance that a randomly selected individual from the population has a cognitive score between 120 and 130. To summarise, the calculation of probabilities for normally distributed random variables follows three steps: find the mean of the variable, find the standard deviation of the variable, compute the probability as the area under the normal curve with the mean found in (1) and the standard deviation found in (2). 15.2.4 Z-scores Instead of always specifying the mean and the standard deviation of the normal curve, we can work with a reference normal curve that has mean = 0 and sd = 1. This is known as the standard normal distribution. Standard normal distribution A standard normal distribution, denoted \\(N(0, 1)\\), is a normal distribution with mean = 0 and sd = 1. The functions dnorm and pnorm assume that the mean = 0 and the sd = 1 if not provided, i.e. they assume a standard normal distribution by default. In order to transform a value \\(x\\) from a normal distribution with mean = \\(\\mu\\) and sd = \\(\\sigma\\) to a score on the standard normal scale, we must use the z-score transformation: \\[ z = \\frac{x - \\mu}{\\sigma} \\] Let’s compute again the probability that a randomly selected individual has a cognitive score between 120 and 130 using a standard normal distribution. First, we must convert the values to z-scores by subtracting the mean and the standard deviation of the distribution. Then we use the function pnorm(): z_u &lt;- (130 - sample_mean) / sample_sd z_l &lt;- (120 - sample_mean) / sample_sd pnorm(z_u) - pnorm(z_l) ## [1] 0.06909445 There is a 7% chance that a randomly selected individual from the population has a cognitive score between 120 and 130. As you can see, this is exactly the same value we found before with the shifted and scaled normal distribution. 15.3 Summary When a histogram shows that a variable’s distribution is symmetric and bell-shaped, we can say that the variable is normally distributed and we can model the distribution with a mathematical curve called the normal probability distribution. We saw that the normal distribution depends on two parameters that control the centre and spread of the normal curve: the mean; the standard deviation. These two parameters let the normal curve model histograms that have different centres and different spreads. In R, the equation of the normal density curve is given by the function dnorm(x, mean, sd). We can use the normal curve to compute the probability of intervals as the area under the curve in that interval. The function to compute the area under a normal curve to the left of x is pnorm(x, mean, sd). Probability R command Probability of observing a value less than or equal x pnorm(x, mean, sd) Probability of observing a value between xl and xu pnorm(xu, mean, sd) - pnorm(xl, mean, sd) Probability of observing a value greater than xu 1 - pnorm(xu, mean, sd) 15.4 Lab In today’s lab we will fit a normal curve to the sampling distribution of the mean using the NFL example introduced in Week 11. Consider again the dataset with the yearly salaries of National Football League (NFL) players. We are interested in the average yearly salary of a NFL player. We have data on 2,099 players: their name, position, team they played for, total money while on a NFL payroll, and yearly salary. Since we are only interested in the yearly salary, we can select the relevant columns: # Step 1: load the required libraries library(tidyverse) library(moderndive) # Step 2: load the data and select the columns of interest nfl &lt;- read_tsv(&#39;https://edin.ac/2TexAFA&#39;) # Step 3: look at the first rows of the data head(nfl) ## # A tibble: 6 x 5 ## Player Position Team TotalMoney YearlySalary ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aaron Rodgers QB Packers 110 22 ## 2 Russell Wilson QB Seahawks 87.6 21.9 ## 3 Ben Roethlisberger QB Steelers 87.4 21.8 ## 4 Philip Rivers QB Chargers 83.2 20.8 ## 5 Cam Newton QB Panthers 104. 20.8 ## 6 Matt Ryan QB Falcons 104. 20.8 # Step 4: check the dimensions of the tibble dim(nfl) ## [1] 2099 5 # Step 5: select relevant variables nfl &lt;- nfl %&gt;% select(Player, YearlySalary) ► Question 1 Compute the sampling distribution of the mean using 10,000 samples of 100 players each. Display the sampling distribution using a density histogram. Hint. Remember to use y = stat(density) inside the function aes(). ► Solution Let’s compute the sampling distribution of the mean for samples of size \\(n = 100\\), using 10,000 samples: # Step 1: obtain many samples of size 100 and compute the mean of each sample sampling_distrib_of_mean &lt;- nfl %&gt;% rep_sample_n(size = 100, reps = 10000) %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) # Step 2: plot the sampling distribution of the mean for samples of size 100 ggplot(sampling_distrib_of_mean) + geom_histogram(aes(x = avg, y = stat(density)), color = &quot;white&quot;) + labs(x = expr(bar(x))) Figure 15.6: Sampling distribution of the mean. ► Question 2 Does the sampling distribution of the mean for samples of size 100 follow a normal curve? ► Solution The distribution of sample means seems to be symmetric and bell-shaped, hence the sample mean is a normally distributed random variable. ► Question 3 Find the normal probability density function that models the sampling distribution of the mean for samples of size \\(n = 100\\). Plot the normal curve in red on top of the histogram. ► Solution The required steps are: Compute the mean of the sample means, which is used to specify where the normal curve should be centred at; Compute the standard deviation of the sample means (SE of the mean) which is used to specify the spread of the normal curve. # Sample mean mean_of_distrib &lt;- sampling_distrib_of_mean %&gt;% pull(avg) %&gt;% mean() mean_of_distrib ## [1] 2.240886 # Sample sd sd_of_distrib &lt;- sampling_distrib_of_mean %&gt;% pull(avg) %&gt;% sd() sd_of_distrib ## [1] 0.297133 # Plot the histogram of the sampling distribution and the fitted normal curve ggplot(sampling_distrib_of_mean) + geom_histogram(aes(x = avg, y = stat(density)), color = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean_of_distrib, sd = sd_of_distrib), color = &#39;red&#39;, size = 2) + labs(x = expr(bar(x))) ► Question 4 What are the population mean and standard deviation? ► Solution mu &lt;- nfl %&gt;% pull(YearlySalary) %&gt;% mean() mu ## [1] 2.238363 sigma &lt;- nfl %&gt;% pull(YearlySalary) %&gt;% sd() sigma ## [1] 3.066385 The population mean is \\(\\mu =\\) 2.24. The population standard deviation is \\(\\sigma =\\) 3.07. ► Question 5 What are the mean and the standard deviation of the sampling distribution of the mean? Reminder: Remember that the standard deviation of the sampling distribution of the mean is also known as the standard error (SE) of the mean. ► Solution We have already calculated them to find the centre and spread of the normal curve: mean_of_distrib ## [1] 2.240886 sd_of_distrib ## [1] 0.297133 The distribution of sample means has mean 2.24. The distribution of sample means has standard deviation 0.3. ► Question 6 What is the relation between the population mean and the mean of the sampling distribution? ► Solution The sampling distribution of the mean is centred at the population mean, \\(\\mu =\\) 2.24. In other words, the average of the sampling distribution of the mean is equal to the population mean. ► Question 7 Do you notice any relation between the population standard deviation and the standard error of the mean? Hint. If the population standard deviation is \\(\\sigma\\), the sampling distribution of the mean should have standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\), where \\(n\\) is the sample size. ► Solution Population standard deviation: \\(\\sigma =\\) 3.07 Standard error of the mean: \\(SE(\\bar{x}) =\\) 0.3 It looks like the SE of the mean is the population standard deviation divided by 10. However, recall that the sample size was 100, so \\(10 = \\sqrt{100}\\). In general, assume that the population has mean \\(\\mu\\) and standard deviation \\(\\sigma\\). It is possible to prove that the distribution of the mean, computed on samples of size \\(n\\), has mean equal to the population mean, \\(\\mu\\), and standard error: \\[SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\] # Simulation-based standard error sd_of_distrib ## [1] 0.297133 # Theory-based standard error sigma / sqrt(100) ## [1] 0.3066385 ► Question 8 Comparison of the simulation-based and theory-based sampling distribution of the mean. Write down on paper the table below and fill the ? with the mean and the standard error computed using the simulation-based approach and the theory-based approach. The sampling distribution of the mean is a normal distribution with mean and standard deviation: Simulation-based approach Theory-based approach Mean ? \\(\\mu =\\) ? SE ? \\(\\frac{\\sigma}{\\sqrt{n}} =\\) ? ► Solution Simulation-based approach Theory-based approach Mean 2.24 \\(\\mu =\\) 2.24 SE 0.3 \\(\\frac{\\sigma}{\\sqrt{n}} =\\) 0.31 The simulation-based approach and the theory-based approach lead to very similar results. This is an important result. It means that, in order to show the sampling distribution of the mean, we would just: load the data, find the population mean \\(\\mu\\), find the population sd \\(\\sigma\\), plot a normal curve with mean = \\(\\mu\\) and sd = \\(\\frac{\\sigma}{\\sqrt{n}}\\). This avoids us the burden of: repeatedly sampling from the population; calculating the sample mean for each sample; plotting the histogram of the sample means. ► Question 9 The lengths of human pregnancies are normally distributed with mean \\(\\mu = 266\\) days and standard deviation \\(\\sigma = 16\\) days. Plot the normal curve representing the length of human pregnancies. Hint: This requires the following steps: Find the population mean and standard deviation. Create a grid of values for the length of pregnancy variable. Recall from Week 12 that 99.7% of all values lie between the mean plus or minus three standard deviations. To be sure to capture all of the values, use the mean plus or minus four standard deviations as limits. Compute the normal curve for the grid of values in (2). Plot using geom_line(). ► Solution First we need to create a grid of days for the plot. Remember from Week 12 that 99.7% of all values lie between the mean plus or minus three standard deviations. To capture all of the values, we will use four times the standard deviation: mu &lt;- 266 sigma &lt;- 16 x &lt;- seq(mu - 4 * sigma, mu + 4 * sigma, by = 0.1) y &lt;- dnorm(x, mu, sigma) df &lt;- tibble(x, y) ggplot(df, aes(x = x, y = y)) + geom_line(color = &#39;red&#39;, size = 2) + labs(x = &#39;length of pregnancy (in days)&#39;, y = &#39;normal density&#39;) ► Question 10 What is the proportion of the population with length of pregnancy greater than 280 days? ► Solution 1 - pnorm(280, mean = mu, sd = sigma) ## [1] 0.190787 The proportion of the population with length of pregnancy greater than 280 days is 19%. Another way to say this is, the probability that a randomly selected person from the population will have a length of pregnancy greater than 280 days is 0.19. ► Question 11 What is the proportion of the population with length of pregnancy between 230 and 260 days? ► Solution pnorm(260, mean = mu, sd = sigma) - pnorm(230, mean = mu, sd = sigma) ## [1] 0.3416058 The proportion of the population with length of pregnancy between 230 and 260 days is 34%. Another way to say this is, the probability that a randomly selected person from the population will have a length of pregnancy between 230 and 260 days is 0.34. ► Question 12 Using the theory-based approach, write down: the mean of the sampling distribution of the average length of pregnancy using samples of size 50; the standard error of the mean. Plot the sampling distribution of the mean for samples of size 50. Hint: Remember to create a grid of values going from mean - 4 * SE to mean + 4 * SE. ► Solution n &lt;- 50 SE &lt;- sigma / sqrt(n) SE ## [1] 2.262742 the sampling distribution is centred at the population mean, \\(\\mu = 266\\); the standard error of the mean is \\(SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}} = \\frac{16}{\\sqrt{50}} = 2.26\\). Let’s plot the sampling distribution of the mean: x &lt;- seq(mu - 4 * SE, mu + 4 * SE, by = 0.1) y &lt;- dnorm(x, mu, SE) df &lt;- tibble(x, y) ggplot(df, aes(x = x, y = y)) + geom_line(color = &#39;red&#39;, size = 2) + labs(x = expr(bar(x)), y = &#39;normal density&#39;) 15.5 Glossary Normal curve. A density curve used to model symmetric and bell-shaped distributions of continuous random variables. Distribution of the sample mean. The sample mean is a random variable which is normally distributed. Its mean is \\(\\mu\\) and its standard deviation (standard error of the mean) is \\(\\frac{\\sigma}{\\sqrt{n}}\\). Simulation-based SE of the mean. The standard deviation of the sampling distribution of the mean, obtained by generating many random samples and calculating the mean for each sample. Theory-based SE of the mean. For a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the SE of the mean computed on samples of size \\(n\\) is \\(SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\). Z-score. The transformation \\(\\frac{x - \\mu}{\\sigma}\\). It translates a value from a \\(N(\\mu, \\sigma)\\) scale to a \\(N(0,1)\\) scale. "],
["chap-one-mean-test.html", "Chapter 16 One Sample Mean Test 16.1 Recap 16.2 Where we are now 16.3 Where we are going 16.4 One sample mean test 16.5 Walkthrough - Tempo of rock music on Spotify ALL IN ONE LINE OF CODE! Assumptions 16.6 Summary 16.7 Lab", " Chapter 16 One Sample Mean Test Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand the t-test for one mean. LO2. Understand the t distribution and the concept of degrees of freedom. LO3. Understand how to assess normality. 16.1 Recap Over the last 5 weeks, we have started to look at how we can use a sample from a population to draw inferences about the population. Some key things which we have learned: Week 11 Distinction between population and sample. Use of sample statistics to draw infereces about population parameters. Sampling variability leads a statistic to vary from sample to sample. The sampling distribution of a statistic is the distribution of the values that a statistic takes for all possible samples of the same size from the same population. To quantify the accuracy of our statistic, we calculate the standard deviation of its sampling distribution. This we call the standard error of the statistic. Week 12 In practice we cannot take lots and lots of samples in order to build up a picture of the sampling distribution of a statistic. We can approximate this process by bootstrap resampling our original sample (repeated random sampling with replacement from the original sample, using the same sample size.). Weeks 13 and 14 Introduced hypothesis testing, statistical significance and statistical power. These concepts relied on us having some measure of sampling variability. In order to test hypotheses or construct confidence intervals, we simulated sampling distributions under different hypothetical parameters, against which we could then compare an observed statistic. We also introduced the concept of a standardised statistic, typically denoted by \\(z\\). This measures how many standard deviations away from the mean of the null distribution the observed statistic is. \\(\\textrm{standardised statistic} = z = \\frac{\\textrm{statistic} - \\textrm{mean of null distribution}}{\\textrm{standard deviation of null distribution}}\\) Week 15 Things got a bit abstract. Constructing a sampling distribution ourselves requires simulate 1000’s of samples (only possible with a computer). We can also work with a theoretical sampling distribution. If we assume that the sampling distribution is normal (symmetric and bell-shaped), we can approximate the standard error using a formula: \\(SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\) 16.2 Where we are now Two ways of estimating the standard error 16.3 Where we are going For the next couple of weeks, we are going to learn about some specific statistical tests we can perform, when we would use them, how to calculate them, and what assumptions we make when we use them. We are going to focus on the theoretical approach, and use the formula for the standard error, rather than bootstrapping. This week, we’re going to look at the one sample mean test. 16.4 One sample mean test The “one sample mean test” does pretty much what it says - if you have one sample and you have a mean, you can perform a statistical test in order to evaluate how likely it is that the population mean (which the sample mean is your estimate of) is equal to a specific value. Example questions Questions which can be answered by a one sample mean test often take the form: Is the average weight of a dog greater than 20kg? Is the mean body temperature not equal to 37 degrees C? On the Beck Depression Inventory (BDI), a score of &gt;25 is considered clinical diagnosis of depression. Is the average score of our population of interest (for instance, people with a specific disease) significantly above this cutoff? Hypotheses Null hypothesis: The population mean (\\(\\mu_1\\)) is equal to some pre-specified number (\\(\\mu_{0}\\)). \\(H_0: \\mu_1 = \\mu_{0}\\) (Note that this is the same as \\(\\mu_1 - \\mu_{0} = 0\\)) Alternative hypothesis: The population mean (\\(\\mu_1\\)) is not equal to/is less than/is greater than some pre-specified number (\\(\\mu_{0}\\)). \\(H_1: \\mu_1 \\neq \\mu_{0}\\) \\(H_1: \\mu_1 &gt; \\mu_{0}\\) \\(H_1: \\mu_1 &lt; \\mu_{0}\\) Formula Standardised statistic \\((z)\\) When we introduced the notion of a standardised statistic in Week 13, we had simulated the null sampling distribution by constructing repeated random samples and calculating a statistic for each one. This allowed us to see how these statistics would vary due to random sampling, assuming the null hypothesis to be true. We could quantify this directly by calculating the standard deviation of the simulated null distribution - the standard error. Reminder: The standard deviation of the sampling distribution of a statistic (e.g. the mean) is also known as the standard error (SE) of the statistic. Using our theoretical approach to approximating the standard error, we can calculate the standard error of the statistic, for a sample of size \\(n\\) using the formula: \\(SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\). Including this in our formula for the standardised statistic becomes: \\[ z = \\frac{\\textrm{statistic} - \\textrm{mean of null distribution}}{\\textrm{standard deviation of null distribution}} = \\frac{\\textrm{statistic} - \\textrm{mean of null distribution}}{\\sigma/\\sqrt{n}} = \\frac{\\bar{x} - \\mu_{0}}{\\sigma/\\sqrt{n}} \\] Standardised statistic (\\(t\\)) In the formula for \\(z\\) above, \\(\\sigma\\) denotes the population standard deviation. But to calculate this we need to have data on the whole population. When the population standard deviation (\\(\\sigma\\)) is unknown, we estimate it using the sample standard deviation (\\(s\\)): \\[ t = \\frac{\\bar{x} - \\mu_{0}}{s/\\sqrt{n}} \\] Note that when we use the sample standard deviation \\(s\\) in this formula, the standardised statistic we calculate gets denoted as \\(t\\), rather than \\(z\\). If we were to draw multiple random samples of the same size from the same population, and perform the same calculation of a \\(t\\)-value for each sample, then the values we would obtain would follow a \\(t\\)-distribution. \\(t\\)-distributions The particular shape of the \\(t\\)-distribution is determined by the degrees of freedom. By ‘degrees of freedom’ we refer to the number of independent observations in a set of data. When we are estimating a mean from a single sample, the degrees of freedom is equal to the sample size minus one. This means that the sampling distribution of \\(t\\)-statistics from samples of size 10, would follow a \\(t\\)-distribution with \\(10-1\\) degrees of freedom. Degrees of freedom (df) ► Question Suppose we have four unkown numbers (\\(a\\), \\(b\\), \\(c\\) and \\(d\\)) which must have a mean of 5. Do the following, in order: Choose a value for \\(a\\). Choose a value for \\(b\\). Choose a value for \\(c\\). Can you choose a value for \\(d\\) while ensuring the mean of the four numbers you have chosen is 5? ► Solution You a free to choose anything you like for \\(a\\), \\(b\\) and \\(c\\). But once those are fixed, you have no freedom to choose \\(d\\). Example: \\(a\\) = 1 \\(b\\) = 2 \\(c\\) = 3 We know that \\(\\frac{1+2+3+d}{4} = 5\\) So there is only one possible value for \\(d\\): \\(\\frac{1+2+3+d}{4} = 5\\) \\(1+2+3+d = 5*4\\) \\(1+2+3+d = 20\\) \\(d = 20-3-2-1\\) \\(d = 14\\) You can see the \\(t\\)-distribution for different degrees of freedom below. Notice that as the degrees of freedom (\\(\\nu\\) in the plot below) gets bigger (so as \\(n\\) gets bigger), the more the \\(t\\)-distibution fits a normal distribution. (Source: https://en.wikipedia.org/wiki/Student%27s_t-distribution) Critical values &amp; significance In order to test the significance of a given \\(t\\)-statistic, we therefore need to assess the probability of obtaining our \\(t\\)-statistic (or one at least as extreme) against a \\(t\\)-distribution with degrees of freedom \\(n-1\\). We can do this in R using the pt() function with pt(x, df). Remember that last week we used the function pnorm(x, mean, sd) to compute the area to the left of x in a normal curve centred at mean and having standard deviation sd. Similarly, pt(x, df) computes the area to the left of x in a \\(t\\)-distribution curve with degrees of freedom df. ► Question Looking at the plot above, for a \\(t\\)-distribution with degrees of freedom of 5 (the blue line), what proportion of the curve is to the left of -2? ► Solution pt(-2, df = 5) ## [1] 0.05096974 From this, we can say that assuming the null hypothesis to be true, the probability of obtaining a \\(t\\)-statistic with 5 degrees of freedom of \\(\\leq -2\\) is 0.051. We can also find the critical values of a \\(t\\)-distribution using the function qt(p, df). This will return the values of \\(t\\) for which \\(p\\)% of the distribution lies to the left. This way, we can find the values of \\(t\\) at which we will reject the null hypothesis (for a given \\(\\alpha\\) level). ► Question At what value of \\(t\\) does 5% of the \\(t\\)-distribution with 5 degrees of freedom lie to the left? At what values of \\(t\\) do 5% of the \\(t\\)-distribution with 5 degrees of freedom lie in either tail? ► Solution qt(.05, df = 5) ## [1] -2.015048 If we perform a one-tailed test of \\(\\mu_1 &lt; \\mu_{0}\\) on a sample of 6 (so our degrees of freedom is 5), we will reject the null hypothesis (\\(\\mu_1 = \\mu_{0}\\)) if our corresponding \\(t\\)-statistic is \\(\\leq -2.015\\). qt(.025, df = 5) ## [1] -2.570582 qt(.975, df = 5) ## [1] 2.570582 # remember that the t-distribution is symmetric and centred on 0! If we perform a two-tailed test of \\(\\mu_1 \\neq \\mu_{0}\\) on a sample of 6, we will reject the null hypothesis (\\(\\mu_1 = \\mu_{0}\\)) if the absolute magnitude of our corresponding \\(t\\)-statistic is \\(\\geq 2.571\\). 16.5 Walkthrough - Tempo of rock music on Spotify Research Question Is the average tempo of music classified as “rock” by Spotify greater than 125 beats per minute (BPM). ► Step 1 - Hypotheses Write out the null and alternative hypotheses, and specify our \\(\\alpha\\) level. ► Solution Null hypothesis (\\(H_0\\)): \\(\\mu_1 = 125\\) Alternative hypothesis (\\(H_1\\)): \\(\\mu_1 &gt; 125\\) We will use \\(\\alpha = 0.05\\) ► Step 2 - Data Get the data! Our hypothesis is about \\(\\mu_1\\), the mean of all rock songs on Spotify. To investigate this, we have a sample of 200 rock songs from Spotify, available at https://edin.ac/2wC5rz3 ► Solution spotify_songs &lt;- read_csv(&quot;https://edin.ac/2wC5rz3&quot;) summary(spotify_songs) ## track_id track_name track_artist playlist_genre track_popularity ## Length:200 Length:200 Length:200 Length:200 Min. : 0.00 ## Class :character Class :character Class :character Class :character 1st Qu.:19.75 ## Mode :character Mode :character Mode :character Mode :character Median :48.00 ## Mean :41.67 ## 3rd Qu.:62.00 ## Max. :85.00 ## tempo duration_ms ## Min. : 74.07 Min. :139600 ## 1st Qu.:110.78 1st Qu.:204730 ## Median :125.41 Median :242027 ## Mean :128.94 Mean :256804 ## 3rd Qu.:146.88 3rd Qu.:289960 ## Max. :200.08 Max. :510933 ► Step 3 - Sample mean Which line of code will give us our sample statistic? spotify_songs %&gt;% group_by(tempo) %&gt;% summarise( xbar = mean(rock) ) spotify_songs %&gt;% summarise( xbar = mean(tempo) ) spotify_songs %&gt;% group_by(rock) %&gt;% count(tempo) ► Solution Note that the entire sample contains only rock songs, so we can simply find the mean of the tempo variable. spotify_songs %&gt;% summarise( xbar = mean(tempo) ) ## # A tibble: 1 x 1 ## xbar ## &lt;dbl&gt; ## 1 129. ► Step 4 - Sample sd Do we know the population standard deviation (\\(\\sigma\\))? That is, do we know the standard deviation of tempos of all rock songs on Spotify? No, so we estimate it with the sample standard deviation (\\(s\\)). Calculate this now. ► Solution spotify_songs %&gt;% summarise( s = sd(tempo) ) ## # A tibble: 1 x 1 ## s ## &lt;dbl&gt; ## 1 26.5 ► Step 5 - Calculate t Fill in the blanks: \\[ \\begin{aligned} \\bar{x} &amp;= \\ ? \\\\ \\mu_{0} &amp;= \\ ? \\\\ s &amp;= \\ ? \\\\ n &amp;= \\ ? \\\\ SE(\\bar{x}) &amp;= \\ ? \\end{aligned} \\] and write out the \\(t\\)-statistic: \\[ t_{obs} \\qquad = \\qquad \\frac{? \\qquad - \\qquad ?}{?} \\] Finally, calculate your \\(t\\)-statistic in R. ► Solution Fill in the blanks: \\[ \\begin{aligned} \\bar{x} &amp;= \\ 128.9\\\\ \\mu_{0} &amp;= \\ 125\\\\ s &amp;= \\ 26.5\\\\ n &amp;= \\ 200\\\\ SE(\\bar{x}) &amp;= 26.5 / \\sqrt{200} \\end{aligned} \\] and write out the \\(t\\)-statistic: \\[ t_{obs} = \\frac{128.9 - 125}{26.5 / \\sqrt{200}} \\] We can do this quickly using R just like a calculator: t_obs = (128.9 - 125) / (26.5 / sqrt(200)) t_obs ## [1] 2.081295 ► New R stuff! New R stuff! Going back to the start, we can use our skills with summarise() to calculate all the terms we need for our \\(t\\)-statistic: terms &lt;- spotify_songs %&gt;% summarise( xbar = mean(tempo), s = sd(tempo), mu_0 = 125, n = n() ) terms ## # A tibble: 1 x 4 ## xbar s mu_0 n ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 129. 26.5 125 200 And then we can plug in these numbers to our equation. We now introduce a new technique: the $ operator. This is similar to the pull() function we previously used, as it pulls out the column from a tibble. (terms$xbar - terms$mu_0) / (terms$s / sqrt(terms$n)) ## [1] 2.099549 ► Step 6 - Critical t value Using the qt() function, calculate the critical value for \\(\\alpha\\) = 0.05. This is the smallest absolute value of \\(t\\) at which you will reject the null hypothesis. You’ll need to work out the degrees of freedom You’ll also need to think about whether we are performing a two-tailed test or a one-tailed test. If a two-tailed test, then remember that the \\(\\alpha\\) is split between the two tails (and so we would reject anything in the most extreme 2.5%) ► Solution The degrees of freedom are \\(n-1 = 200-1 = 199\\) We’re performing a one-tailed test here because our alternative hypothesis (\\(H_1\\)) is that the mean tempo is &gt; 125. So we will reject a \\(t\\)-statistic which falls in the upper 5% of the distribution. qt(.95, df = 199) # 5% to the right ## [1] 1.652547 We will reject a \\(t\\)-statistic which is greater (in magnitude) than 1.65. ► Step 7 - Probability of observed t-statistic We have our observed \\(t\\)-statistic of 2.1. We know that this is greater than the critical value of 1.65. What is the probability of obtaining a \\(t\\)-statistic at least as extreme as 2.1, assuming the null hypothesis to be true? In other words, what is the p-value? Things you’ll need to work out: Do we want the area under the curve which is to the left or to the right of \\(t = 2.1\\)? Remember that the total area under probability curves such as the normal and the \\(t\\) is always equal to 1, so area to the right = 1 - area to the left. ► Solution The degrees of freedom is \\(n-1 = 200-1 = 199\\) We want to look at the area to the right, because we are testing whether 128.9 (our sample mean) is significantly greater than 125 (our null hypothesis mean). pvalue = 1 - pt(2.1, df = 199) pvalue ## [1] 0.01849448 ALL IN ONE LINE OF CODE! Now that we’ve gone through all that, you’ll be happy to know that we can do all of what we just did above (and more!) using just one simple function in R, called t.test(). The t.test() function takes several arguments, but for the current purposes, we are interested in t.test(x, mu, alternative). x is the data mu is the hypothesized value of the mean in \\(H_0\\) alternative is either &quot;two.sided&quot; (default), &quot;less&quot;, or &quot;greater&quot;, and specifies the direction of the alternative hypothesis. Note again that we can use the pull() and %&gt;%: spotify_songs %&gt;% pull(tempo) %&gt;% t.test(mu = 125, alternative = &quot;greater&quot;) or we can use our new friend, the $ operator: t.test(x = spotify_songs$tempo, mu = 125, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: spotify_songs$tempo ## t = 2.0995, df = 199, p-value = 0.01851 ## alternative hypothesis: true mean is greater than 125 ## 95 percent confidence interval: ## 125.8378 Inf ## sample estimates: ## mean of x ## 128.9353 Hooray!! We can see this gives us the same results - it shows us the mean of our sample (128.9353), it writes out our alternative hypothesis for us, and returns our \\(t\\) value of 2.0995 and our p-value of 0.0185. Additionally, it even gives us some 95% confidence intervals for the population mean! Assumptions One last important thing to note is that when we perform a one sample mean tests, we assume a few basic things: The data are continuous (not discrete); The data are normally distributed OR the sample size is large enough (rule-of-thumb \\(n\\) = 20) and the data are not strongly skewed; The data are randomly sampled from a population. If any of these assumptions are not met, the results of the test are unreliable. We can assess whether a set of numbers are normally distributed (assumption 2, above) by: Producing an appropriate plot Plots such as the histogram, or density plots, showing us the shape of our distribution: ggplot(spotify_songs, aes(x=tempo))+ geom_histogram()+ labs(title=&quot;Histogram&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(spotify_songs, aes(x=tempo))+ geom_density()+ labs(title=&quot;Density plot&quot;) We can also use a plot called a QQplot (Quantile-Quantile plot), which orders the data and plots it against the equivalent quantile of the normal distribution: ggplot(spotify_songs, aes(sample = tempo))+ geom_qq()+ stat_qq_line()+ labs(title=&quot;QQplot&quot;, subtitle=&quot;The closer the data fit to the line the more normally distributed they are.&quot;) Conducting a test of normality We can also conduct a formal hypothesis test for normality, such as the Shapiro-Wilk test. The null hypothesis of the Shapiro-Wilk test is that the sample came from a population that is normally distributed. The alternative hypothesis is that the sample came from a population that is not normally distributed. The test returns a test statistic W, and a p-value. The p-value corresponds to the probability of observing data of this shape of distribution, assuming the data are drawn from a normally distributed population (i.e., assuming the null hypothesis to be true). In R: shapiro.test(spotify_songs$tempo) ## ## Shapiro-Wilk normality test ## ## data: spotify_songs$tempo ## W = 0.9868, p-value = 0.05945 The p-value here is 0.059, which is greater than \\(\\alpha = 0.05\\). We therefore fail to reject the null hypothesis of the Shapiro-Wilk test that the sample came from a population that is normally distributed. So our assumption for the one sample mean test holds! 16.6 Summary A standardised statistic is the relative location of a statistic to the mean of the null distribution, in terms of standard deviations. When we do not know the population standard deviation (\\(\\sigma\\)), we compute the standardised \\(t\\)-statistic using the formula \\(t = \\frac{\\bar{x} - \\mu_{0}}{s/\\sqrt{n}}\\). The sampling distribution of \\(t\\) follows a \\(t\\)-distribution. The shape of the \\(t\\)-distribution varies depending upon the degrees of freedom (the number of independent observations free to vary - for a one sample mean test, this is \\(n-1\\)). To assess the statistical significance of an observed \\(t\\)-statistic, we compute the critical values for a \\(t\\)-distribution with the appropriate degrees of freedom. If our observed \\(t\\) is farther away from 0 than our critical value, we have reason to reject the null. To calculate the p-value for a given \\(t\\), we compute the area of the \\(t\\)-distribution curve to the left/right of \\(t\\) (depending on whether our hypothesis is one or two-tailed). We need to make sure we assess whether our data violates the assumptions for our test (for instance by using shapiro.test() to test whether our data come from a normally distributed population). 16.7 Lab Please attempt the questions before looking at the solutions. Copy &amp; pasting the solutions will not help with learning! 16.7.1 Pets’ weights Data for a sample of 2000 licensed pets from the city of Seattle, USA, can be found at the following url: https://edin.ac/2VfPzg7. It contains information on the license numbers, issue date and zip-code, as well as data on the species, breeds and weights (in kg) of each pet. We are interested in whether the average weight of a dog is different from 20kg. Null hypothesis, \\(H_0: \\mu_1 = 20\\) Alternative hypothesis, \\(H_1: \\mu_1 \\neq 20\\) ► Question 1 Read in the data from the url using read_csv(). Make sure to assign it as an object, using [yourdataname] &lt;- read_csv(...) (choose your own name for the data). Use summary() to have a look at your data. Which variables are you going to need for our analysis? Does anything jump out as relevant? ► Solution pets &lt;- read_csv(&quot;https://edin.ac/2VfPzg7&quot;) ## Parsed with column specification: ## cols( ## license_issue_date = col_character(), ## license_number = col_character(), ## animals_name = col_character(), ## species = col_character(), ## primary_breed = col_character(), ## secondary_breed = col_character(), ## zip_code = col_character(), ## weight_kg = col_double() ## ) summary(pets) ## license_issue_date license_number animals_name species primary_breed ## Length:2000 Length:2000 Length:2000 Length:2000 Length:2000 ## Class :character Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## secondary_breed zip_code weight_kg ## Length:2000 Length:2000 Min. : 0.3941 ## Class :character Class :character 1st Qu.: 4.6970 ## Mode :character Mode :character Median : 16.3954 ## Mean : 15.2411 ## 3rd Qu.: 22.4816 ## Max. :103.4838 ## NA&#39;s :15 We’re going to need the weight_kg variable. Notice that there are some missing values (you can see that there are 15 NA’s). We will need to decide what to do with them. Also, there are some cats in our data as well as the dogs which we are interested in. There are even a couple of goats! We will want to get rid of them.. ► Question 2 Some of the 2000 pets are not the ones of interest (they aren’t dogs). Create a new dataset and call it dogs, which only has the dogs in it. ► Solution dogs &lt;- pets %&gt;% filter(species == &quot;Dog&quot;) ► Question 3 What does the following command appear to do, and how? dogs &lt;- dogs %&gt;% filter(!is.na(weight_kg)) Tip: look at the help documentation for is.na (search in the bottom right window of Rstudio, or type ?is.na) ► Solution It takes the dogs dataset, and it filters so that it will keep any rows where !is.na(weight_kg) is TRUE. The is.na(weight_kg) will be TRUE wherever weight_kg is an NA and FALSE otherwise. The ! before it flips the TRUEs and FALSEs, so that we have TRUE wherever weight_kg is a value other than NA, and FALSE if it is NA. You can read it as “keep all rows where there isn’t an NA in the weight_kg variable”. ► Question 4 Using summarise(), calculate \\(\\bar{x}\\), \\(s\\) and \\(n\\). What is \\(\\mu_{0}\\), and what are our degrees of freedom (\\(df\\))? ► Solution dogstats &lt;- dogs %&gt;% summarise( xbar = mean(weight_kg), s = sd(weight_kg), n = n() ) dogstats ## # A tibble: 1 x 3 ## xbar s n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 20.4 6.23 1333 \\(\\mu_{0}\\) is 20kg, and our degrees of freedom is \\(n-1\\), which is \\(1333-1 = 1332\\). ► Question 5 Calculate the standardised statistic \\(t\\), using $ to access the numbers you just calculated above. ► Solution t_obs = (dogstats$xbar - 20) / (dogstats$s / sqrt(dogstats$n)) t_obs ## [1] 2.202048 ► Question 6 Calculate the p-value using pt(). Our degrees of freedom are \\(n-1\\) Remember that the total area under a probability curve is equal to 1. pt() gives us the area to the left, but we want the area in the smaller tail (if \\(\\bar{x}\\) is greater than \\(\\mu_{0}\\), we want the area to the right of \\(t_{obs}\\) (see Week 15). Is our hypothesis one- or two-sided? If it is two-sided, what do we need to do to get our p-value? ► Solution Our sample statistic (\\(\\bar{x}\\) = 20.376kg) is greater than the hypothesised mean (\\(\\mu_{0}\\) = 20kg), so we want the area to the right. Reminder: For a probability distribution, the area under the curve to the right of x is 1 minus the area to the left. This is equivalent to saying that the probability of observing a value greater than x is 1 minus the probability of observing a value less than x. p_righttail = 1 - pt(t_obs, df = 1332) Because our alternative hypothesis is two-tailed (\\(H_1: \\mu_1 \\neq 20\\)), we will reject the null hypothesis for extreme \\(t\\)-statistics in either direction. So we are calculating the probability of observing a value at least as extreme in either direction, and must multiply the one tail by 2. p2tail = 2 * p_righttail p2tail ## [1] 0.02783276 ► Question 7 Finally, use the t.test() function. Check that the results match the ones you just calculated. ► Solution t.test(x = dogs$weight_kg, mu = 20, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: dogs$weight_kg ## t = 2.202, df = 1332, p-value = 0.02783 ## alternative hypothesis: true mean is not equal to 20 ## 95 percent confidence interval: ## 20.04104 20.71104 ## sample estimates: ## mean of x ## 20.37604 ► Look at all the funny names! For fun, take a look at some of the names of pets in the dataset. This is actually real data, apart from the weights_kg variable. You can see a humerous visualisation of some of the more unusual names for pets below: (Pet names in Seattle, from [@W_R_Chase](https://twitter.com/W_R_Chase)) 16.7.2 Procrastination scores The Procrastination Assessment Scale for Students (PASS) was designed to assess how individuals approach decision situations, specifically the tendency of individuals to postpone decisions (see Solomon &amp; Rothblum, 1984). The PASS assesses the prevalence of procrastination in six areas: writing a paper; studying for an exam; keeping up with reading; administrative tasks; attending meetings; and performing general tasks. For a measure of total endorsement of procrastination, responses to 18 questions (each measured on a 1-5 scale) are summed together, providing a single score for each participant (range 0 to 90). The mean score from Solomon &amp; Rothblum, 1984 was 33. Research Question Do Edinburgh University students report endorsing procrastination to less than the norm of 33? ► Question 8 Read in the data (a .csv is at https://edin.ac/2wJgYwL), produce some descriptive statistics, and conduct a one sample mean test to evaluate whether Edinburgh University students’ average score on the PASS is not equal to 33. Remember about the assumptions of your test! ► Solution pass_scores &lt;- read_csv(&quot;https://edin.ac/2wJgYwL&quot;) ## Parsed with column specification: ## cols( ## sid = col_character(), ## school = col_character(), ## PASS = col_double() ## ) shapiro.test(pass_scores$PASS) ## ## Shapiro-Wilk normality test ## ## data: pass_scores$PASS ## W = 0.93617, p-value = 0.2028 t.test(pass_scores$PASS, mu = 33, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: pass_scores$PASS ## t = -3.1073, df = 19, p-value = 0.0029 ## alternative hypothesis: true mean is less than 33 ## 95 percent confidence interval: ## -Inf 31.9799 ## sample estimates: ## mean of x ## 30.7 ► Question 9 Write up the results from Question 8. Hint: See the lecture slides! ► Solution A one-sided one-sample t-test was conducted in order to determine if the average score on the Procrastination Assessment Scale for Students (PASS) for a sample of 20 students at Edinburgh University was significantly (\\(\\alpha = .05\\)) lower than the average score obtained during development of the PASS. Edinburgh University students scored lower (Mean=30.7, SD=3.31) than the score reported by the authors of the PASS (Mean = 33). This difference was statistically significant (t(19)=-3.11, p &lt; .05, one-tailed). 16.7.3 Cat weights! ► Question 10 Without looking at the data(and without googling either), do you think that the average weight of a pet cat more than/less than/equal to 4.5kg? Write out your null and alternative hypotheses, and conduct the appropriate test. ► Solution cats &lt;- pets %&gt;% filter(species == &quot;Cat&quot;, !is.na(weight_kg)) shapiro.test(cats$weight_kg) ## ## Shapiro-Wilk normality test ## ## data: cats$weight_kg ## W = 0.99825, p-value = 0.7611 qqnorm(cats$weight_kg) t.test(cats$weight_kg, mu=4.5, alternative=&quot;greater&quot;) ## ## One Sample t-test ## ## data: cats$weight_kg ## t = -1.2444, df = 649, p-value = 0.8931 ## alternative hypothesis: true mean is greater than 4.5 ## 95 percent confidence interval: ## 4.463972 Inf ## sample estimates: ## mean of x ## 4.484496 16.7.4 Glossary Degrees of freedom. The number of independent observations in a set of data. Often the total number of datapoints (\\(n\\)) minus the number of parameters being estimated. One-sample t-test/One-sample mean test. Compare the mean in a sample to a known (or hypothesised) mean. Assumptions. Requirements of the data in order to ensure that our test is appropriate. Violation of assumptions changes the conclusion of the research and interpretation of the results. Shapiro-Wilks. Tests whether sample is drawn from a population which is normally distributed. QQplot/Quantile-Quantile plot. Displays the quantiles of the sample against the quantiles of a normal distribution. If the data points fall on the diagonal line, the sample is normally distributed. knitr::opts_chunk$set(fig.align = &#39;center&#39;, out.width = &#39;70%&#39;) "],
["two-indep-samples.html", "Chapter 17 Comparing two means (independent samples) 17.1 Recap 17.2 Key terminology 17.3 Walkthrough: Got a friend? 17.4 Lab: Does name increase tips? 17.5 Glossary 17.6 References", " Chapter 17 Comparing two means (independent samples) Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand how to perform a two-sample \\(t\\)-test and interpret the results. LO2. Understand how to calculate a two-sample \\(t\\)-interval and interpret the results. LO3. Being able to check the assumptions of \\(t\\)-procedures. 17.1 Recap Last week you explored how to draw conclusions about population parameters on the basis of sample statistics. In particular, given a random sample of size \\(n\\) from a population, you tested if the population mean was equal to some hypothesized value. Let the sample mean be \\(\\bar{x}\\). We test if the sample was taken from a population with mean \\(\\mu_0\\) by comparing the observed \\(t\\)-statistic \\[ t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\] with: for one-sided hypotheses, the critical value of level \\(\\alpha\\) from a \\(t\\)-distribution with \\(n-1\\) degrees of freedom \\[ t^*_\\textrm{df} = \\texttt{qt(p = 1 - alpha, df = n - 1)} \\] for two-sided hypotheses, the critical value of level \\(\\frac{\\alpha}{2}\\) from a \\(t\\)-distribution with \\(n-1\\) degrees of freedom \\[ t^*_\\textrm{df} = \\texttt{qt(p = 1 - alpha/2, df = n - 1)} \\] We reject the null hypothesis if the observed \\(t\\)-statistic is, in absolute value, as extreme or more extreme than the critical value: Reject \\(H_0\\) if \\(|t| \\geq t^*_\\textrm{df}\\) The above procedure applies when testing a single parameter (a proportion or a mean) from a single population. Today you will explore and apply inference procedures for comparing parameters between two populations or treatment groups. 17.2 Key terminology Units and variables The individual entities on which data are collected are called observational units or cases. The number of observational units in the study is known as the sample size, and is typically denoted by \\(n\\). A variable is any characteristic that varies from observational unit to observational unit. Categorical and quantitative variables Variables are either categorical or quantitative: A categorical variable divides the units into groups, placing each unit into exactly one of two or more categories. Technology tip: In R, a categorical variable should be a factor. A quantitative variable measures a numerical quantity for each case. Numerical operations like adding and averaging make sense only for quantitative variables. A special kind of categorical variable is a binary variable, for which only two possible categories exist. Note: One simple way to distinguish between categorical and quantitative variables is to ask yourself if it makes sense to take an average of the values. Explanatory and response variables If we are using one variable to help us understand or predict values of another variable, we call the former the explanatory variable and the latter the response variable. Other names explanatory variable = independent variable = predictor variable response variable = dependent variable = outcome variable Observational studies vs randomized experiments An observational study is a study in which the researcher does not manipulate the value of any variable, but simply observes the values as they naturally exist. A randomized experiment is a study in which the researcher determines at random the explanatory variable for each unit, before the response variable is measured. 17.3 Walkthrough: Got a friend? In today’s in-class activity we will use data from the General Social Survey conducted in the US in 2004. One of the questions asked to a random sample of adult Americans in the 2004 General Social Survey (GSS) was: “From time to time, most people discuss important matters with other people. Looking back over the last six months — who are the people with whom you discussed matters important to you? Just tell me their first names or initials.” The interviewer was asked to record how many names were mentioned, along with the person’s sex. For more details, see the GSS webpage. Note: This survey was conducted in the US in the year 2004, at which point US officials recorded sex on a binary scale. We are merely analysing the data that were collected in that survey as it is an extensive and open-source dataset. The fact that we are presenting this study as an in-class example is not an endorsement to the view that gender is a binary variable. How many names would you mention if you had to answer this question? How do you expect the responses to differ between men and women? Do you expect women to mention more names than men, or vice-versa, or perhaps the number of names to be similar between men and women? In today’s walkthrough, we will explore whether men and women differ with regard to the number of names they tend to mention when answering this question. For simplicity, we will refer to the people with whom you talk about important personal matters as “close friends”. The survey data are stored in the file close_friends.txt which has short url https://edin.ac/2vQrCBi. Before learning a new test procedure to assess a potential difference in the centre of a quantitative variable between two independent groups, it is good practice to display, explore and summarise the data. ► Question A.1 Identify the observational units in this study. ► Solution The observational units are the sampled adult Americans taking part in the 2004 General Social Survey (GSS). ► Question A.2 Identify the recorded variables in this study. Classify each variable either as categorical (also binary) or quantitative. Identify each variable’s role: explanatory or response. Variable’s name Type Role ? ? ? ? ? ? ► Solution The recorded variables are the sex of the participant and the number of names given by the participant. The former is categorical and binary, while the latter is a numerical value that varies from participant to participant. We are interested in how the number of mentioned names tends to vary with the sex of the participant. For this reason, sex is the explanatory variable, while number of close friends in the response variable. Variable’s name Type Role Sex Categorical and binary Explanatory Number of close friends Quantitative Response ► Question A.3 Did the study make use of random assignment, random sampling, both or neither? ► Solution This study only involved random sampling of the observational units from the population of adult Americans in 2004. ► Question A.4 Is this an observational study or an experiment? Explain why. ► Solution This is an observational study as the researchers limited themselves to record the values that naturally occur. The researchers did not perform any manipulation of the variables, such as random assignment of observational units to groups. If this were the case, we would be analysing data from a randomised experiment. ► Question A.5 State, in words, the null and alternative hypotheses to test whether the sample data provide evidence that American males and females tend to differ with regard to the average number of close friends they mention. ► Solution The null hypothesis is that the population mean number of close friends is the same for adult American males as for females. In other words, the null hypothesis states that there is no difference in the population mean number of close friends between males and females. The alternative hypothesis is that the population mean number of close friends is not the same for males as for females. In other words, the alternative hypothesis states that there is a difference in the population mean number of close friends between males and females. ► Question A.6 Define the parameters of interest in this study, and identify appropriate symbols for them. ► Solution \\(\\mu_f\\): population mean number of close friends mentioned by adult American females \\(\\mu_m\\): population mean number of close friends mentioned by adult American males ► Question A.7 State the null and alternative hypotheses in symbols. ► Solution \\[ H_0 : \\mu_f = \\mu_m \\qquad \\textrm{OR} \\qquad H_0 : \\mu_f - \\mu_m = 0\\\\ H_1 : \\mu_f \\neq \\mu_m \\qquad \\textrm{OR} \\qquad H_1 : \\mu_f - \\mu_m \\neq 0 \\] ► Question A.8 Explain why the one-sample \\(t\\)-test is not appropriate to answer the research question of this study. ► Solution The one-sample \\(t\\)-test introduced in Week 16 tests if the population that the observed sample came from has a hypothesized mean. Here, instead, we want to compare the means of two populations (or, if it were a randomized experiment, between two treatment groups). ► Question A.9 Load the data into R and inspect it. Pay particular attention to: the variable names; the dimensions of the tibble; the format of the data (i.e., make sure that variables are correctly encoded). ► Solution Load the data: library(tidyverse) gss &lt;- read_tsv(&#39;https://edin.ac/2vQrCBi&#39;, col_names = TRUE) Inspect the names of the variables and the first six rows: head(gss) ## # A tibble: 6 x 2 ## Sex NumberCloseFriends ## &lt;chr&gt; &lt;dbl&gt; ## 1 male 3 ## 2 male 0 ## 3 male 0 ## 4 male 0 ## 5 male 2 ## 6 male 2 Check the number of observational units and variables: dim(gss) ## [1] 1467 2 The tibble says that Sex is of class chr (character). As Sex is a categorical variable, we will encode it as a factor: gss &lt;- gss %&gt;% mutate(Sex = factor(Sex)) # check encoding head(gss) ## # A tibble: 6 x 2 ## Sex NumberCloseFriends ## &lt;fct&gt; &lt;dbl&gt; ## 1 male 3 ## 2 male 0 ## 3 male 0 ## 4 male 0 ## 5 male 2 ## 6 male 2 ► Question A.10 Summarise the survey responses by showing the counts of the number of close friends by sex. Try sketching by hand histograms showing, for each sex, the frequency of each reported number of close friends. ► Solution Create a two-way frequency table of the participants’ responses by sex: tally &lt;- xtabs(~ Sex + NumberCloseFriends, data = gss) tally ## NumberCloseFriends ## Sex 0 1 2 3 4 5 6 ## female 201 146 155 132 86 56 37 ## male 196 135 108 100 42 40 33 Add the totals for each sex: tally &lt;- addmargins(tally, margin = 2) tally ## NumberCloseFriends ## Sex 0 1 2 3 4 5 6 Sum ## female 201 146 155 132 86 56 37 813 ## male 196 135 108 100 42 40 33 654 Here, margin = 1 calculates the totals over the rows, while margin = 2 calculates the totals over the columns. The sketch should look something like this: ► Question A.11 Report a table of descriptive summaries by sex. Include the sample size, mean, SD, minimum, lower quartile, median, upper quartile, and maximum for each sex. ► Solution When comparing a quantitative response variable between two independent groups, encoded in a categorical variable, we could report: sample size mean SD minimum lower quartile median upper quartile maximum The median is the value such that 50% of the data lies below and 50% of the data lies above that value. The median cuts the data into two parts: the part to the left of the median and part to the right of the median. The median of left part is known as the lower quartile, and represents the value for which 25% of the data lie below that value. The median of right part is known as the upper quartile, and represents the value for which 25% of the data lie above that value. The interquartile range (IQR) is simply the difference between the upper quartile and the lower quartile, and represents the width of the interval containing the middle 50% of all observations. The minimum, lower quartile, median, upper quartile, and maximum jointly form the so-called five-number summary of the distribution of a quantitative variable. descr_stats &lt;- gss %&gt;% group_by(Sex) %&gt;% summarise( SampleSize = n(), Mean = mean(NumberCloseFriends), SD = sd(NumberCloseFriends), Minimum = min(NumberCloseFriends), LowerQuartile = quantile(NumberCloseFriends, p = 0.25), Median = median(NumberCloseFriends), UpperQuartile = quantile(NumberCloseFriends, p = 0.75), Maximum = max(NumberCloseFriends) ) descr_stats ## # A tibble: 2 x 9 ## Sex SampleSize Mean SD Minimum LowerQuartile Median UpperQuartile Maximum ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 813 2.09 1.76 0 1 2 3 6 ## 2 male 654 1.86 1.78 0 0 1 3 6 To format the above tibble as a nice HTML table, you can use the function kable from the package knitr: knitr::kable(descr_stats, &quot;html&quot;, digits = 2) Sex SampleSize Mean SD Minimum LowerQuartile Median UpperQuartile Maximum female 813 2.09 1.76 0 1 2 3 6 male 654 1.86 1.78 0 0 1 3 6 ► Question A.12 Are the descriptive summaries from the previous question parameters or statistics? Explain why. ► Solution The values in the above table are statistics. They are numerical summaries computed on observational units which represent a random sample from the population of adult Americans in 2004. ► Question A.13 Produce a visual display of the five-number summaries by sex. ► Solution A visual display of the five-number summary is called a boxplot. This is typically a very good visualisation when your data involve a quantitative response variable and a categorical explanatory variable. ggplot(gss, aes(x = Sex, y = NumberCloseFriends)) + geom_boxplot(color = &#39;darkorange&#39;) + theme_classic(base_size = 15) + coord_flip() ► Question A.14 Visualise the distribution of the number of close friends by sex. ► Solution Separate plots: ggplot(gss, aes(x = NumberCloseFriends, fill = Sex)) + geom_histogram(binwidth = 1, color = &#39;white&#39;) + facet_grid(cols = vars(Sex)) Unique plot: ggplot(gss, aes(x = NumberCloseFriends, fill = Sex)) + geom_histogram(binwidth = 0.5, color = &#39;white&#39;, position = &#39;dodge&#39;) ► Question A.15 Comment on what the histograms reveal about the shapes of the distributions. ► Solution The distribution of number of close friends for both males and females appears to be skewed to the right. The variability in the number of close friends seems to be similar across males and females. The sample mean number of close friends seems to be slightly higher for females than males. ► Question A.16 Report the sample mean and sample standard deviation of the number of close friends for each sex using appropriate symbols. ► Solution The sample mean number of close friends for females is \\(\\bar{x}_f\\) = 2.09, with standard deviation \\(s_f\\) = 1.76 friends. For males, the sample mean number of close friends is \\(\\bar{x}_m\\) = 1.86, with standard deviation \\(s_m\\) = 1.78 friends. ► Question A.17 Compute the difference in the sample mean number of close friends between females and males. Do you think it could be possible to obtain sample means this far apart even if the population means were actually equal? Explain why. ► Solution The difference in sample means is \\(\\bar{x}_f - \\bar{x}_m\\) = 0.23. Due to sampling variability, we can not conclude that, because the sample means differ, the means of the two populations must differ too. We must resort to a principled framework to test this, and we have already learned to use statistical hypothesis testing in order to assess if sample results (in our case, the observed difference in sample mean number of close friends) are significant in the sense of being unlikely to have occurred by chance (from random sampling) alone. Estimating the magnitude of the difference in population means. How can we estimate the magnitude of the difference in population means? By using a confidence interval for the difference in means! Comparing the means of two independent groups Suppose you wish to test for the significance of the difference between two population means denoted \\(\\mu_1\\) and \\(\\mu_2\\) and, if the difference is significant, you wish to quantify the magnitude of the difference using a confidence interval. Test of significance Null hypothesis: \\(H_0: \\mu_1 = \\mu_2\\) \\(\\quad\\) or \\(\\quad\\) \\(H_0: \\mu_1 - \\mu_2 = 0\\) Alternative hypothesis: \\(H_1: \\mu_1 &lt; \\mu_2\\) \\(\\quad\\) or \\(\\quad\\) \\(H_1: \\mu_1 - \\mu_2 &lt; 0\\) \\(H_1: \\mu_1 &gt; \\mu_2\\) \\(\\quad\\) or \\(\\quad\\) \\(H_1: \\mu_1 - \\mu_2 &gt; 0\\) \\(H_1: \\mu_1 \\neq \\mu_2\\) \\(\\quad\\) or \\(\\quad\\) \\(H_1: \\mu_1 - \\mu_2 \\neq 0\\) Test statistic: \\[ t = \\frac{\\bar x_1 - \\bar x_2}{SE(\\bar x_1 - \\bar x_2)} \\] \\(p\\)-value: \\(\\mathrm{Pr}(T_\\textrm{df} \\leq t)\\) \\(\\mathrm{Pr}(T_\\textrm{df} \\geq t)\\) \\(2 \\times \\mathrm{Pr}(T_\\textrm{df} \\geq |t|)\\) where \\(T_\\textrm{df}\\) denotes a \\(t\\)-distribution with \\(\\textrm{df}\\) degrees of freedom. SE and df: Population variances unknown and not equal: Very difficult degrees of freedom, let the R function t.test calculate them. The standard error of the difference in means is \\[ SE(\\bar x_1 - \\bar x_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}} \\] Population variances unknown and equal: Degrees of freedom \\(\\textrm{df} = (n_1 - 1) + (n_2 - 1) = n_1 + n_2 - 2\\). The standard error of the difference in means is \\[ \\qquad \\qquad SE(\\bar x_1 - \\bar x_2) = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}, \\quad \\textrm{where} \\quad s_p = \\sqrt{\\frac{(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 - 2}} \\] Confidence interval for \\(\\mu_1 - \\mu_2\\) \\[ (\\bar x_1 - \\bar x_2) \\pm t^*_\\textrm{df} \\times SE(\\bar x_1 - \\bar x_2) \\] where \\(t^*_\\textrm{df}\\) denotes the critical value corresponding to a desired \\(\\alpha\\) level for a \\(t\\)-distribution with \\(\\textrm{df}\\) degrees of freedom. Validity conditions (assumptions) These above procedures are considered valid if: The sample data arise from independent random samples from two populations OR from random assignment of the units to treatment groups. Either the quantitative variable of interest is normally distributed in both populations OR both sample sizes are large (as a convention, \\(n_1 \\geq 20\\) and \\(n_2 \\geq 20\\)) and the sample distributions should not be strongly skewed. ► Question A.18 Use the summary statistics computed in Question A.11 to calculate the value of the \\(t\\)-statistic for testing the hypotheses stated in Question A.7. ► Solution Let’s extract the relevant statistics from the table of descriptive summaries: n_f &lt;- descr_stats %&gt;% filter(Sex == &#39;female&#39;) %&gt;% pull(SampleSize) n_m &lt;- descr_stats %&gt;% filter(Sex == &#39;male&#39;) %&gt;% pull(SampleSize) xbar_f &lt;- descr_stats %&gt;% filter(Sex == &#39;female&#39;) %&gt;% pull(Mean) xbar_m &lt;- descr_stats %&gt;% filter(Sex == &#39;male&#39;) %&gt;% pull(Mean) s_f &lt;- descr_stats %&gt;% filter(Sex == &#39;female&#39;) %&gt;% pull(SD) s_m &lt;- descr_stats %&gt;% filter(Sex == &#39;male&#39;) %&gt;% pull(SD) Step 1. Can the population variances be assumed equal? Test the following hypotheses: \\[ H_0 : \\sigma_f^2 = \\sigma_m^2 \\\\ H_1 : \\sigma_f^2 \\neq \\sigma_m^2 \\] or, equivalently, \\[ H_0 : \\frac{\\sigma_f^2}{\\sigma_m^2} = 1 \\\\ H_1 : \\frac{\\sigma_f^2}{\\sigma_m^2} \\neq 1 \\] Use the F-test to test for equality of the population variances: var.test(NumberCloseFriends ~ Sex, data = gss) ## ## F test to compare two variances ## ## data: NumberCloseFriends by Sex ## F = 0.98094, num df = 812, denom df = 653, p-value = 0.7937 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.847352 1.134264 ## sample estimates: ## ratio of variances ## 0.9809412 At a significance level of 0.05, the \\(p\\)-value = 0.79 leads us to not rejecting the null hypothesis of equal variances across the two populations. Step 2. We can now perform the \\(t\\)-test calculations using the appropriate formula for the standard error of the difference in means. As the population variances are assumed equal, we use the formula involving the pooled standard deviation: # Pooled SD s_p &lt;- sqrt( ((n_f - 1) * s_f^2 + (n_m - 1) * s_m^2) / (n_f + n_m - 2) ) SE &lt;- s_p * sqrt(1/n_f + 1/n_m) t_stat &lt;- (xbar_f - xbar_m) / SE t_stat ## [1] 2.4523 ► Question A.19 Compare the \\(t\\)-statistic with the appropriate 5% critical value from a \\(t\\)-distribution. Compute the \\(p\\)-value and interpret the results. ► Solution Critical value: t_crit &lt;- qt(0.975, n_f + n_m - 2) t_crit ## [1] 1.961585 \\(p\\)-value: p_value &lt;- 2 * (1 - pt(abs(t_stat), n_f + n_m - 2)) p_value ## [1] 0.01431058 At a 5% significance level, the observed difference in mean number of close friends between adult American females and males is significantly different from 0 (\\(t(1465) = 2.45\\), \\(p\\)-value \\(&lt;.05\\), two-tailed). In other words, an observed difference in sample mean number of close friends of 0.23 is highly unlikely to occur by chance alone. The sample data provide very strong evidence that, on average, adult American females and males tend to not have the same number of close friends. Technology detour: two-sample \\(t\\)-test with built-in functions In R, we perform a two-sample \\(t\\)-test with the function t.test. This is the same function you saw to perform a one-sample mean test. Before applying it though, we need to check with var.test whether to assume the population variances to be different or equal. Both R functions require a formula as first argument: goal( y ~ x ) where y is the response variable x is the explanatory variable Step 1. Test for equality of the population variances: var.test(NumberCloseFriends ~ Sex, data = gss) Step 2. According to the previous test, use the appropriate \\(t\\)-test using either one of the following code chunks. If you can not reject the null hypothesis of equal population variances, use a \\(t\\)-distribution with \\(n_1 + n_2 - 2\\) degrees of freedom: t.test(NumberCloseFriends ~ Sex, data = gss, var.equal = TRUE) If the population variances were not equal, we would have used the Welch-Satterthwaite approximation to the degrees of freedom: # Welch-Satterthwaite approximation t.test(NumberCloseFriends ~ Sex, data = gss, var.equal = FALSE) # If not provided, var.equal = FALSE by default t.test(NumberCloseFriends ~ Sex, data = gss) ► Question A.20 Verify your results using the built-in R function t.test. Before applying it though, you need to check with var.test whether the population variances are different or can be assumed to be equal. ► Solution We have already tested before for equality of the population variances: var.test(NumberCloseFriends ~ Sex, data = gss) ## ## F test to compare two variances ## ## data: NumberCloseFriends by Sex ## F = 0.98094, num df = 812, denom df = 653, p-value = 0.7937 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.847352 1.134264 ## sample estimates: ## ratio of variances ## 0.9809412 As we can not reject the null hypothesis of equal variances across the two populations, we use a \\(t\\)-test with \\(\\textrm{df} = n_1 + n_2 - 2\\): t.test(NumberCloseFriends ~ Sex, data = gss, var.equal = TRUE) ## ## Two Sample t-test ## ## data: NumberCloseFriends by Sex ## t = 2.4523, df = 1465, p-value = 0.01431 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.04556467 0.40984457 ## sample estimates: ## mean in group female mean in group male ## 2.088561 1.860856 ► Question A.21 State the assumptions of the two-sample \\(t\\)-test. Should the strong skewness in the sample distributions cause you any concerns about the validity of your results? ► Solution Assumptions: The sample data arise from independent random samples from two populations OR from random assignment of the units to treatment groups. Either the quantitative variable of interest is normally distributed in both populations OR both sample sizes are large (as a convention, \\(n_1 \\geq 20\\) and \\(n_2 \\geq 20\\)) and the sample distributions should not be strongly skewed. Checks: The data were collected from a random sample of adult Americans. Even though the distribution of the number of close friends is clearly skewed, the sample sizes (813 and 654) are quite large. Hence, the conditions required for the two-sample \\(t\\)-test results to be valid are satisfied. ► Question A.22 Now that we have established that there is significant evidence of a difference in the population mean number of close friends between females and males, what is the magnitude of this difference in the population means? Construct and interpret a 95% confidence interval for the difference in population mean number of close friends between females and males. Pay particular attention on whether the interval is negative, positive, or contains zero. ► Solution In order to estimate the magnitude of the difference in the population means we can use a confidence interval for the difference in means. ci &lt;- tibble( Lower = (xbar_f - xbar_m) - t_crit * SE, Upper = (xbar_f - xbar_m) + t_crit * SE) ci ## # A tibble: 1 x 2 ## Lower Upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0456 0.410 A 95% confidence interval for the difference in the mean number of close friends between females and males is [0.046, 0.41]. The confidence interval is entirely positive, supporting our conclusion that females and males tend to differ with regard to the average number of close friends. We are 95% confident that American females have between 0.046 and 0.41 more close friends, on average, than American males do. ► Question A.23 Causation: Do the data provide evidence that how many close friends one has is caused by ones’ sex? Explain why. ► Solution No, we can not conclude that the person’s sex was responsible for the number of close friends. This data was collected as part of an observational study, hence the explanatory variable sex was simply observed in the observational units. ► Question A.24 Generalisation: To which population can the results of this study be applied to? Explain why. ► Solution Because the observational units are a random sample from the population of adult Americans in year 2004, we might apply our results to adult American men and women in that year. We might hesitate in generalising the results to all Americans and to a different year, as younger people were not included in the survey, and because the trend could have changed over time. 17.4 Lab: Does name increase tips? Can a waitress earn higher tips simply by introducing herself by name when greeting customers? How can she investigate this? After data are collected, how can she decide if the results provide convincing evidence that giving her name does really lead to higher tips? And if she decides that introducing herself by name really helps, how can she estimate how much higher will the tips be, on average, when introducing herself by name? Researchers Garrity and Degelman (1990) investigated the effect of a server introducing herself by name on restaurant tipping. The study involved forty, 2-person parties eating a $23.21 fixed-price buffet Sunday brunch at Charley Brown’s Restaurant in Huntington Beach, California, on April 10 and 17, 1988. Each two-person party was randomly assigned by the waitress to either a name or a no name introduction condition using a random mechanism. The waitress kept track of the two-person party condition and how much the party tipped at the end of the meal. The sample mean tip for the 20 parties in the name condition was \\(\\bar x_{name}= \\$5.44\\), with a standard deviation \\(s_{name} = \\$1.75\\). For the 20 parties in the no name condition, the sample mean tip was \\(\\bar x_{no\\ name}= \\$3.49\\), with a standard deviation \\(s_{no\\ name} = \\$1.13\\). ► Question B.1 Identify the observational units in this study. ► Solution The observational units are the 2-person parties eating Sunday brunch in that restaurant on April 10 and 17, 1988. We can also refer to the observational units as experimental units because, as we will see later, they are part of an experiment. ► Question B.2 Is this an observational study or a randomized experiment? Explain why. ► Solution This study is a randomized experiment. The waitress uses a random mechanism to assign the experimental units (the two-person parties) either to a name or no name condition. We also need to keep in mind that the waitress was not blind to which condition each party was assigned to and might have inadvertently provided better service to the parties she expected to give her a larger tip. ► Question B.3 What are the explanatory and response variables in this study? Classify them as either categorical (also binary) or quantitative. ► Solution Explanatory variable: condition (name or no name). \\(\\qquad\\) Type: categorical and binary. Response variable: tipping amount. \\(\\qquad \\qquad \\qquad \\qquad\\) Type: quantitative. ► Question B.4 State, in words and in symbols, the waitress’ null and alternative hypotheses. ► Solution The null hypothesis is that there is no effect on the tipping amount from the waitress giving her name as part of her greeting to the customers. Equivalently, the null hypothesis states that the population mean tip amount is the same whether the waitress introduces herself by name or not. The alternative hypothesis is that there is a positive effect on the tipping amount from the waitress giving her name as part of her greeting to the customers. Equivalently, the alternative hypothesis states that the population mean tip amount is greater when the waitress introduces herself by name than when she does not. In symbols, \\[ H_0 : \\mu_{name} = \\mu_{no\\ name} \\\\ H_1 : \\mu_{name} &gt; \\mu_{no\\ name} \\] ► Question B.5 Comment on what a Type I error and a Type II error would mean in this particular study. Would you consider one of these two errors to be more worrying than the other? Explain why. ► Solution A Type I error is committed when the waitress decides that introducing herself by name helps when, in reality, it does not. A Type II error is committed when the waitress decides that introducing herself by name is not helpful when, in reality, it actually is. A Type I error means that the waitress will spend just a tiny amount of time longer as part of her greeting without getting any extra benefit from it. A Type II error means that the waitress will not bother giving customers her name and thus would lose out on a higher tip. Clearly, as the burden of adding the name to the introduction is minimal, we consider as more worrying (or worst error) losing out on potential tips. So, in this specific study, a Type II error is of higher concern. ► Question B.6 Assuming that the population variances are not equal, calculate the test statistic and the \\(p\\)-value. For your convenience, we have already calculated the degrees of freedom, which are \\(\\textrm{df} =\\) 32.5. Hint: As you do not have the party-by-party tipping amounts, but only summary statistics, you can not use the t.test() function, which requires the data at the finest level (the observational units). ► Solution The two-sample \\(t\\)-test in the case of unequal population variances involves the Welch-Satterthwaite approximation to the degrees of freedom. The \\(t\\)-statistic is: n_name &lt;- 20 xbar_name &lt;- 5.44 s_name &lt;- 1.75 n_no &lt;- 20 xbar_no &lt;- 3.49 s_no &lt;- 1.13 SE &lt;- sqrt(s_name^2 / n_name + s_no^2 / n_no) t_stat &lt;- (xbar_name - xbar_no) / SE t_stat ## [1] 4.186343 The question provides us the degrees of freedom calculated using Welch’s formula: \\(\\textrm{df} = 32.5\\). df &lt;- 32.5 Critical value: qt(0.95, df = df) ## [1] 1.693112 \\(p\\)-value: 1 - pt(t_stat, df = df) ## [1] 0.0001010876 The \\(p\\)-value is &lt;.0005. ► Question B.7 At the significance level \\(\\alpha = 0.05\\), what would you conclude? ► Solution As the \\(p\\)-value \\(&lt;.05\\), we reject the null hypothesis that there is no effect of giving her name on the tipping amount. The sample results provide strong evidence that including her name as part of the customer’s greeting tends to lead to higher tips on average. ► Question B.8 The paper only reports the sample mean tips and standard deviations for the name and no name conditions. Does the paper provide enough information to check whether the validity conditions of the two-sample \\(t\\)-test are satisfied? If yes, check that the conditions are met. If not, explain which additional information you would need. ► Solution We do not have enough information to check whether the validity conditions are met. We are told that the two-person parties were randomly assigned either to the name or no name condition, but the two sample sizes (20 and 20) are not very large, so we should check whether the data came from normal distributions. However, we only have summary statistics and not the actual tip amounts for each party (experimental unit). We would ask the waitress to provide us the party-by-party tipping amounts in order to check if the populations the samples came from can be assumed to be normal. ► Question B.9 Calculate a 95% confidence interval for the difference in population mean tipping amount between the name and no name conditions. Write a sentence or two interpreting what the interval reveals. Hint: The degrees of freedom were given in Question B.6. ► Solution First, we must find the appropriate critical value \\(t^*_\\textrm{df}\\) (for the desired confidence level) from a \\(t\\)-distribution with degrees of freedom given by Welch’s method (see Question B.6 for the value). The critical value for a 95% confidence level is: t_crit_ci &lt;- qt(0.975, df) t_crit_ci ## [1] 2.035705 We compute a 95% confidence interval for the difference in population means, \\(\\mu_{name} - \\mu_{no\\ name}\\), as follows: \\[ (\\bar x_{name} - \\bar x_{no\\ name}) \\pm t^*_\\textrm{df} \\sqrt{\\frac{s_{name}^2}{n_{name}} + \\frac{s_{no\\ name}^2}{n_{no\\ name}}} \\\\ (5.44 - 3.49) \\pm 2.036 \\sqrt{\\frac{1.75^2}{20} + \\frac{1.13^2}{20}} \\] ci &lt;- tibble( Lower = (xbar_name - xbar_no) - t_crit_ci * sqrt(s_name^2 / 20 + s_no^2 / 20), Upper = (xbar_name - xbar_no) + t_crit_ci * sqrt(s_name^2 / 20 + s_no^2 / 20) ) ci ## # A tibble: 1 x 2 ## Lower Upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 2.90 The 95% confidence interval is [1, 2.9]. We are 95% confident that the waitress would earn, on average, between $1 and $2.9 more per party with a $23.21 bill, by including her name as part of the greeting. ► Question B.10 Regardless of whether the validity conditions of the \\(t\\)-test are met, summarise your conclusions from this test. Make sure to also comment on causation and generalisability of your results. ► Solution The study involved random assignment of the experimental units to the groups, so the only difference between the groups was whether the party was given the waitress’ name or not. As the parties in the name condition tend to give significantly higher tips on average than those in the no name condition (\\(t(32.5)\\) = 4.19, \\(p\\)-value &lt; .0005, one-sided), we can attribute this difference in means to being told the waitress’ name as part of the greeting. In other words, we can conclude a causal link between being given the name as part of the greeting and receiving higher tips on average. However, as the waitress was not blind to the treatment condition, we must be wary to the fact that the waitress could have given better service to the parties who she gave her name to. So, the results hold unless the waitress gave better service to the name condition. Having established that there is a significant difference in means between the two groups, a confidence intervals lets us now to estimate, on average, how much higher the tips will be when giving her name. Including her name as part of the greeting to customers increases the waitress’ tips, on average, by $1 to $2.9 per party. We must be careful when generalising these results to the population. As only one particular waitress participated in the study, we don’t want to generalise these results to other waitresses. Furthermore, we might also avoid generalising these results to customers different from those who eat Sunday brunch at Charley Brown’s Restaurant in Huntington Beach, California. Finally, the \\(p\\)-values and confidence interval are valid only if the response variable “tipping amount” is normally distributed in the two populations. 17.5 Glossary Boxplot. Visual display of the five-number summary for a quantitative variable. Five-number summary. Minimum, lower quartile, median, upper quartile, maximum. Lower quartile. The value in the data such that 25% of the data lie below that value. Upper quartile. The value in the data such that 25% of the data lie above that value. Assumptions of the two-sample \\(t\\)-test. The data should arise from random sampling from two populations and either the variable is normally distributed in both populations or both sample sizes are large (\\(n_1 \\geq 20\\) and \\(n_2 \\geq 20\\)) and the sample distributions should not be strongly skewed. 17.6 References Garrity, K., &amp; Degelman, D. (1990). Effect of server introduction on restaurant tipping. Journal of Applied Social Psychology, 20(2), 168-172. Tintle, N., Chance, B. L., Cobb, G. W., Rossman, A. J., Roy, S., Swanson, T., &amp; VanderStoep, J. (2015). Introduction to statistical investigations. New York: Wiley. Material adapted from: Rossman, A. J., &amp; Chance, B. L. (2011). Workshop statistics: discovery with data. John Wiley &amp; Sons. "],
["chap-paired-t-test.html", "Chapter 18 Paired T-Test Recap Paired \\(t\\)-test 18.1 Walkthrough 18.2 Summary 18.3 Lab", " Chapter 18 Paired T-Test Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Understand how to perform a paired-sample \\(t\\)-test and interpret the results. LO2. Learn how to compute Cohen’s \\(D\\) for different types of \\(t\\)-test. Recap Last week we extended our understanding of the \\(t\\)-test to compare the means of two populations (or the means of two treatment groups, if doing a randomized experiment). We therefore now have the tools to answer questions of the form: is [population mean] different from [hypothesized value] ? (one sample \\(t\\)-test) is [population mean 1] different from [population mean 2] ? (independent samples \\(t\\)-test) Note: this is the same as “is the mean of [variable] different between [group-1] and [group-2] ?” One sample \\(t\\)-test (Week 16) \\[ t = \\frac{\\bar{x} - \\mu_0}{SE(\\bar{x})}, \\qquad SE(\\bar{x}) = \\frac{s}{\\sqrt{n}} \\] Validity conditions: i. The data are continuous (not discrete). ii. The quantitative variable of interest is normally distributed in the population OR the sample size is large enough (as a convention, \\(n \\geq 20\\)) and the data are not strongly skewed. iii. The data are randomly sampled from a population. Two independent samples \\(t\\)-test (with equal variances) (Week 17) \\[ t = \\frac{\\bar x_1 - \\bar x_2}{SE(\\bar x_1 - \\bar x_2)}, \\qquad SE(\\bar{x}_1 - \\bar{x}_2) = S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}, \\qquad S_p = \\sqrt\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} \\] Validity conditions: i. The data are continuous (not discrete). ii. The quantitative variable of interest is normally distributed in both populations OR both sample sizes are large (as a convention, \\(n_1 \\geq 20\\) and \\(n_2 \\geq 20\\)) and the sample distributions should not be strongly skewed. iii. Independence of observations within and across groups. iv. Homogeneity of variance across groups. We also learnt that when the response variable does not have equal variance across groups, we could use the Welch-Satterthwaite approximation to the degrees of freedom, and set var.equal=FALSE in the t.test() function. Paired \\(t\\)-test The last \\(t\\)-test we are going to learn about is the paired \\(t\\)-test, or ‘dependent samples \\(t\\)-test’. Recall that last week we were concerned with evaluating the differences in means between two groups, and one of our assumptions in conducting our test was that the two groups were independent. However, we are often interested in asking whether there is a difference in means between two sets of observations which are paired. Paired data We say that two sets of observations \\(A\\) and \\(B\\) are paired when there is some pairing in the sets such that \\(A_1\\) is linked to \\(B_1\\) in the same way that \\(A_2\\) is linked to \\(B_2\\), and so on. ► Pairs of time points: Same person, same variable, different times Do peoples’ scores differ between timepoint one and timepoint two? (Same participants and same test administered at multiple timepoints) subject time_1 time_2 sub1 25 25 sub2 18 19 … … … ► Pairs of variables: Same person, different variables Do peoples’ scores on maths tests differ from their scores on English tests? (Same people taking both tests and producing a score on maths and score on English) subject maths_test english_test sub1 42 38 sub2 46 40 … … … ► Pairs of people: Same variable, different individual from a dyad (pair of people) Do employees work different amount of hours to their bosses? (Same variable (hours worked per day) collected from each person in pairs of people (i.e., each employee and their respective bosses)). pair line_manager_hours employee_hours pair1 10 9 pair2 8 9 … … … Comparing the means of two dependent groups (paired \\(t\\)-test) Suppose you wish to determine whether the mean difference between two dependent sets of observations is zero, and if the difference is significant, you wish to quantify the magnitude of the difference using a confidence interval. Test of significance Null hypothesis: The null hypothesis assumes that the true mean difference (\\(\\mu_d\\)) is equal to zero. \\(H_0: \\mu_d = 0\\) Alternative hypothesis: The alternative hypothesis assumes that \\(\\mu_d\\) is less than/greater than/not equal to zero. \\[ \\begin{matrix} i. &amp; H_1: \\mu_d &lt; 0 \\\\ ii. &amp; H_1: \\mu_d &gt; 0 \\\\ iii. &amp; H_1: \\mu_d \\neq 0 \\end{matrix} \\] Test statistic: \\[ t = \\frac{\\bar{d} - 0}{SE(\\bar{d})}, \\qquad SE(\\bar{d})= \\frac{s_d}{\\sqrt{n}} \\] \\(p\\)-value: \\[ \\begin{matrix} i. &amp; \\mathrm{Pr}(T_{df} \\leq t) \\\\ ii. &amp; \\mathrm{Pr}(T_{df} \\geq t) \\\\ iii. &amp; 2 \\times \\mathrm{Pr}(T_{df} \\geq |t|) \\end{matrix} \\] where \\(T_{df}\\) denotes a \\(t\\)-distribution with \\(df\\) degrees of freedom. Confidence interval for \\(\\mu_d - 0\\) \\[ (\\bar d) \\pm t^*_{df} \\times SE(\\bar d) \\] where \\(t^*_{df}\\) denotes the critical value corresponding to a desired \\(\\alpha\\) level for a \\(t\\)-distribution with \\(df\\) degrees of freedom. Validity conditions These above procedures are considered valid if: Data are matched pairs (design issue). The differences score arise from independent random samples from the population (i.e., independence within group/time). Either the difference scores are normally distributed in the population OR the sample size is large (as a convention, \\(n \\geq 20\\)) and the sample distribution should not be strongly skewed. THE PAIRED \\(t\\)-TEST IS JUST THE ONE SAMPLE \\(t\\)-TEST IN DISGUISE!! Steps for the one sample \\(t\\)-test: calculate the sample mean (\\(\\bar x\\)) calculate the sample standard deviation (\\(s_x\\)) calculate \\(t\\) calculate the probability of observing a \\(t\\)-statistic at least as extreme assuming the null hypothesis (\\(\\mu_1 = \\mu_0\\)) to be true. Steps for paired \\(t\\)-test: calculate the difference score for each pairs calculate the sample mean difference score (\\(\\bar d\\)) calculate the sample standard deviation of difference scores (\\(s_d\\)) calculate \\(t\\) calculate the probability of observing a \\(t\\)-statistic at least as extreme assuming the null hypothesis (\\(\\mu_d = 0\\)) to be true. The formula for the \\(t\\)-statistic, assessing statistical significance, and constructing confidence intervals is identical to the one-sample \\(t\\)-test. We have simply changed the notation: Instead of talking about a population mean \\(\\mu\\) which is estimated by a sample mean \\(\\bar x\\), we are talking about a population mean difference \\(\\mu_d\\) estimated by the sample mean difference \\(\\bar d\\). And we are testing whether \\(\\mu_d\\) is different from 0. 18.1 Walkthrough Change in score on the ACE-III Addenbrooke’s Cognitive Examination-III (ACE-III) is a brief cognitive test that assesses five cognitive domains: attention, memory, verbal fluency, language and visuospatial abilities. The total score is 100 with higher scores indicating better cognitive functioning. A research project is examining changes in cognitive functioning with age, and administers the ACE-III to a set of participants at age 60, then again at age 70. ► Question A.1 How is the data from this study paired? ► Solution Same people, same variable, different time points. ► Question A.2 Write out the null and alternative hypotheses in words, and using the appropriate symbols. ► Solution As the study is asking about the change in cognitive functioning with age, we are not presupposing that scores increase or decrease. Our null hypothesis is that the population mean difference in scores on the ACE-III between ages 60 and 70 is equal to zero (i.e., no change). \\(H_0: \\mu_d = 0\\) Our alternative hypothesis is that the population mean difference in scores on the ACE-III between ages 60 and 70 is not equal to zero (e.g., scores at age 70 are different from scores at age 60). \\(H_1: \\mu_d \\neq 0\\) ► Question A.3 Read in the data, and make a new column of the difference in scores for each pair. The data is available as a .csv from https://edin.ac/2Tz1cNH. ► Solution acedata &lt;- read_csv(&quot;https://edin.ac/2Tz1cNH&quot;) acedata &lt;- acedata %&gt;% mutate( diff_score = ace_70 - ace_60 ) ► Question A.4 Perform a one-sample \\(t\\)-test to see whether the mean difference score is equal to 0. ► Solution Calculate the \\(\\bar d\\), \\(s_d\\), \\(n\\): aceterms &lt;- acedata %&gt;% summarise( dbar = mean(diff_score), s_d = sd(diff_score), n = n() ) aceterms ## # A tibble: 1 x 3 ## dbar s_d n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 -2.48 5.50 25 Calculate \\(t\\): t_stat = (aceterms$dbar - 0)/(aceterms$s_d / sqrt(aceterms$n)) t_stat ## [1] -2.254173 Calculate the p-value: \\(2 \\times \\mathrm{Pr}(T_{df} \\geq |t|)\\): 2 * (1 - pt(abs(t_stat), df = 24)) ## [1] 0.03358854 Compute the confidence interval: \\[ (\\bar d) \\pm t^*_{df} \\times SE(\\bar d) \\] #We know dbar aceterms$dbar ## [1] -2.48 #And we can calculate the SE(dbar) aceterms$s_d / sqrt(aceterms$n) ## [1] 1.100182 We need to know the critical values of \\(t\\), for our desired \\(\\alpha\\), with our \\(df=24\\). crit_t = qt(.975, df = 24) crit_t ## [1] 2.063899 # lower interval aceterms$dbar - ( crit_t * (aceterms$s_d / sqrt(aceterms$n))) ## [1] -4.750664 # upper interval aceterms$dbar + ( crit_t * (aceterms$s_d / sqrt(aceterms$n))) ## [1] -0.2093364 Or all using t.test() function: t.test(acedata$diff_score, mu = 0) ## ## One Sample t-test ## ## data: acedata$diff_score ## t = -2.2542, df = 24, p-value = 0.03359 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -4.7506636 -0.2093364 ## sample estimates: ## mean of x ## -2.48 Or, equivalently: t.test(acedata$ace_70, acedata$ace_60, paired = TRUE) ## ## Paired t-test ## ## data: acedata$ace_70 and acedata$ace_60 ## t = -2.2542, df = 24, p-value = 0.03359 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.7506636 -0.2093364 ## sample estimates: ## mean of the differences ## -2.48 ► Question A.5 Provide a write-up of the results. ► Solution A paired-sample \\(t\\)-test was conducted in order to determine if a statistically significant (\\(\\alpha\\) = .05) mean change in cognitive functioning (as measured using the total scores on the ACE-III) was present between the ages of 60 and 70, in a sample of 25 participants. The mean difference in score on the ACE-III from age 60 to age 70 was -2.48 (95% CI=(-4.75 - -0.21). The difference was statistically significant (\\(t\\)(24)= -2.25, \\(p\\) &lt; . 05, two-tailed). Thus, we reject the null hypothesis of no difference. Data organisation - reshaping In the lecture, we talked briefly about data organisation, and how we might reshape data to make it go from this (long format): participant age ace_score sub1 ace_60 93 sub1 ace_70 85 sub2 ace_60 95 sub2 ace_70 92 sub3 ace_60 93 sub3 ace_70 90 … … … To this (wide format): participant ace_60 ace_70 sub1 93 85 sub2 95 92 sub3 93 90 sub4 93 95 sub5 96 88 sub6 91 85 … … … New R stuff! We can reshape datasets using the functions pivot_longer() and pivot_wider(), which we mentioned briefly in the lecture. (source: https://www.fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/) Recall that our data was wide: acedata ## # A tibble: 25 x 4 ## participant ace_60 ace_70 diff_score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sub1 93 85 -8 ## 2 sub2 95 92 -3 ## 3 sub3 93 90 -3 ## 4 sub4 93 95 2 ## 5 sub5 96 88 -8 ## 6 sub6 91 85 -6 ## 7 sub7 94 92 -2 ## 8 sub8 90 90 0 ## 9 sub9 90 92 2 ## 10 sub10 96 91 -5 ## # … with 15 more rows We can make it long, by: acedata_long &lt;- acedata %&gt;% pivot_longer(ace_60:ace_70, names_to = &quot;age&quot;, values_to=&quot;ace_score&quot;) acedata_long ## # A tibble: 50 x 4 ## participant diff_score age ace_score ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sub1 -8 ace_60 93 ## 2 sub1 -8 ace_70 85 ## 3 sub2 -3 ace_60 95 ## 4 sub2 -3 ace_70 92 ## 5 sub3 -3 ace_60 93 ## 6 sub3 -3 ace_70 90 ## 7 sub4 2 ace_60 93 ## 8 sub4 2 ace_70 95 ## 9 sub5 -8 ace_60 96 ## 10 sub5 -8 ace_70 88 ## # … with 40 more rows This takes the columns from ace_60 to ace_70 (and anything in between), and puts the names (i.e. the variable names) into a column we call “age”, and puts the values into a column we call “ace_score”. Note that we could now run the same paired \\(t\\)-test using: t.test(acedata_long$ace_score ~ acedata_long$age, paired = T) ## ## Paired t-test ## ## data: acedata_long$ace_score by acedata_long$age ## t = 2.2542, df = 24, p-value = 0.03359 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.2093364 4.7506636 ## sample estimates: ## mean of the differences ## 2.48 We can turn it back to wide using pivot_wider(): acedata_long %&gt;% pivot_wider(names_from = age, values_from = ace_score) ## # A tibble: 25 x 4 ## participant diff_score ace_60 ace_70 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sub1 -8 93 85 ## 2 sub2 -3 95 92 ## 3 sub3 -3 93 90 ## 4 sub4 2 93 95 ## 5 sub5 -8 96 88 ## 6 sub6 -6 91 85 ## 7 sub7 -2 94 92 ## 8 sub8 0 90 90 ## 9 sub9 2 90 92 ## 10 sub10 -5 96 91 ## # … with 15 more rows ► Question A.6 The code below produces a visualisation of the scores on the ACE-III at ages 60 and 70. ggplot(data = ace_iii, aes(x = age, y = score)) + geom_boxplot() Based on what we found out when we conducted our \\(t\\)-test, sketch what you think the code will produce. What does the data ace_iii need to look like for this to work? ► Solution We can tell that for the plot to work, it must find all the scores in a column called score, and all the ages (e.g., 60 and 70) in a column called age. So the data will have to look something like this (i.e., in long format): participant age score sub1 60 93 sub1 70 85 sub2 60 95 sub2 70 92 sub3 60 93 sub3 70 90 … … … ace_iii &lt;- acedata %&gt;% pivot_longer(ace_60:ace_70, names_to = &quot;age&quot;, values_to=&quot;score&quot;) ggplot(data = ace_iii, aes(x = age, y = score)) + geom_boxplot() Effect sizes We also learned in the lecture about Cohen’s \\(D\\) - a means of reporting a standardised magnitude of our difference. Formula for Cohen’s \\(D\\): Test Cohen’s \\(D\\) One sample \\(t\\)-test \\(D = \\frac{ \\bar x - \\mu_0}{s_x}\\) Independent samples \\(t\\)-test \\(D = \\frac{ \\bar{x}_1 - \\bar{x}_2}{s_p}\\) Paired \\(t\\)-test \\(D = \\frac{ \\bar{d} - 0}{s_d}\\) Interpreting Cohen’s \\(D\\): ~ 0.2 = small effect ~ 0.5 = moderate effect ~ 0.8 = large effect ► Question A.7 Calculate the effect size for the mean difference in scores on the ACE-III from age 60 to 70. ► Solution We know all the terms for this already. In fact, we calculated them and saved them in an object called aceterms. aceterms ## # A tibble: 1 x 3 ## dbar s_d n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 -2.48 5.50 25 So we can calculate our effect size: (aceterms$dbar - 0) / aceterms$s_d ## [1] -0.4508346 18.2 Summary The paired \\(t\\)-test is the same as the one sample \\(t\\)-test. Instead of assessing the distance from \\(\\bar x - \\mu_0\\), we are investigating the distance of \\(\\bar d - 0\\). We can conduct a paired \\(t\\)-test in R by either: calculating the difference scores and using t.test(difference_scores, mu = 0) running t.test(score1, score2, paired = TRUE) We can compute Cohen’s \\(D\\) for standardised size of our effect. We can reshape our data between long to wide. This helps with plotting, and also means we can conduct our test using t.test(score ~ group, paired = TRUE). 18.3 Lab Please attempt the questions before looking at the solutions. Copy &amp; pasting the solutions will not help with learning! Age differences in heterosexual marriages Research Q Is there an age difference in heterosexual married couples? (and in what direction?) Data on the ages of both brides and grooms at point of marriage is available at https://edin.ac/2TBoZMY. You can read it in using the code below: marriages&lt;-read_csv(&quot;https://edin.ac/2TBoZMY&quot;) ► Question B.1 Add new column which is the husband’s age minus the wife’s age. As we plan on conducting a test on these differences, what assumptions are we making about them, and how can we check whether those assumptions hold? ► Solution marriages &lt;- marriages %&gt;% mutate( agediff = Husband - Wife ) Remember, as this is just a one sample \\(t\\)-test in disguise, we are assuming that the differences come from a normally distribution. Our sample size is quite large (\\(n = 105\\)) so it is not essential to test for normality (using shapiro.test()), but we should always plot the data first, to check for thing like skew. ggplot(marriages, aes(x=agediff)) + geom_density() ► Question B.2 Think carefully.. If we are about to conduct a \\(t\\)-test on the difference scores we just calculated, what will the results mean? If we get a positive \\(t\\)-statistic (\\(t &gt; 0\\)), what direction is the difference? Who will it mean tends to be older than whom? ► Solution Because we calculated the husband’s age minus wife’s age, a positive \\(t\\)-statistic will indicates that this value is &gt; 0 (meaning that in heterosexual marriages, husbands tend be the older of the two). ► Question B.3 Conduct a one sample \\(t\\)-test that the age differences are not equal to 0. Using the paired = TRUE argument of t.test(), conduct the same test on the ages themselves (rather than the differences). Check that the results from 1 and 2 match. ► Solution t.test(marriages$agediff, mu = 0) ## ## One Sample t-test ## ## data: marriages$agediff ## t = 5.596, df = 101, p-value = 1.888e-07 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 1.531495 3.213603 ## sample estimates: ## mean of x ## 2.372549 t.test(marriages$Husband, marriages$Wife, paired = TRUE) ## ## Paired t-test ## ## data: marriages$Husband and marriages$Wife ## t = 5.596, df = 101, p-value = 1.888e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.531495 3.213603 ## sample estimates: ## mean of the differences ## 2.372549 ► Question B.4 Calculate Cohen’s \\(D\\). ► Solution marriages %&gt;% summarise( dbar = mean(agediff), s_d = sd(agediff) ) ## # A tibble: 1 x 2 ## dbar s_d ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.37 4.28 2.37 / 4.28 ## [1] 0.5537383 Smoking interventions Research Q A researcher has developed a cognitive behavioural therapy (CBT) based smartphone app, and is testing how effective it is in helping people to stop smoking. They recruit 60 participants who are trying to quit smoking, and over a week record the average number of cigarettes smoked per day for each participant. Thirty participants are given the app, and asked that every time they feel like smoking they open the app on their phone and complete one five-minute task. All 60 participants are followed up one month later, and the average number of cigarettes smoked per day (over a week) is recorded. At point of recruitment, did the 60 participants smoke more or less than 20 cigarettes per day? Calculate the average number of cigarettes smoked per day at both time-points for each group. Did the average number of cigarettes smoked per day differ between the groups at the initial recruitment? Did the group given the app change their smoking habits from the immediate to one month follow up? If so, was it a big change? (e.g, calculate the effect size). The data (.csv) is available at https://edin.ac/2vRr3r9. ► Question C Answer research questions 1 to 4. This requires various things you have learned over the entire course, to do with data manipulation (grouping, summarising, filtering, etc.), as well as deciding on and conducting appropriate tests. ► C1 At point of recruitment, did the 60 participants smoke more or less than 20 cigarettes per day? This is asking whether the mean number of cigarettes per day is different to 20. So we are asking whether a mean is different from an hypothesized number - this is a one sample \\(t\\)-test! First, lets check our assumptions. Our \\(n \\geq 20\\), but let’s check normality anyway: shapiro.test(cbtsmoke$cigs_pday) ## ## Shapiro-Wilk normality test ## ## data: cbtsmoke$cigs_pday ## W = 0.97845, p-value = 0.3666 At a significance level of 0.05, the \\(p\\)-value = 0.37 means that we fail to reject the null hypothesis that the data is drawn from a normally distributed population. We can now perform our \\(t\\)-test: t.test(cbtsmoke$cigs_pday, mu = 20) ## ## One Sample t-test ## ## data: cbtsmoke$cigs_pday ## t = -11.095, df = 59, p-value = 4.544e-16 ## alternative hypothesis: true mean is not equal to 20 ## 95 percent confidence interval: ## 13.86217 15.73783 ## sample estimates: ## mean of x ## 14.8 At a significance level of 0.05, we reject the null hypothesis that the average number of cigarettes smoked per day at point-of-recruitment is equal to 20 (\\(t(39)=-11.1, p&lt;.05\\) two-sided). ► C2 Calculate the average number of cigarettes smoked per day at both time-points for each group. cbtsmoke %&gt;% group_by(app_group) %&gt;% summarise( initial = mean(cigs_pday), followup = mean(cigs_pday_1month) ) ## # A tibble: 2 x 3 ## app_group initial followup ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 no 14.8 12.7 ## 2 yes 14.8 9.7 ► C3 Did the average number of cigarettes smoked per day differ between the groups at the initial recruitment? The two sets of observations (from the different groups) are independent, so we are going to conduct an independent samples \\(t\\)-test. First, we need to check the assumption of equal variances between groups: var.test(cbtsmoke$cigs_pday ~ cbtsmoke$app_group) ## ## F test to compare two variances ## ## data: cbtsmoke$cigs_pday by cbtsmoke$app_group ## F = 0.72721, num df = 29, denom df = 29, p-value = 0.3961 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.3461274 1.5278697 ## sample estimates: ## ratio of variances ## 0.7272121 At a significance level of 0.05, the \\(p\\)-value = 0.4 leads us to not rejecting the null hypothesis of equal variances across the two populations. Because \\(n \\geq 20\\), we don’t need to check for normality, but it is important to look for the skew, especially when our \\(n\\) is not that much bigger than 20. The plot below suggest that skew is not a problem ggplot(cbtsmoke, aes(x = cigs_pday, col = app_group)) + geom_density() We can now perform our \\(t\\)-test: t.test(cbtsmoke$cigs_pday ~ cbtsmoke$app_group, var.equal = TRUE) ## ## Two Sample t-test ## ## data: cbtsmoke$cigs_pday by cbtsmoke$app_group ## t = -0.070519, df = 58, p-value = 0.944 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.959022 1.825689 ## sample estimates: ## mean in group no mean in group yes ## 14.76667 14.83333 ► C4 Did the group given the app change their smoking habits from the immediate to one month follow up? If so, was it a big change? (e.g, calculate the effect size). We will need to use only the data from the group who was given the app: appgroupsmoke &lt;- cbtsmoke %&gt;% filter(app_group == &quot;yes&quot;) We can then perform a paired \\(t\\)-test: t.test(appgroupsmoke$cigs_pday_1month, appgroupsmoke$cigs_pday, paired = TRUE) ## ## Paired t-test ## ## data: appgroupsmoke$cigs_pday_1month and appgroupsmoke$cigs_pday ## t = -5.0883, df = 29, p-value = 1.985e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.196663 -3.070004 ## sample estimates: ## mean of the differences ## -5.133333 Or, we could calculate the difference scores… appgroupsmoke &lt;- appgroupsmoke %&gt;% mutate( change = cigs_pday_1month - cigs_pday ) and run the same test using: t.test(appgroupsmoke$change, mu = 0) ## ## One Sample t-test ## ## data: appgroupsmoke$change ## t = -5.0883, df = 29, p-value = 1.985e-05 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -7.196663 -3.070004 ## sample estimates: ## mean of x ## -5.133333 To calculate Cohen’s \\(D\\), we use the mean difference (\\(\\bar d\\)) divided by the standard deviation of the differences (\\(s_d\\)): appgroupsmoke %&gt;% summarise( dbar = mean(change), s_d = sd(change) ) ## dbar s_d ## 1 -5.133333 5.525698 abs(-5.13) / 5.53 ## [1] 0.9276673 knitr::opts_chunk$set(fig.align=&#39;center&#39;, message=FALSE, out.width = &#39;80%&#39;) "],
["chap-chi-square.html", "Chapter 19 Chi-square tests Recap Walkthrough: Birthdays of the week The chi-square goodness-of-fit test Lab: Joking for a tip The chi-square test of independence Summary Glossary References", " Chapter 19 Chi-square tests Instructions The lab material consists of a worked example for the walkthrough, and a series of questions for you to attempt. The Rmarkdown file for this week is here. Learning outcomes By the end of this lab, you should be able to: LO1. Understand when to use a chi-square goodness-of-fit and chi-square test of independence. LO2. Check the validity conditions. LO3. Report the test results. Recap Test type One-sample Independent samples Paired data Null hypothesis \\(H_0 : \\mu = \\mu_0\\) \\(H_0 : \\mu_1 - \\mu_2 = 0\\) \\(H_0 : \\mu_d = 0\\) Alternative hypothesis \\[ \\begin{array}{ll} i. &amp; H_1 : \\mu &lt; \\mu_0 \\\\ ii. &amp; H_1 : \\mu &gt; \\mu_0 \\\\ iii. &amp; H_1 : \\mu \\neq \\mu_0 \\end{array} \\] \\[ \\begin{array}{ll} i. &amp; H_1 : \\mu_1 - \\mu_2 &lt; 0 \\\\ ii. &amp; H_1 : \\mu_1 - \\mu_2 &gt; 0 \\\\ iii. &amp; H_1 : \\mu_1 - \\mu_2 \\neq 0 \\end{array} \\] \\[ \\begin{array}{ll} i. &amp; H_1 : \\mu_d &lt; 0 \\\\ ii. &amp; H_1 : \\mu_d &gt; 0 \\\\ iii. &amp; H_1 : \\mu_d \\neq 0 \\end{array} \\] Test statistic \\[ t = \\frac{\\bar x - \\mu_0}{SE(\\bar x)} \\] \\[ t = \\frac{\\bar x_1 - \\bar x_2}{SE(\\bar x_1 - \\bar x_2)} \\] \\[ t = \\frac{\\bar d - 0}{SE(\\bar d)} \\] Standard error \\(SE(\\bar x) = \\frac{s}{\\sqrt{n}}\\) (a) Unequal population variances\\(SE(\\bar x_1 - \\bar x_2) = \\sqrt{ \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} }\\)(b) Equal population variances\\(SE(\\bar x_1 - \\bar x_2) = s_p \\sqrt{ \\frac{1}{n_1} + \\frac{1}{n_2} }\\)\\(s_p = \\sqrt{\\frac{(n_1 - 1 )s_1 ^2 + (n_2 - 1 )s_2 ^2}{n_1 + n_2 - 2}}\\) \\(SE(\\bar d) = \\frac{s_d}{\\sqrt{n}}\\) P-value \\[ \\begin{array}{ll} i. &amp; \\Pr(T_{df} \\leq t) \\\\ ii. &amp; \\Pr(T_{df} \\geq t) \\\\ iii. &amp; 2 \\Pr(T_{df} \\geq |t|) \\end{array} \\] \\[ \\begin{array}{ll} i. &amp; \\Pr(T_{df} \\leq t) \\\\ ii. &amp; \\Pr(T_{df} \\geq t) \\\\ iii. &amp; 2 \\Pr(T_{df} \\geq |t|) \\end{array} \\] \\[ \\begin{array}{ll} i. &amp; \\Pr(T_{df} \\leq t) \\\\ ii. &amp; \\Pr(T_{df} \\geq t) \\\\ iii. &amp; 2 \\Pr(T_{df} \\geq |t|) \\end{array} \\] In the past weeks we focused on tests for: a quantitative response variable (weeks 16 and 18); a quantitative response variable and a binary categorical explanatory variable (week 17). This week we will investigate tests for: a categorical response variable (chi-square goodness-of-fit test); a categorical response variable and a categorical explanatory variable (chi-square test of independence). For the first time, you will be working with a statistical test that applies to categorical variables with more than two categories. The two tests that you will be applying today are the chi-square goodness-of-fit test and the chi-square test of independence. The first is used to assess whether sample results conform with an hypothesis about the proportional breakdown of the various categories in the population. The latter tests if two categorical variables are associated in the population or not. Walkthrough: Birthdays of the week The chi-square goodness-of-fit test What day of the week were you born on? Are people equally likely to be born on any of the seven days of the week? Or are some days more likely to be a person’s birthday than other days? To investigate this question, days of birth were recorded for the 147 “noted writers of the present” listed in The World Almanac and Book of Facts 2000. The data are stored in the file writers.txt, accessible via the link https://edin.ac/2Ul2yLj. ► Question A.1 Identify the observational units and variable here. Is the variable categorical or quantitative? If it is categorical, is it also binary? Observational units: Variable: Type: ► Solution Observational units: The 147 noted writers of the present listed in World Almanac and Book of Facts 2000 Variable: Day of the week on which the birthday falls Type: Categorical ► Question A.2 Read the data into R. [Hint: The data is a .txt file - think about what function you are going to use.] Summarise the data by creating a frequency table of the seven days of the week. ► Solution Let’s read the data into R: library(tidyverse) writers &lt;- read_tsv(&#39;https://edin.ac/2Ul2yLj&#39;, col_names = TRUE) Inspect the first six rows: head(writers) ## # A tibble: 6 x 1 ## DayOfWeek ## &lt;chr&gt; ## 1 Thursday ## 2 Thursday ## 3 Tuesday ## 4 Friday ## 5 Friday ## 6 Sunday Check the dimensions of the tibble: dim(writers) ## [1] 147 1 Day of the week is encoded as a character rather than a factor (remember that day of the week is categorical). Let’s fix the encoding: writers &lt;- writers %&gt;% mutate(DayOfWeek = factor(DayOfWeek)) head(writers) ## # A tibble: 6 x 1 ## DayOfWeek ## &lt;fct&gt; ## 1 Thursday ## 2 Thursday ## 3 Tuesday ## 4 Friday ## 5 Friday ## 6 Sunday Check that the levels of the factor are in the order we expect: levels(writers$DayOfWeek) ## [1] &quot;Friday&quot; &quot;Monday&quot; &quot;Saturday&quot; &quot;Sunday&quot; &quot;Thursday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; They are not, so let’s fix this: writers &lt;- writers %&gt;% mutate( DayOfWeek = factor(DayOfWeek, levels= c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;)) ) levels(writers$DayOfWeek) ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Friday&quot; &quot;Saturday&quot; &quot;Sunday&quot; There are many equivalent ways to create a table of counts. One option could be: writers_table &lt;- writers %&gt;% group_by(DayOfWeek) %&gt;% summarise(count = n()) writers_table ## # A tibble: 7 x 2 ## DayOfWeek count ## &lt;fct&gt; &lt;int&gt; ## 1 Monday 17 ## 2 Tuesday 26 ## 3 Wednesday 22 ## 4 Thursday 23 ## 5 Friday 19 ## 6 Saturday 15 ## 7 Sunday 25 ► Question A.3 Construct a bar graph of these data which displays proportions on the y-axis. Comment on what it reveals about whether the seven days of the week are equally likely to be a person’s birthday. ► Solution The proportion of birthdays on a Monday is calculated as the count of birthdays on a Monday divided by the sample size, \\(n =\\) 147. The procedure is similar for the other days of the week. We can obtain the sample size using the function nrow(), which returns the total number of rows in the tibble: nrow(writers) ## [1] 147 First, we compute the sample proportions: writers_table &lt;- writers_table %&gt;% mutate(prop = count / nrow(writers)) writers_table ## # A tibble: 7 x 3 ## DayOfWeek count prop ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Monday 17 0.116 ## 2 Tuesday 26 0.177 ## 3 Wednesday 22 0.150 ## 4 Thursday 23 0.156 ## 5 Friday 19 0.129 ## 6 Saturday 15 0.102 ## 7 Sunday 25 0.170 Note: Remember that the sample proportions sum to one: sum(writers_table$prop) ## [1] 1 The following bar graph displays the data: ggplot(writers_table, aes(x = DayOfWeek, y = prop)) + geom_col(fill = &#39;lightblue&#39;) + labs(x = &#39;Day of the week&#39;, y = &#39;Proportion of writers&#39;) + theme_classic(base_size = 15) There does not appear to be a great difference in the proportion of people born on any given day of the week. ► Question A.4 Let \\(p_{Mo}\\) represent the proportion of all people who were born on a Monday, or equivalently, the probability that a randomly selected individual was born on a Monday. Similarly, define \\(p_{Tu},\\ p_{We},\\ ...,\\ p_{Su}\\). Are these parameters or statistics? Explain why. ► Solution The values \\(p_{Tu},\\ p_{We},\\ ...,\\ p_{Su}\\) represent the proportion of all people who were born on Tuesday, Wednesday, …, and Sunday, respectively. These values are parameters because they describe the entire population. ► Question A.5 The null hypothesis says that the seven days of the week are equally likely to be a person’s birthday. In that case, what are the values of \\(p_{Mo},\\ p_{Tu},\\ ...,\\ p_{Su}\\)? ► Solution The values of the proportions specified by the null hypothesis are: \\[ \\begin{array}{l} p_{Mo} = 1/7 = .1429 \\\\ p_{Tu} = 1/7 = .1429 \\\\ ... \\\\ p_{Su} = 1/7 = .1429 \\end{array} \\] The test procedure we are about to apply is called a chi-square goodness-of-fit test. It applies to a categorical variable, which does not have to be binary. The null hypothesis asserts specific values for the population proportion in each category. The alternative hypothesis simply states that at least one of the population proportions is not as specified in the null hypothesis. As always, the test statistic measures how far the observed sample results deviate from what is expected if the null hypothesis is true. With a chi-square test, you construct the test statistic by comparing the observed sample counts in each category to the expected counts under the null hypothesis. ► Question A.6 Intuitively, what value would make sense for the expected count of Monday birthdays in this study (with a sample size of 147), under the null hypothesis that one-seventh of all birthdays occur on Mondays? Explain why. ► Solution You would expect the frequency of birthdays on a Monday to be \\(147 \\times \\frac{1}{7}= 21\\). We would multiply the total sample size by the proportion of birthdays we expect on a Monday. We calculate the expected count for a particular category by multiplying the sample size by the hypothesized population proportion for that category: \\[ E_i = n \\times p_{i0} \\] [Note: the subscript \\(i\\) denotes the particular category, while the subscript \\(0\\) on \\(p_{i0}\\) emphasizes that this is the hypothesized value for \\(p_i\\) in the null hypothesis, \\(H_0\\).] ► Question A.7 Calculate the expected counts for each of the seven days. ► Solution As in this particular study \\(p_{Mo0} = p_{Tu0} = \\cdots = p_{Su0} = 1/7\\), the expected counts will all be computed as \\(147 (1/7) = 21\\). The sample size is: n &lt;- nrow(writers) n ## [1] 147 We multiply the sample size by the hypothesized proportion for each category, 1/7: writers_table &lt;- writers %&gt;% group_by(DayOfWeek) %&gt;% summarise(observed = n()) %&gt;% mutate(expected = rep(n * (1/7), 7)) writers_table ## # A tibble: 7 x 3 ## DayOfWeek observed expected ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Monday 17 21 ## 2 Tuesday 26 21 ## 3 Wednesday 22 21 ## 4 Thursday 23 21 ## 5 Friday 19 21 ## 6 Saturday 15 21 ## 7 Sunday 25 21 Note: The command rep(x, times) will repeat x a given number of times. For instance: rep(&quot;a&quot;, 4) ## [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; We calculated the expected count, n * (1/7), and then repeated it for the 7 days of the week, as in this particular example the null hypothesis specifies the same proportions for all seven days of the week: rep(n * (1/7), 7) ## [1] 21 21 21 21 21 21 21 How do we measure how far are the observed counts from the expected counts under the null hypothesis? If you simply subtract the expected counts from the observed counts and then add them up, you will obtain zero: writers_diff &lt;- writers_table %&gt;% mutate(diff = observed - expected) writers_diff ## # A tibble: 7 x 4 ## DayOfWeek observed expected diff ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Monday 17 21 -4 ## 2 Tuesday 26 21 5 ## 3 Wednesday 22 21 1 ## 4 Thursday 23 21 2 ## 5 Friday 19 21 -2 ## 6 Saturday 15 21 -6 ## 7 Sunday 25 21 4 # Check that the sum of (observed - expected) is zero sum(writers_diff$diff) ## [1] 0 Instead, we will square the differences between the observed and expected counts, and then add them up. One issue, however, remains to be solved. A squared difference between observed and expected counts of 100 has a different weight in these two scenarios: Scenario 1. \\(O = 30\\) and \\(E = 20\\) lead to a squared difference \\((O - E)^2 = 10^2 = 100\\). Scenario 2. \\(O = 3000\\) and \\(E = 1990\\) lead to a squared difference \\((O - E)^2 = 10^2 = 100\\). However, it is clear that a squared difference of 100 in Scenario 1 is much more substantial than a squared difference of 100 in Scenario 2. It is for this reason that we divide the squared differences by the the expected counts to “standardize” the squared deviation. The test-statistic (denoted \\(\\chi^2\\), spelled chi-square, pronounced “kai-square”) is obtained by adding up the standardized squared deviations: \\[ \\chi^2 = \\sum_{i} \\frac{(O_i - E_i)^2}{E_i} \\] and the sum is over all categories. Note: As \\((O - E)^2\\) is equal to \\((E - O)^2\\), you can equivalently write the test-statistic as: \\[ \\chi^2 = \\sum_{i} \\frac{(O_i - E_i)^2}{E_i} = \\sum_{i} \\frac{(E_i - O_i)^2}{E_i}. \\] ► Question A.8 For each of the seven days of the week, compute \\(\\frac{(O - E)^2}{E}\\). ► Solution We can compute the standardized squared deviations as follows: writers_table &lt;- writers_table %&gt;% mutate(std_sq_diff = (observed - expected)^2 / expected) writers_table ## # A tibble: 7 x 4 ## DayOfWeek observed expected std_sq_diff ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Monday 17 21 0.762 ## 2 Tuesday 26 21 1.19 ## 3 Wednesday 22 21 0.0476 ## 4 Thursday 23 21 0.190 ## 5 Friday 19 21 0.190 ## 6 Saturday 15 21 1.71 ## 7 Sunday 25 21 0.762 ► Question A.9 Add the standardized squared differences up, in order to compute the chi-square test statistic. ► Solution The value of the chi-square test statistic in this sample is: chi_stat &lt;- sum(writers_table$std_sq_diff) chi_stat ## [1] 4.857143 Equivalently, as $ is just a shortcut for the pull() function: chi_stat &lt;- writers_table %&gt;% pull(std_sq_diff) %&gt;% sum() ► Question A.10 What would be the value of the chi-square statistic if the observed and expected frequencies had been exactly equal? Based on this, what kind of values (e.g., large or small) of the test statistic constitute evidence against the null hypothesis that the seven days of the week are equally likely to be a person’s birthday? Do you think the value we just calculated (4.857) provides convincing evidence? If you are not sure, what additional information do you need? Explain your reasoning. ► Solution If the observed and expected frequencies had been exactly equal, the chi-square statistic would have been 0 and you would have had no reason to doubt the null hypothesis that all days of the week are equally likely to be someone’s birthday. \\[ \\chi^2 = \\frac{(21 - 21)^2}{21} + \\cdots + \\frac{(21 - 21)^2}{21} = 0 \\] Large values of the test statistic constitute evidence against the null hypothesis. Answers will vary about whether this test statistic provides convincing evidence. The additional evidence you need is the p-value. You need to know how likely it is that you would obtain a test statistic this large (or larger) by random chance alone if all seven days of the week are equally likely to be a person’s birthday. As always, your next step is to calculate the p-value. The p-value tells you the probability of getting sample data at least as far from the hypothesized proportions as these data are, by random chance if the null hypothesis is true. So again, a small p-value indicates the sample data is unlikely to have occurred by chance alone if the null hypothesis is true, providing evidence in favour of the alternative. When the test statistic is large enough to produce a small p-value, then the sample data provide strong evidence against the null hypothesis ► Question A.11 Calculate the p-value for this test using the function pchisq(&lt;test statistic&gt;, df). Write a sentence interpreting the p-value meaning in the context of this study about birthdays of the week. [Hint: the degrees of freedom are equal to the number of categories minus 1.] ► Solution We want the probability to the right of the chi-square test statistic: 1 - pchisq(chi_stat, df = 6) ## [1] 0.562262 If all seven days of the week are equally likely to be a person’s birthday, the probability that you would obtain a test statistic this large (or larger) by random chance alone is .56. ► Question A.12 Based on this p-value, what would be your test decision at the \\(\\alpha = .10\\) level? And at the \\(\\alpha = .05\\) and \\(\\alpha = .01\\) levels? ► Solution At the \\(\\alpha = .10\\) level, we fail to reject the null hypothesis that all seven days of the week are equally likely to be a person’s birthday, as the p-value is larger than the significance level. The same conclusion would be reached at the other two significance levels. Validity conditions Certain technical conditions must be satisfied for this chi-square procedure to provide accurate p-values. In addition to requiring a random sample from the population of interest, all expected counts need to be at least five. When this condition is not met, one option is to combine similar categories together to force all expected counts to be at least five. ► Question A.13 Is the expected counts condition satisfied for this study on birthdays? What about the random sampling condition? If not, would you be comfortable in generalising the results to a larger population anyway? Explain why. ► Solution Yes, all the expected counts are 21, which is greater than 5. However, the random sampling condition is not met. These are the birthdays of “noted writers of the present” which is not a random sample of the population of all citizens. ► Question A.14 Summarise your conclusion about whether these sample data provide evidence against the null hypothesis that any of the seven days of the week are equally likely to be a person’s birthday. ► Solution You have no statistical evidence against the null hypothesis that the seven days of the week are all equally likely to be a person’s birthday (at least for the population of famous writers). We can’t generalise this to the population of all citizens for the shortcoming discussed in the previous question. Technology detour: The chi-square test using built-in R functions In R, we can perform a chi-square test using the function chisq.test(&lt;frequency table&gt;, p=&lt;null hypothesis&gt;) Step 1. Frequency table and null hypothesis: observed &lt;- writers %&gt;% group_by(DayOfWeek) %&gt;% summarise(observed = n()) %&gt;% pull(observed) observed ## [1] 17 26 22 23 19 15 25 probabilities &lt;- rep(1/7, 7) probabilities ## [1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 Step 2. Use the function chisq.test: gof_test &lt;- chisq.test(observed, p = probabilities) gof_test ## ## Chi-squared test for given probabilities ## ## data: observed ## X-squared = 4.8571, df = 6, p-value = 0.5623 The results match those that we calculated above. The object returned by the function chisq.test() contains different variables: names(gof_test) ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;method&quot; &quot;data.name&quot; &quot;observed&quot; &quot;expected&quot; ## [8] &quot;residuals&quot; &quot;stdres&quot; You can obtain the expected counts as: gof_test$expected ## [1] 21 21 21 21 21 21 21 ► Question A.15 Which days of the week had the highest contributions to the chi-square test statistic? ► Solution Standardized squared deviations One possible way to answer this question is to look at the individual contribution of each category to the chi-square statistic. You computed these values in Question A.8. The categories Tuesday and Saturday contributed the most to the chi-square statistic as they had the highest standardized squared deviations, equal to 1.19 and 1.71, respectively. From the barplot in Question A.3 we notice that Tuesday had a higher proportion than expected under the null, while Saturday a lower proportion than expected under the null. ggplot(writers_table, aes(x = DayOfWeek, y = observed / nrow(writers))) + geom_col(fill = &#39;lightblue&#39;) + geom_hline(yintercept = 1/7, color = &#39;red&#39;) + labs(x = &#39;Day of the week&#39;, y = &#39;Proportion of writers&#39;) + theme_classic(base_size = 15) Pearson residuals Equivalently, you could answer by looking at Pearson residuals: gof_test$residuals ## [1] -0.8728716 1.0910895 0.2182179 0.4364358 -0.4364358 -1.3093073 0.8728716 The highest values are for Tuesday and Saturday. The value for Tuesday, 1.09, is positive. This means that the observed counts were larger than expected. In the same way we interpret the residual for Saturday, -1.31. As this value is negative, this means that the observed counts of birthdays on a Saturday were less than expected. Lab: Joking for a tip The chi-square test of independence Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer? A study published in the Journal of Applied Social Psychology6 investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France. The waiter randomly assigned coffee-ordering customers to one of three groups. When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all. The data are stored in the file TipJoke.csv, accessible via the link https://edin.ac/2U6zfgO. The variables are: Card: None, Joke, Ad. Tip: 1 = The customer left a tip, 0 = The customer did not leave tip. In the following, we will consider leaving a tip as “success”. ► Question B.1 Load the data into R and inspect it. [Hint: the data is a .csv file - think about what function you are going to use.] Pay particular attention to: the variable names; the dimensions of the tibble; the format of the data (i.e., make sure that variables are correctly encoded). ► Solution Load the data: library(tidyverse) tipjoke &lt;- read_csv(&#39;https://edin.ac/2U6zfgO&#39;, col_names = TRUE) Inspect the first six rows: head(tipjoke) ## # A tibble: 6 x 2 ## Card Tip ## &lt;chr&gt; &lt;dbl&gt; ## 1 None 1 ## 2 Joke 1 ## 3 Ad 0 ## 4 None 0 ## 5 None 1 ## 6 None 0 Check the dimensions of the tibble: dim(tipjoke) ## [1] 211 2 We have 211 observations on 2 variables. Make the variables factors and give more meaningful names to the variable Tip: tipjoke &lt;- tipjoke %&gt;% mutate(Card = factor(Card), Tip = factor(Tip, levels = c(0, 1), labels = c(&#39;NoTip&#39;, &#39;Tip&#39;))) head(tipjoke) ## # A tibble: 6 x 2 ## Card Tip ## &lt;fct&gt; &lt;fct&gt; ## 1 None Tip ## 2 Joke Tip ## 3 Ad NoTip ## 4 None NoTip ## 5 None Tip ## 6 None NoTip ► Question B.2 Identify the observational units in this study. Identify the explanatory and response variables in this study, and classify them as either categorical (also binary) or quantitative. ► Solution Observational units: The 211 adults ordering coffee at the bar of the seaside resort on the west coast of France. Explanatory variable: Type of card (if any) given to the customer. Type: Categorical but not binary as it has three categories. Response variable: Whether or not the customer left a tip. Type: Categorical and binary. ► Question B.3 Create a barplot that visually summarises the data. Comment on what the graph reveals. ► Solution ggplot(tipjoke, aes(x = Card, fill = Tip)) + geom_bar() From the stacked bar graph, it appears that perhaps the joke card does produce a higher success rate, but we need to investigate whether the differences are statistically significant. ► Question B.4 Create a two-way table showing how many customers left or not a tip for each type of card (including none given). ► Solution In R, we can construct a two-way table using the function table(): observed &lt;- table(tipjoke$Tip, tipjoke$Card) observed ## ## Ad Joke None ## NoTip 60 42 49 ## Tip 14 30 16 ► Question B.5 State, in words, the null and alternative hypotheses of this study. ► Solution The null hypothesis asserts that whether a customer left a tip or not is independent of the type of card given. The alternative hypothesis says that the two variables are associated (or related). ► Question B.6 Which test would you use to test the hypotheses stated in the previous question? ► Solution We need to use a chi-square test of independence. ► Question B.7 Compute the value of the test statistic and corresponding \\(p\\)-value. ► Solution ind_test &lt;- chisq.test(observed) ind_test ## ## Pearson&#39;s Chi-squared test ## ## data: observed ## X-squared = 9.9533, df = 2, p-value = 0.006897 The chi-square statistic in the sample is \\(\\chi^2 = 9.953\\). The \\(p\\)-value is \\(\\Pr(\\chi^2(2) \\geq 9.953) = 0.007\\). ► Question B.8 Inspect the expected frequencies. Look at the Pearson residuals and comment on what you notice. [Hint: If out &lt;- chisq.test(&lt;table&gt;), the expected counts can be obtained as out$expected and the Pearson residuals as out$residuals.] ► Solution The output of chisq.test returns many objects: names(ind_test) ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;method&quot; &quot;data.name&quot; &quot;observed&quot; &quot;expected&quot; ## [8] &quot;residuals&quot; &quot;stdres&quot; We are interested in the expected frequencies: ind_test$expected ## ## Ad Joke None ## NoTip 52.95735 51.52607 46.51659 ## Tip 21.04265 20.47393 18.48341 Recall the observed frequencies: observed ## ## Ad Joke None ## NoTip 60 42 49 ## Tip 14 30 16 More customers than expected under the null hypothesis: didn’t tip after receiving an advertising card; tipped after receiving a joke card; didn’t tip after receiving no card. Less customers than expected under the null hypothesis: tipped after receiving an advertising card; didn’t tip after receiving a joke card; tipped after receiving no card. The Pearson residuals are: ind_test$residuals ## ## Ad Joke None ## NoTip 0.9677724 -1.3270892 0.3641203 ## Tip -1.5352747 2.1052950 -0.5776407 The top three contributions to the chi-square statistic are due to: more customers than expected under the null hypothesis tipped after receiving a joke card; less customers than expected under the null hypothesis tipped after receiving an advertisement card; less customers than expected under the null hypothesis didn’t tip after receiving a joke card. ► Question B.9 Check that the assumptions of the test are satisfied. These are the conditions required for the test results to be valid. ► Solution Assumptions checks: A simple random sample is taken from a large population. Each outcome can be classified into one cell according to its category on one variable (type of card) and its category on the second variable (whether a tip was given or not). The expected frequency in each cell is 5 or greater. Hence, the conditions required for the test results to be valid are met. ► Question B.10 Write your conclusion linked to the test results and in the context of the problem. ► Solution A chi-square test of independence between tipping and card type was performed (\\(\\chi^2(2) = 9.953\\), \\(p = .007\\)). The chi-square test shows that the tip rate differs depending on the which card (Joke, Ad, or None) the waiter gave to the customer. In other words, the two variables are associated. To understand the nature of this difference in the chi-square test, we need to compare observed and expected frequencies. The observed number of tips in the joke group (30) is quite a bit higher than expected (20.47), while the observed counts for both the advertising and no card groups are lower than expected. So, if you ever find yourself taking coffee orders in a cafe of a seaside resort in the west coast of France, you probably want to learn some jokes! Summary In this lab you applied two types of statistical tests which apply to categorical variables having two or more categories. The chi-square goodness-of-fit test involves one categorical variable. It determines whether it is reasonable to assume that your sample came from a population in which, for each category, the proportion of the population that falls into the category is equal to some hypothesized proportion. The chi-square test of independence involves two categorical variables. It determines whether it is reasonable to assume that two categorical variables are independent or not in the population. If they are not independent, we also say that the two variables are associated. Both tests involve using the chi-square test statistic, which has the following form: \\[ \\chi^2 = \\sum_{all\\ cells} \\frac{(Observed - Expected)^2}{Expected} \\] The degrees of freedom are: Goodness-of-fit: df = # categories - 1; Independence: df = (# rows - 1) \\(\\times\\) (# columns - 1). Validity conditions. For the test results to be valid, the data should come from a random sample of the population of interest, and the expected counts should all be at least 5. Glossary Cell contribution. The contribution of a cell in a table to the chi-square statistic. It is helpful in determining where large differences between observed data and what would be expected if the null hypothesis were true, exist. Chi-square statistic. A standardized statistic for quantifying the distance between the observed frequencies in the sample with those expected under the null hypothesis. Expected counts. The expected frequencies for the cells of the table assuming the null hypothesis is true. Chi-square goodness-of-fit test. Are the proportions of the different categories in the population equal to the hypothesized proportions? Chi-square test of independence. Are two categorical variables independent in the population? Observed counts. The observed frequencies in the cells of the table from the study. Validity conditions for the chi-square test. All expected counts must be at least 5. References Gueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. Journal of Applied Social Psychology, 32(9), 1955-1963. Material adapted from: Rossman, A. J., &amp; Chance, B. L. (2011). Workshop statistics: discovery with data. John Wiley &amp; Sons. Cannon, A. R., Cobb, G. W., Hartlaub, B. A., Legler, J. M., Lock, R. H., Moore, T. L., … &amp; Witmer, J. (2013). Stat2: Building models for a world of data. W.H. Freeman. Tintle, N., Chance, B. L., Cobb, G. W., Rossman, A. J., Roy, S., Swanson, T., &amp; VanderStoep, J. (2015). Introduction to statistical investigations. New York: Wiley. Gueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. Journal of Applied Social Psychology, 32(9), 1955-1963.↩ "]
]
