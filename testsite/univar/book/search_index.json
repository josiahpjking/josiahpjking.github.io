[
["index.html", "Univariate Statistics and Methodology using R Overview of the Course The team R Cheatsheets R Community", " Univariate Statistics and Methodology using R Department of Psychology, University of Edinburgh 2019-2020 Overview of the Course Univariate Statistics and Methodology using R is aimed to teach (or consolidate) fundamental methodological and statistical understanding, and introduce the use of R as a powerful tool for understanding data (not just NHST). On this page you will find the weekly lab exercises, along with links to useful content online, perhaps walkthroughs. etc. Each week, the solutions will be made available here for the previous weeks’ lab. The team Lecturer &amp; Course Organiser: Professor Martin Corley martin.corley@ed.ac.uk Senior Teching Coordinators (Labs): Dr Josiah King josiah.king@ed.ac.uk R Cheatsheets You can find a collection of cheatshets that summarise some of the most useful commands you will be using for tasks such as data transformation, visualisation, RMarkdown and many others here/ Some key ones for this course are: RMarkdown Data Visualisation (ggplot2) Data transformation with dplyr Data Import R Community R has great representation in Edinburgh. Check out these pages to find out more: R Ladies Edinburgh EdinburghR And worldwide you have: R Studio Community "],
["lab-1.html", "Chapter 1 Lab 1 1.1 Let’s get going! 1.2 A simple analysis", " Chapter 1 Lab 1 Welcome to the first Univariate statistics and Methodology in R lab. The labs are designed to give you an opportunity to learn and practice working with R as well as manipulating, visualising, and analysing data. Over the next 11 weeks, you will be given various tasks that will gently guide you through the process. These labs are the best time for you to make sure you understand the material covered and keep on top of things. So please, if you have any questions, ask the lecturers or the tutors. We are all keen to help!   The aim of this first lab is to help you get started with the programming language. So, first of all, let us know if you had any installation problems with: R, the programming language core RStudio, the integrated development environment for programming in R Swirls, little interactive R lessons (not compulsory, but can be useful to practice) The installation instructions are in the “Lecture notes” tab of the course’s LEARN page, but you can also check the Navarro book, Part II, Chapter 3.1. If you have not installed R on your laptop yet, do it now. 1.1 Let’s get going! Now that you have R installed on your machine, you can practice creating your first R project and writing your own first R script. A script is simply a text file, where you write some R commands that the R engine then executes for you. Again, instructions on how to run R code along with useful tips and tricks can be found on LEARN. Do check them out! 1.1.1 RStudio Task: First of all, let’s get organised: Create a folder called “Univariate” somewhere sensible on your computer (e.g., in My Documents). You will use this folder to create R scripts and store data for these labs. It may also be a good idea to keep other documents related to this course in there. Make a choice where you want this folder to sit and stick to it. Moving it later on will cause more hassle than it’s worth so don’t do it! Task: Start RStudio. Select the Project drop-down (top right) and create a new empty project in a new directory called “Lab1” (see Figure 1) within your “Univariate” folder.   Fig. 1: Creating a new project in a directory called Lab1.   Tip: If you’re not familiar with the idea of a directory (also sometimes called a folder), read Navarro, pp. 81-84, for an explanation. If, for example, you wanted the folder to be on your Desktop, you would create the project as a subdirectory of ~/Desktop (Mac) or ~/../Desktop (PC). Don’t do it though. Instead create the project in your [whatever location]/Univariate/ folder. Pro tip: Before you do anything else, go to the View menu and zoom in or out to your satisfaction. RStudio will remember this setting when it next starts. Task: Select File &gt; New File &gt; R Script. In the window that appears, click on the floppy disk1 icon () just below Untitled1 and save the (blank) script as myscript.R Your RStudio window should now look like the one in Figure 2 (we’ve added some labels to the various windows for ease of reference). Fig. 2: myscript.R ready to edit. Projects are handy for several reasons. For instance, when you create a project, the default working directory will be set in the project’s folder2. That means that, unless you tell it otherwise, R will look in the project folder for any files you tell it to access. Projects are also easily connected with version control software (e.g., Git) and are great for collaborative scripts and documents. Additionally, R can be configured to have different default settings for each project which can be very useful. These topics may be somewhat advanced but it is a good idea to get into the habit of using RStudio projects. We strongly recommend you create a separate project within the Univariate folder for each lab. 1.1.2 Console The “heart of R” is the Console window. This is where instructions are sent to R, and its responses are given. The console is, almost exclusively, the way of talking to R in RStudio. Task: Click on the console and type 1 + 1 (then hit ↵Enter). You should see something like the following: &gt; 1 + 1 ## [1] 2 The Information area (all of the right-hand side of RStudio) shows you useful information about the state of your project. At the moment, you can see some relevant files in the bottom pane, and an empty “Global Environment” at the top. The global environment is a virtual storage of all objects you create in R. So, for example, if you read in some data into R (more on that later), this is where they will be put and where R will look for them if you tell it to manipulate or analyse the data. Task: Click on the console and type x &lt;- 1 + 1 and see what happens. Look at the Environment tab, in particular. As you can see, the tab now contains a “Values” section and, within it, there is a x and it’s associated value of 2 (the result of the 1 + 1 operation). What happened here is that you told R to run the 1 + 1 command and store its result inside an object labelled x. In, other words, you assigned the value resulting from the 1 + 1 command to the object x using the assignment operator &lt;- (think of it as an arrow pointing the result of the right-hand side command to the left-hand side object). Task: Hit Ctrl + ↑ (⌘ + ↑ on Mac). Highlight the x &lt;- 1 + 1 line and press ↵Enter (this will rewrite it to the console). Edit it so that it reads x &lt;- x + 1. What will this do? Press ↵Enter to confirm your guess. If you look in at the Environment tab, you will see that x now stores the value of 3. Though it may look like the x object simply got incremented by 1, let not that deceive you. What R really did was it took whatever was inside the x object and added 1 to it, storing the result of the operation in a different object. It then took the x label from the old object and put it on the new one, immediately forgetting all about the original object. It may seem to you like a distinction without a difference but, as will become clear later on, it is an important one. Task: Try running the command y &lt;- c(1, 2, 3, 4, 5). What happens? What kind of thing is y? Anything you put on the right-hand side of the &lt;- must be a valid R command and, as such, must be able run on its own. So, if you’re not sure what c(1, 2, 3, 4, 5) does, just type it into the console and press ↵Enter… As you can see R returned a vector of numbers. A vector is just a fancy name for an ordered sequence of elements. In this case, since the elements are all numbers, the result is a numeric vector of five numbers (1-5). An important habit to get into is to think of commands in terms of their output and of objects in terms of their content. If someone asks you what the command c(1, 2, 3, 4, 5) does, you should say: “It returns a numeric vector of length 5 with values 1, 2, 3, 4, and 5.” Similarly, the answer to the question what is y should be the similar: “An object containing a numeric vector of length 5 with values 1, 2, 3, 4, and 5.” This mode of thinking and talking about commands and objects is not self-serving and will become crucial later on, when you are doing more complex things3 in R. Task: Try y &lt;- y * 2 and guess what will happen before you press ↵Enter. Task: Type y followed by ↵Enter. This is one way you can get R to print out a value into the console. As you can see, each element of y was multiplied by 2. This means, that addition (as well as other arithmetic operations) in R is a vectorised function - it gets applied to each element of a vector. Task: You can also perform arithmetic operations on two vectors. Try running the command y / c(1, 2, 3, 4, 5). Here we performed a series of divisions: the first element of y (2) was divided by the first element of the vector c(1, 2, 3, 4, 5) (1), then the second elements of these vectors (4 and 2, respectively) were used for the division, and so on. The result is a numeric vector of length 5 containing five twos. If the vectors given to such a vectorised function differ in length, the shorter vector gets recycled Task: Try y + c(2, 3) to see how that works. Right, so we added 2 to the first element of y, 3 to the 2nd, 2 again to the 3rd element, then 3 to the 4th and, once again, 2 to the last element of y. The ever so helpful R also warned us that the length of the longer of the two added vectors isn’t a whole number multiple of the shorter one and so the recycling was not complete. Being able to perform all sorts of calculations and manipulations of objects is crucial for data processing and analysis. For instance, you might need to calculate a total score based on certain columns of your data representing items on a questionnaire. Some of these items might also be reverse-scored, meaning that you need to flip the values of that variable around4. Imagine, you have such a variable - a numeric vector of numbers between 1 and 5. Reverse-scoring means, that you want to turn 1s into 5s, 2s into 4, 4s into 2s, and 5s into 1s (3s stay as they are). The easiest, and most elegant, way of doing this is simply subtracting this vector from 6. Try it out by hand on a few numbers to convince yourself that this does indeed do the trick.   Moving on… Task: Try plot(y, y + 3). You should see a simple scatterplot appear in the bottom right panel, where information is shown. A scatterplot is a type of graph that shows a relationship between two numeric variables. Here, the x axis represents the y variable and the y axis shows y + 3. Both y and y + 3 are numeric vectors (remember, think about commands in terms of their result!). Imagine them side by side as two columns, like this: ## y y + 3 ## [1,] 2 5 ## [2,] 4 7 ## [3,] 6 9 ## [4,] 8 11 ## [5,] 10 13 Each point on the plot represents one row of this table, e.g., the point in the bottom left corner is the first row with a y value of 2 and y + 3 value of 5. The plot shows a positive relationship: as y gets larger, so does y + 3. Moreover, the relationship is perfectly linear (in case you’re wondering, yes, it is obvious). Note that RStudio has automatically switched tab to show the Plots tab, since you’ve just created a plot. Previously, it showed the Files tab. Finally, the Packages tab lists all the packages that you have installed and are available to be loaded using the library() command. Tip: Click on Zoom to see your plot on a larger scale. Remember to close the pop-up window once you’re done admiring the plot. Task: Type ?plot into the Console. What happens? This is the generic way of getting help on any function (such as plot()). R help can range from very useful to quite esoteric; once you’re familiar with R, the help will become more intelligible. In the meantime, the Web is often your friend. An aside on functions Every operation on a value, vector, object, and other structures in R is done using functions. You’ve already used several: c() is a function, as are &lt;-, +, -, *, etc. A function is a special kind of object that takes some arguments and uses them to run the code contained inside of it. Every function has the form function.name()5 and the arguments are given inside the brackets. Arguments must be separated with a comma. Different functions take different number of various arguments. The help file brought up by the ?... gives a list of arguments used by the given function along with the description of what the arguments do and what legal values they take. In the help file for plot(), we can see that, apart from the x and y coordinates of the points to plot, the function takes an entire host of graphical parameters. For instance, if we add the main = argument and give it a legal value (i.e., a string of characters, such as &quot;My first plot&quot;), the value will be used as a title of the plot. It is crucial to understand what arguments can be accepted by the functions you use and what the legal values of these arguments are. Functions are extremely useful and you will learn how to write your own later.   The next exercise is a little harder: Imagine you like your scatterplot but you don’t like the open circles it uses to mark each point. Instead, you want to use filled squares. Task: Open a web browser, and search the web for something like “change shape of points in R plot”. Using the links returned, see if you can reproduce the plot in Task 12 but make the points come out as filled squares. If that turns out to be easy, try red filled squares. Then try each square a different colour. Tip: There are a lot of ways to do this! The simplest is to look for a graphical parameter to do with plot characters and add the appropriate argument to your plot() command. Often a picture is worth a thousand words. A good plot will communicate important information in a clear and concise way both to you as the analyst and your intended audience. Data visualisation is one of R’s main strengths. 1.1.3 Editor So far you’ve been entering simple instructions into the Console. But this can be a pain, for two reasons: As you change commands like plot(), it would be nice to be able to edit what you have already typed in. It would be good to keep a record of what you’ve done, so that you can do it again at a later stage, perhaps with changes. Instead of using the console, you can use the Editor to type commands. The Editor is used to write your script that then gets interpreted by R in a line by line fashion. Since the script is basically a plain text file (with some nice colours added by RStudio to improve readability) the commands you type in can be edited, added, or deleted just like if you were writing an ordinary document. You can run them again later, or build up complex commands and functions over several lines of text. Task: In the Editor, type x &lt;- seq(-3.5, 3.5) and hit ↵Enter. What happens? Task: Click on the line you just typed in, and hit Ctrl + ↵Enter (⌘ + ↵Enter if you’re on a Mac). What happens now? This is an important practical distinction between the Console and the Editor: In the Console, ↵Enter runs the command. In the Editor, it just adds a new line. The purpose of this is to facilitate writing scripts without running each line of code. It also enables you to beak down your commands over multiple lines so that you don’t end up with a line that’s hundreds of characters long. For example: poisson_model &lt;- glm( # R knows that an open bracket can&#39;t be the end of command... n_events ~ gender + scale(age) + scale(n_children) + # ...nor can a plus... I(SES - min(SES)) * scale(years_emp, , F), # ...or a comma df, family = &quot;poisson&quot;) # closing bracket CAN be the end Task: What does x become? What does seq() do? Since seq is followed by a set of brackets, you know it’s a function. Whenever you encounter an unknown function, it is a good idea to look at its documentation by calling the ?... command. Task: Add the following lines to the script in the Editor: y &lt;- dnorm(x) # plot normal distribution plot(x, y, type = &#39;l&#39;) Task: Click on y &lt;- dnorm(x), and hit Ctrl + ↵Enter (⌘ + ↵Enter). Press the keys twice more to pass the other lines to the console. What does # do? What is the end result? The hash (#) marks everything to the right of it as comment. Comments are useful for annotating the code so that you can remember what it means when you return to your code months later (it will happen!). It also improves code readability if you’re working on a script in collaboration with others. Comments should be clear but also concise. There is no point in paragraphs of verbose commentary. You might not be very satisfied with your end result (not a very curvy curve!). This is probably because we calculated values of the normal distribution (in y) for too few values of x. Maybe we can fix that… Task: Change the first line of your script in the editor so that it reads: x &lt;- seq(-3.5, 3.5, length.out = 49) Task: select your entire script using the mouse (or Ctrl + A), and press Ctrl + ↵Enter (⌘ + Enter). What happens? Look both at the plot and at the Global Environment above it. Task: change the last line of your script so that it reads: # that&#39;s a small L, not a one plot(x, y, type = &#39;l&#39;, lwd = 3, lty = 2, col = &#39;red&#39;) Task: Create a new plot. Do you need to run just the plot() line or the whole script? Why? What does the plot look like now? Since the edited plot(...) line has the same first two arguments (x and y) as the version before, and since these two variables have already been created, there is no need to re-run anything but this particular line. Task: Advanced task: Using www.rseek.org and/or R help, try and edit your plot command to produce a plot similar to that in Fig. 3, including the title and axis labels x &lt;- seq(-3.5, 3.5, length.out = 49) # notice how a long line of code can be wrapped over several lines # in the editor to improve readability. plot(x, dnorm(x), main = &#39;The Normal Distribution&#39;, ylab = &#39;Density&#39;, xlab = &#39;SDs from the Mean&#39;, lwd = 4, col = &#39;red&#39;, type = &#39;l&#39;) Figure 1.1: Fig. 3: Distribution produced using seq(), dnorm(), and plot(). Task: Save myscript.R (click on the disk icon). Writing (and saving) scripts has just too many advantages over coding in the console to list and it it is crucial that you learn how to do it. It will enable you to write reproducible code you can rerun whenever needed, reuse chunks of code you created for a previous project in your analysis, and, when you realise you made a mistake somewhere (when, not if, because this too will happen!), you’ll be able to edit the code and recreate your analysis in a small fraction of the time it would take you to analyse your data anew without a script. 1.2 A simple analysis The point of this section is to get you used to playing with R, including loading, creating, plotting, and analysing data. What follows isn’t necessarily meaningful or the ‘best’ way of doing things; all of the numbers are made up and the techniques used are a bit cumbersome. They should, however, give you a flavour of some of the things you can do. Don’t worry if you’re not familiar with all of the statistical concepts yet (although many of you will be); these will all be explained as the course progresses. Task: Open a new blank script; save it as iq.R (see Task 2) Task: Type in the following and execute the commands: ## compare iqs library(foreign) df &lt;- read.spss(&#39;http://is.gd/iqs_spss&#39;, to.data.frame = TRUE) ## re-encoding from CP1252 Task: You should see a new variable df in the Global environment. Click on this. You should get something like figure 4. Fig. 4: Viewing the data in df. df is some data in a data frame, roughly equivalent to an Excel spreadsheet or worksheet in SPSS. As you’ve seen, you can inspect it by clicking it in the Global Environment, but it’s not editable (through point and click); we’ll get on to editing etc., later in the course. The two lines of script you’ve run have done a huge amount of work, in fact. They’ve connected to the internet and downloaded an SPSS .sav data file, and converted it to R format. If you look at the Console, you will see some (harmless) warnings reflecting the fact that the data was created in a recent version of SPSS. The data is supposed to represent the IQs of a particular football team, Fulchester United (11 players plus one substitute). We’d like to know whether Fulchester’s mean IQ is higher than what you might expect from a totally ‘average’ football team, drawn indiscriminately from the general population. Task: Type the following in to iq.R (you might need to click on the tab first): # draw 12 random IQs from the `general population&#39; # assuming mean population IQ is 100 and SD is 15 iqs &lt;- rnorm(12, mean = 100, sd = 15) Task: What does iqs contain? (You might want to type iqs into the Console to see all of the values without affecting your script) Task: Type summary(iqs) into the Console for some useful further information Task: look at your neighbour’s Console, if they’re at roughly the same stage as you. Why is their output for summary(iqs) different? Task: Now add the following lines to your script and run them # create new data frame with labels df2 &lt;- data.frame(ID = LETTERS[13:24], IQ = iqs) Task: Look at df2 and see if you can work out what you’ve created Pro tip: LETTERS[13:24] is an example of subscripting. Go the console and type LETTERS. What do you get? On the next line, type 13:24 and see what that returns. Now run LETTERS[13:24]. Can you see what it’s doing? Try LETTERS[c(13, 1, 18, 20, 9, 14)] Task: Type the following in to the iq.R script (rbind() binds by rows) and run it: # bind the two data frames together df &lt;- rbind(df, df2) Task: What has happened to df? (If you want to click on df but the tab is already open, close the tab first) Note that df contains all of the IQs in one column. We need to add another column to index which sample the IQs come from (called a factor in R). Task: Add the following to your script and run it: # add a factor df$source &lt;- gl(2, 12, labels = c(&#39;Fulchester&#39;, &#39;Random&#39;)) gl() stands for ‘generate levels’ (of a factor). The first number gives the number of levels; the second, how many times each level is repeated; the labels = argument allows us to call the levels something more useful than c(1, 2) (which would be the default). Task: Look at df again (you might need to close the tab and reopen it). What have you just added? It’s time to do some statistics! This is a very simple test, and the results will vary depending on the random sample you created earlier, but let’s compare the Fulchester sample with our random sample: Task: Add the following to your script and run it: Tip: IQ ~ source can be read as “IQ is predicted by source”. There are other ways of specifying a t-test but this way makes it clear what you are expecting to affect what # run a t-test t.test(IQ ~ source, data = df) ## ## Welch Two Sample t-test ## ## data: IQ by source ## t = 4.2322, df = 19.393, p-value = 0.0004335 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 9.784088 28.878328 ## sample estimates: ## mean in group Fulchester mean in group Random ## 113.41564 94.08444 Task: The output of the t-test should be in the Console below. What can you say about your totally fake data? 1.2.1 Last Things Task: Exit the project (Lab1 &gt; Close Project, save anything when asked) You should get an empty RStudio. You can create a different project for different work (using the Project drop-down), or you can re-open Lab1 and you should be exactly where you were before. When you restart RStudio next time, you can go straight to the Projects tab to open an existing project (RStudio will remember where the relevant files are), or you can create a brand new project in a new directory (and switch between projects at will at any time).   1.2.1.1 That’s it for today! We would really appreciate you feedback on this and future labs and on whether you feel you learnt anything. If the plot disappears after a split-second, just click inside the RStudio plotting window and it should pop back up.↩ You can have R print out the path to the current working directory by running the command getwd().↩ For example: df$total_score &lt;- rowMeans(df[ , grep(&quot;item&quot;, names(df))], na.rm = T)↩ Including such items is considered important good practice in psychometrics. For example, when measuring extraversion on a scale where low scores indicate introversion and high scores indicate extraversion, you may ask your participants to express their agreement with given statements on a scale from 1 (“Strongly disagree”) to 5 (“Strongly agree”). If one of the statements is, let’s say, “Usually, I would rather stay at home and read a book than go to a wild party.”, the participants’ scores on this item need to be reversed because strong extraverts will score low and strong introverts will get high scores.↩ Some functions have an additional infix form: x &lt;- 1 + 2 is just a shorthand for the basic (prefix) form `&lt;-`(x, `+`(1, 2)).↩ "],
["lab-2.html", "Chapter 2 Lab 2 2.1 First things first 2.2 Classes 2.3 Data structures 2.4 Replacement", " Chapter 2 Lab 2 Today, you will learn about basic data structures in R and how to manipulate them. Before we kick off, there’s one thing we’d like to address. Some people feel reluctant to play around with R for fear of breaking something. This is quite normal, especially if they don’t really understand how R works yet. If this is your case, we’d like to reassure you that there’s nothing you can do by typing in the script or the console that will cause anything to explode (unless you really know what you’re doing and you set out to cause problems). So, be curious, play around, try things out, look up other people’s code (from trusted sources), break it down, try to understand it. That’s the best way to learn! 2.1 First things first Task: Create a new RStudio project in a new directory called “Lab2” within your “Univariate” folder. Last week, we discussed objects in R. We talked about how basically everything in R is an object and how we can assign some content to an object using the &lt;- assignment operator. This week, we will talk about the different kinds (classes) of content and the different ways this content can be organised (data structures). 2.2 Classes All data in R are organised in some structures. We’ve already encountered one of them – vectors – but there are other types we will talk about in due course. First let’s focus on what these structures are made up of – elements. An element is a single number, boolean (TRUE/FALSE), or a character string (anything in “quotes”). Elements come in several classes: &quot;numeric&quot;, as the name suggests, a numeric element is a single number: 1, 2, -725, 3.14159265, etc.. A numeric element is never in ‘single’ or “double” quotes! Numbers are cool because you can do a lot of maths (and stats!) with them.   &quot;character&quot;, a string of characters, no matter how long. It can be a single letter, 'g', but it can equally well be a sentence, &quot;Elen s?la lumenn' omentielvo.&quot; (if you want the string to contain any single quotes, use double quotes to surround the string with and vice versa). Notice that character strings in R are always in ‘single’ or “double” quotes. Conversely anything in quotes is a character string: class(3) ## [1] &quot;numeric&quot; Task: Try running the same command, just put the 3 in quotes. class(&quot;3&quot;) # in quotes, therefore character! ## [1] &quot;character&quot; Task: It stands to reason that you can’t do any maths with cahracter strings, not even if it’s a number that’s inside the quotes! Try adding &quot;3&quot; and &quot;2&quot;. &quot;3&quot; + &quot;2&quot; ## Error in &quot;3&quot; + &quot;2&quot;: non-numeric argument to binary operator   The error message R gives you is a little cryptic so let’s decode it. “Binary operator” refers to the addition. Other binary operators are, e.g., *, ^, or %% (modulo). So R is telling you that you are trying to perform an arithmetic operation on non-numbers. Why does it not just say so like a normal person? No idea! You are most likely to get this error when you try to calculate something from your data (e.g., a total score) but one of the variables is actually not numeric for some reason. Maybe the data weren’t read in right or you’ve selected the wrong column, etc.   &quot;logical&quot;, a logical element can take one of two values, TRUE or FALSE. Logicals are usually the output of logical operations (anything that can be phrased as a yes/no question, e.g., is x equal to y?). In formal logic, TRUE is represented as 1 and FALSE as 0. This is also the case in R: # recall that c() is used to bind elements into a vector # (that&#39;s just a fancy term for an ordered group of elements) class(c(TRUE, FALSE)) ## [1] &quot;logical&quot; Task: We can force (‘coerce’, in R jargon) the vector to be numeric. Try doing that using the as.numeric() function instead of class(). as.numeric(c(TRUE, FALSE)) ## [1] 1 0 This has interesting implications. First, is you have a logical vector of many TRUEs and FALSEs, you can quickly count the number of TRUEs by just taking the sum of the vector: # consider vector of 50 logicals x ## [1] TRUE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [12] TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE ## [23] FALSE TRUE TRUE TRUE FALSE TRUE TRUE FALSE TRUE TRUE TRUE ## [34] TRUE FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE FALSE ## [45] TRUE FALSE TRUE TRUE TRUE FALSE # number of TRUEs sum(x) ## [1] 30 # number of FALSEs is 50 minus number of TRUEs length(x) - sum(x) ## [1] 20 Second, you can perform all sorts of arithmetic operations on logicals: # TRUE/FALSE can be shortened to T/F T + T ## [1] 2 F - T ## [1] -1 (T * T) + F ## [1] 1 Third, you can coerce numeric elements to valid logicals: # zero is FALSE as.logical(0) ## [1] FALSE # everything else is TRUE as.logical(c(-1, 1, 12, -231.3525)) ## [1] TRUE TRUE TRUE TRUE Now, you may wonder that use this can possible be?! Well, this way you can perform basic logical operations, such as AND, OR, and XOR (see Lecture 2): # x * y is equivalent to x AND y as.logical(T * T) ## [1] TRUE as.logical(T * F) ## [1] FALSE as.logical(F * T) ## [1] FALSE as.logical(F * F) ## [1] FALSE # x + y is equivalent to x OR y as.logical(T + T) ## [1] TRUE as.logical(T + F) ## [1] TRUE as.logical(F + T) ## [1] TRUE as.logical(F + F) ## [1] FALSE # x - y is equivalent to x XOR y (eXclusive OR, either-or) as.logical(T - T) ## [1] FALSE as.logical(T - F) ## [1] TRUE as.logical(F - T) ## [1] TRUE as.logical(F - F) ## [1] FALSE   &quot;factor&quot;, factors are a bit weird. They are used mainly for telling R that a vector represents a categorical variable. For instance, you can be comparing two groups, treatment and control. # create a vector of 15 &quot;control&quot;s and 15 &quot;treatment&quot;s x &lt;- rep(c(&quot;control&quot;, &quot;treatment&quot;), each = 15) x ## [1] &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; ## [6] &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; ## [11] &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; ## [16] &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; ## [21] &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; ## [26] &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; &quot;treatment&quot; Task: Run the code above. Don’t copy-paste, type it in manually. It will help you learn the syntax. Now, turn x into a factor using factor() or as.factor(). # turn x into a factor x &lt;- as.factor(x) x ## [1] control control control control control control control ## [8] control control control control control control control ## [15] control treatment treatment treatment treatment treatment treatment ## [22] treatment treatment treatment treatment treatment treatment treatment ## [29] treatment treatment ## Levels: control treatment The first thing to notice is the line under the last printout that says “Levels: control treatment”. This informs you that x is now a factor with two levels (or, a categorical variable with two categories). Second thing you should take note of is that the words control and treatment don’t have quotes around them. This is another way R uses to tell you this is a factor. With factors, it is important to understand how they are represented in R. Despite, what they look like, under the hood, they are numbers. A one-level factor is a vector of 1s, a two-level factor is a vector of 1s and 2s, a n-level factor is a vector of 1s, 2s, 3s … ns. The levels, in our case control and treatment, are just labels attached to the 1s and 2s. Let’s demonstrate this: typeof(x) ## [1] &quot;integer&quot; # integer is fancy for &quot;whole number&quot; Task: Try coercing x to numeric and see what happens. Don’t assign the result to x, just have it prited out. as.numeric(x) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Task: Now look at the levels() of x. levels(x) ## [1] &quot;control&quot; &quot;treatment&quot; The labels attached to the numbers in a factor can be whatever. Let’s say that in your raw data file, treatment group is coded as 1 and control group is coded as 0. # create a vector of 15 zeros and 15 ones x &lt;- rep(0:1, each = 15) x ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # turn x into a factor x &lt;- as.factor(x) x ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## Levels: 0 1 Since x is now a factor with levels 0 and 1, we know that it is stored in R as a vector of 1s and 2s and the zeros and ones, representing the groups, are only labels: as.numeric(x) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 levels(x) ## [1] &quot;0&quot; &quot;1&quot; The fact that factors in R are represented as labelled integers has interesting implications. First, certain functions will coerce factors into numeric vectors which can shake things up. cbind() which binds its arguments by column is one such function: x ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## Levels: 0 1 # let&#39;s bind the first 15 elements and the last 15 elements together as columns Task: Try cbind()-ing the first 15 and the last 15 elements of x into two columns. Hint: x[1:15] gives you the first 15 elements. # printout truncated to first 5 rows to save space cbind(x[1:15], x[16:30]) ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 2 ## [3,] 1 2 ## [4,] 1 2 ## [5,] 1 2 ## [ reached getOption(&quot;max.print&quot;) -- omitted 10 rows ] cbind() binds the vectors you provide into the columns of a matrix. Since matrices (yep, that’s the plural of ‘matrix’; also, more on matrices later) can only contain logical, numeric, and character elements, the cbind() function coerces the elements of the x factor (haha, the X-factor) into numeric, stripping the labels and leaving only 1s and 2s. The other two consequences of this labelled numbers system stem from the way the labels are stored. Every R object comes with a list of so called attributes attached to it. These are basically information about the object. For objects of class factor, the attributes include its levels (or the labels attached to the numbers) and class: attributes(x) ## $levels ## [1] &quot;0&quot; &quot;1&quot; ## ## $class ## [1] &quot;factor&quot; So the labels are stored separately of the actual elements. This means, that even if you delete some of the numbers, the labels stay the same. Let’s demonstrate this implication on the plot() function. This function is smart enough to know that if you give it a factor it should plot it using a bar chart, and not a histogram or a scatter plot (more on plots next week). Task: Try plotting x with the plot() function. The result should be something like this: plot(x) Task: Now, let’s take the first 15 elements of x, which are all 0s, store them in y and plot them: y &lt;- x[1:15] plot(y) As you can see, even though our new object y only includes 0s, the levels attribute still tells R that this is a factor of (at least potentially) two levels: &quot;0&quot; and &quot;1&quot; and so plot() leaves a room for the 1s. The last consequence is directly related to this. Since the levels of an object of class factor are stored as its attributes, any additional values put inside the objects will be invalid and turned into NAs (R will warn us of this). In other words, you can only add those values that are among the ones produced by levels() to an object of class factor: Task: Try adding invalid values -4 and 3 to the end of vector x (that’s x[31:32]) and then print out x to see what happened. x[31:32] &lt;- c(-4, 3) ## Warning in `[&lt;-.factor`(`*tmp*`, 31:32, value = c(-4, 3)): invalid factor ## level, NA generated x ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [15] 0 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [29] 1 1 &lt;NA&gt; &lt;NA&gt; ## Levels: 0 1 As you can see, R sort of goes with it but warns you that you are adding values that are not valid levels of the factor. Warnings are different from errors in that the commands still get executed but the outcome might not be what you want it to be. You need to be sure you understand the behaviour in order to judge whether or not this is a problem in any particular situation. The only way to add these values to a factor is to first coerce it to numeric, then add the values, and then turn it back into factor: # coerce x to numeric x &lt;- as.numeric(x[1:30]) class(x) ## [1] &quot;numeric&quot; # but remember that 0s and 1s are now 1s and 2s! x ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # so subtract 1 to make the values 0s and 1s again x &lt;- x - 1 # add the new values x &lt;- c(x, -4, 3) # back into fractor x &lt;- as.factor(x) x ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 ## [24] 1 1 1 1 1 1 1 -4 3 ## Levels: -4 0 1 3 # SUCCESS! # reset x &lt;- as.factor(rep(0:1, each = 15)) # one-liner x &lt;- as.factor(c(as.numeric(x[1:30]) - 1, -4, 3)) x ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 ## [24] 1 1 1 1 1 1 1 -4 3 ## Levels: -4 0 1 3 Told you factors were weird…   &quot;ordered&quot;, finally, these are the same as factors but, in addition to having levels, these levels are ordered and thus allow comparison (notice the Levels: 0 &lt; 1 below): # create an ordered x x &lt;- as.ordered(rep(0:1, each = 15)) x ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## Levels: 0 &lt; 1 # we can now compare the levels x[1] &lt; x[30] ## [1] TRUE # this is not the case with factors y &lt;- as.factor(rep(0:1, each = 15)) y[1] &lt; y[30] ## Warning in Ops.factor(y[1], y[30]): &#39;&lt;&#39; not meaningful for factors ## [1] NA Objects of class ordered are useful for storing ordinal variables, e.g., age group.   In addition to these five sorts of elements, there are three special wee snowflakes: NA, stands for “not applicable” and is used for missing data. Unlike other kinds of elements, it can be bound into a vector along with elements of any class. NaN, stands for “not a number”. It is technically of class numeric but only occurs as the output of invalid mathematical operations, such as dividing zero by zero or taking a square root of a negative number: 0 / 0 ## [1] NaN sqrt(-12) ## Warning in sqrt(-12): NaNs produced ## [1] NaN Inf (or -Inf), infinity. Reserved for division of a non-zero number by zero (no, it’s not technically right): 235 / 0 ## [1] Inf -85.123 / 0 ## [1] -Inf As you can see, understanding how R represents and treats different classes of data is crucial for data processing and analysis as well as avoiding potential pitfalls. 2.3 Data structures   So that’s most of what you need to know about elements. Let’s talk about putting elements together. As mentioned above, elements can be grouped in various data structures. These differ in the ways in which they arrange elements: vectors arrange elements in a line. they don’t have dimensions and can only contain elements of same class (e.g., &quot;numeric&quot;, &quot;character&quot;, &quot;logical&quot;). # a vector letters[5:15] ## [1] &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; If you try to force elements of different classes to a single vector, they will all be converted to the most complex class. The order of complexity, from least to most complex, is: logical, numeric, and character. Elements of class factor and ordered cannot be meaningfully bound in a vector with other classes (nor with each other): they either get converted to numeric, character - if you’re lucky - or to NA. # c(logical, numeric) results in numeric x &lt;- c(T, F, 1:6) x ## [1] 1 0 1 2 3 4 5 6 class(x) ## [1] &quot;integer&quot; # integer is like numeric but only for whole numbers to save computer memory # adding character results in character x &lt;- c(x, &quot;foo&quot;) # the numbers 1-6 are not numeric any more! x ## [1] &quot;1&quot; &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;foo&quot; Task: Verify that this is indeed the case by looking at class of x. class(x) ## [1] &quot;character&quot;   matrices arrange elements in a square/rectangle, i.e., a two-dimensional arrangement of rows and columns. They can also only accommodate elements of the same class and cannot store attributes of elements. That means, you can’t use them to store (ordered) factors. # a matrix matrix(sample(20), ncol = 5) # must be square/rectangular ## [,1] [,2] [,3] [,4] [,5] ## [1,] 7 1 9 16 10 ## [2,] 18 15 6 20 12 ## [3,] 14 13 3 8 2 ## [4,] 4 11 5 17 19 # not suitable for factors x &lt;- factor(sample(10, 20, replace = T)) x ## [1] 6 10 7 8 3 4 1 2 6 2 2 1 5 1 8 4 10 10 6 9 ## Levels: 1 2 3 4 5 6 7 8 9 10 Task: Create a new factor x using the command above and put it in a matrix with 5 columns to see what happens. # not factors any more! matrix(x, ncol = 5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;6&quot; &quot;3&quot; &quot;6&quot; &quot;5&quot; &quot;10&quot; ## [2,] &quot;10&quot; &quot;4&quot; &quot;2&quot; &quot;1&quot; &quot;10&quot; ## [3,] &quot;7&quot; &quot;1&quot; &quot;2&quot; &quot;8&quot; &quot;6&quot; ## [4,] &quot;8&quot; &quot;2&quot; &quot;1&quot; &quot;4&quot; &quot;9&quot;   lists arrange elements in a collection of vectors or other data structures. Different vectors/structures can be of different lengths and contain elements of different classes. Elements of lists and, by extension, data frames can be accessed using the $ operator, provided we gave them names. # a list my_list &lt;- list( # 1st element of list is a numeric matrix A = matrix(rnorm(20, 1, 1), ncol = 5), # 2nd element is a character vector B = letters[1:5], # third is a data.frame C = data.frame(x = c(1:3), y = LETTERS[1:3]) ) my_list ## $A ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.370343 1.7627046 1.1591492 0.9500066 0.3882741 ## [2,] 2.039706 1.7531991 -0.8776922 1.7634308 1.1053965 ## [3,] 1.490569 0.1393050 3.1276867 0.3352242 1.0815455 ## [4,] 1.309309 0.5922975 2.0394034 -0.5445432 0.8034688 ## ## $B ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; ## ## $C ## x y ## 1 1 A ## 2 2 B ## 3 3 C # we can use the $ operator to access NAMED elements of lists my_list$B ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; # this is also true for data frames my_list$C$x ## [1] 1 2 3 # but not for vectors or matrices my_list$A$1 ## Error: &lt;text&gt;:2:11: unexpected numeric constant ## 1: # but not for vectors or matrices ## 2: my_list$A$1 ## ^   data frames are lists but have an additional constraint: all the vectors of a data.frame must be of the same length. That is the reasons why your datasets are always rectangular. Task: Create a data frame called df that looks like the one from Lecture 2. The id and dpt variables should be factors, degree should be ordered, and age should be numberic. Hint: The function that creates a data frame is data.frame() and is used the same way as list(), e.g., data.frame(variable1_name = vector1, variable2_name = vector2...). The data frame should look like this: df &lt;- data.frame(id = factor(1001:1006), age = c(23, 25, 33, 26, 25, 24), eye_col = factor(c(&quot;blue&quot;, &quot;brown&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;brown&quot;)), dpt = factor(c(&quot;psy&quot;, &quot;cog&quot;, &quot;lel&quot;, &quot;psy&quot;, &quot;phi&quot;, &quot;lel&quot;)), degree = ordered(c(3, 1, 1, 1, 2, 2), labels = c(&quot;1&quot;, &quot;2:1&quot;, &quot;2:2&quot;))) df ## id age eye_col dpt degree ## 1 1001 23 blue psy 2:2 ## 2 1002 25 brown cog 1 ## 3 1003 33 brown lel 1 ## 4 1004 26 green psy 1 ## 5 1005 25 blue phi 2:1 ## 6 1006 24 brown lel 2:1   Different data structures are useful for different things: data frames are like spreadsheets – each row represents an observation and each column represents a variable in your data set. Vectors are often used as representations of a single variable or observation. Matrices are the result of operations such as cbind(), rbind(), and others and lists are good for intermediate steps in complex data processing and, importantly, the output of many statistical methods in R. No matter the structure, though, bear in mind that, ultimately, they are all just bunches of elements. This understanding is crucial for working with data.   2.3.1 Subsetting Now that you understand that all data boil down to elements, let’s look at how to ask R for the subsets elements you want. There are only two ways to do this: indices logical vector Let’s take a closer look at indexing now. We’ll save logical vector subsetting for next week. Indexing is a way of asking for elements of a data structure by simply providing the numeric positions of the desired elements in the structure (vector, list…) in a set of square brackets [] at the end of the object name: x &lt;- c(&quot;I&quot;, &quot; &quot;, &quot;l&quot;, &quot;o&quot;, &quot;v&quot;, &quot;e&quot;, &quot; &quot;, &quot;R&quot;) # get the 6th element x[6] ## [1] &quot;e&quot;   To get more than just one element at a time, you need to provide a vector of indices. For instance, to get the elements 3-6 of x, we can do: x[3:6] ## [1] &quot;l&quot; &quot;o&quot; &quot;v&quot; &quot;e&quot; This is equivalent to inserting the vector c(3, 4, 5, 6) inside the square brackets. Task: Try doing that. First create the x vector as above and then subset it using c(3, 4, 5, 6). x[c(3, 4, 5, 6)] ## [1] &quot;l&quot; &quot;o&quot; &quot;v&quot; &quot;e&quot;   Remember that some structures can contain as their elements other structures. For example asking for the first element of my_list will return: my_list[1] ## $A ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.370343 1.7627046 1.1591492 0.9500066 0.3882741 ## [2,] 2.039706 1.7531991 -0.8776922 1.7634308 1.1053965 ## [3,] 1.490569 0.1393050 3.1276867 0.3352242 1.0815455 ## [4,] 1.309309 0.5922975 2.0394034 -0.5445432 0.8034688   The $A at the top of the output indicates that we have accessed the element A of my_list but not really accessed the matrix itself. Thus, at this stage, we wouldn’t be able to ask for its elements. To access the matrix contained in my_list$A, we need to write either exactly that, or use double brackets: my_list[[1]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.370343 1.7627046 1.1591492 0.9500066 0.3882741 ## [2,] 2.039706 1.7531991 -0.8776922 1.7634308 1.1053965 ## [3,] 1.490569 0.1393050 3.1276867 0.3352242 1.0815455 ## [4,] 1.309309 0.5922975 2.0394034 -0.5445432 0.8034688 # with the $A now gone from output, we can access the matrix itself my_list[[1]][1] ## [1] 2.370343   As discussed above, some data structures are dimensionless (vectors, lists), while others are arranged in n-dimensional rectangles (where n &gt; 1). When indexing/subsetting elements of dimensional structures, we need to provide coordinates of the elements for each dimension. This is done by providing n numbers or vectors in the []s separated by a comma. A matrix, for instance has 2 dimensions, rows and columns. The first number/vector in the []s represents rows and the second columns. Leaving either position blank will return all rows/columns: mat &lt;- matrix(LETTERS[1:20], ncol = 5) mat ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;A&quot; &quot;E&quot; &quot;I&quot; &quot;M&quot; &quot;Q&quot; ## [2,] &quot;B&quot; &quot;F&quot; &quot;J&quot; &quot;N&quot; &quot;R&quot; ## [3,] &quot;C&quot; &quot;G&quot; &quot;K&quot; &quot;O&quot; &quot;S&quot; ## [4,] &quot;D&quot; &quot;H&quot; &quot;L&quot; &quot;P&quot; &quot;T&quot; # blank spaces technically not needed but improve code readability mat[1, ] # first row ## [1] &quot;A&quot; &quot;E&quot; &quot;I&quot; &quot;M&quot; &quot;Q&quot; Task: Your turn now. Create the matrix mat as above and index out the first column. mat[ , 1] # first column ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot;   In order to index more than one element, you need to provide a vector of indices: mat[c(2, 4), ] # rows 2 and 4, notice the c() ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;B&quot; &quot;F&quot; &quot;J&quot; &quot;N&quot; &quot;R&quot; ## [2,] &quot;D&quot; &quot;H&quot; &quot;L&quot; &quot;P&quot; &quot;T&quot; Task: OK, now ask for rows 2 and 4 of columns 1-3. mat[c(2, 4), 1:3] # elements 2 and 4 of columns 1-3 ## [,1] [,2] [,3] ## [1,] &quot;B&quot; &quot;F&quot; &quot;J&quot; ## [2,] &quot;D&quot; &quot;H&quot; &quot;L&quot;   To get the full matrix, we simply type its name. However, you can think of the same operation as asking for all rows and all columns of the matrix: mat[ , ] # all rows, all columns ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;A&quot; &quot;E&quot; &quot;I&quot; &quot;M&quot; &quot;Q&quot; ## [2,] &quot;B&quot; &quot;F&quot; &quot;J&quot; &quot;N&quot; &quot;R&quot; ## [3,] &quot;C&quot; &quot;G&quot; &quot;K&quot; &quot;O&quot; &quot;S&quot; ## [4,] &quot;D&quot; &quot;H&quot; &quot;L&quot; &quot;P&quot; &quot;T&quot;   The same is the case with data frames: df[1, ] # first row ## id age eye_col dpt degree ## 1 1001 23 blue psy 2:2 Task: Get rows 4-6 of columns 1 and 3 of our df. df[4:6, c(1, 3)] ## id eye_col ## 4 1004 green ## 5 1005 blue ## 6 1006 brown   Take home message: when using indices to ask for elements, remember that to request more than one, you need to give a vector of indices (i.e., numbers bound in a c()). Also remember that some data structures need you to specify dimensions separated by a comma (most often just rows and columns for matrices and data frames). A frequent error is to specify the wrong number of dimensions: x[2, 3] # 2 dimensions where there&#39;s only 1 ## Error in x[2, 3]: incorrect number of dimensions df[4:6] # 1 dimension where there are 2 ## Error in `[.data.frame`(df, 4:6): undefined columns selected   Remember these error messages. Guaranteed this is not the last time you see them! :)   2.3.2 Subsetting using names The columns of data frames must have names: names(df) ## [1] &quot;id&quot; &quot;age&quot; &quot;eye_col&quot; &quot;dpt&quot; &quot;degree&quot; You can use the names for subsetting in a few ways: # using $ just like with lists df$eye_col # notice NO quotes ## [1] blue brown brown green blue brown ## Levels: blue brown green # using [[]] df[[&quot;eye_col&quot;]] # notice the quotes ## [1] blue brown brown green blue brown ## Levels: blue brown green # using [] putting the name in the 2nd dimension df[ , &quot;eye_col&quot;] # quotes! ## [1] blue brown brown green blue brown ## Levels: blue brown green The latter way can be also used to subset several columns at a time. All you need to do is provide a vector of names – data.frame[ , c(&quot;name1&quot;, &quot;name2&quot;, &quot;nameN&quot;)]. Task: Get rows 1 and 3 of columns age, dpt, and degree of our df. df[c(1, 3), c(&quot;age&quot;, &quot;dpt&quot;, &quot;degree&quot;)] ## age dpt degree ## 1 23 psy 2:2 ## 3 33 lel 1 2.4 Replacement Subsetting can be used for two things. First, it’s good for creating subsets of your data by assigning the outcome of the command to a new object. Let’s say we only want to work with the first 3 rows of df. We can assign the subset to a separate object: df_subset &lt;- df[1:3, ] df_subset ## id age eye_col dpt degree ## 1 1001 23 blue psy 2:2 ## 2 1002 25 brown cog 1 ## 3 1003 33 brown lel 1 Second, we can use subsetting to replace values in the object with other values. This is done by putting the subset to the left of the &lt;- operator: df_subset$eye_col[2] &lt;- &quot;green&quot; df_subset ## id age eye_col dpt degree ## 1 1001 23 blue psy 2:2 ## 2 1002 25 green cog 1 ## 3 1003 33 brown lel 1 Task: Replace the 3th, 5th, and 6th value of age in df with NA. # ANY of the following works df$age[c(3, 5, 6)] &lt;- NA df[c(3, 5, 6), &quot;age&quot;] &lt;- NA df[c(3, 5, 6), 2] &lt;- NA df[[&quot;age&quot;]][c(3, 5, 6)] &lt;- NA df[[2]][c(3, 5, 6)] &lt;- NA In case it’s not clear, subsetting constitutes the vast majority of all data processing/cleaning/manipulation. The importance of your understanding how it’s done simply cannot be overstated. Once you really get the hang of it, that’s pretty much you sorted with R Task: Finally, exit the project (Lab2 &gt; Close Project, save anything when asked).   2.4.0.1 That’s it for today! Feedback on this, and whether you feel you learnt anything, very welcome.     "],
["lab-3.html", "Chapter 3 Lab 3 3.1 Logical subsetting 3.2 Sorting 3.3 Subsetting with commands 3.4 Creating variables", " Chapter 3 Lab 3 This week, we’ll pick up where we left off last time. We’ll talk about using logical vectors for subsetting, and how to use subsetting to do all sorts of data manipulations. Task: Create a new RStudio project in a new directory called “Lab3” within your “Univariate” folder. 3.1 Logical subsetting Last time, you learnt how use numeric indices to subset vectors, matrices, and data frames. Another way of going about the same task is to use vectors of booleans (TRUE/FALSE) inside the []s. An important requirement here is that the vector must be the same length as the one being subsetted. So, for a vector with three elements, we need to provide three logical values, TRUE for “I want this one” and FALSE for “I don’t want this one”. Let’s demonstrate this on the same vector we used for indices: x &lt;- c(&quot;I&quot;, &quot; &quot;, &quot;l&quot;, &quot;o&quot;, &quot;v&quot;, &quot;e&quot;, &quot; &quot;, &quot;R&quot;) # get the 6th element x[c(F, F, F, F, F, T, F, F)] ## [1] &quot;e&quot; #get elements 3-6 x[c(F, F, T, T, T, T, F, F)] ## [1] &quot;l&quot; &quot;o&quot; &quot;v&quot; &quot;e&quot;   All the other principles we talked about regarding indexing apply also to logical vectors. Note also, that 2D structures need a logical row vector and a logical column vector: # recall our mat mat &lt;- matrix(LETTERS[1:20], ncol = 5) mat ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;A&quot; &quot;E&quot; &quot;I&quot; &quot;M&quot; &quot;Q&quot; ## [2,] &quot;B&quot; &quot;F&quot; &quot;J&quot; &quot;N&quot; &quot;R&quot; ## [3,] &quot;C&quot; &quot;G&quot; &quot;K&quot; &quot;O&quot; &quot;S&quot; ## [4,] &quot;D&quot; &quot;H&quot; &quot;L&quot; &quot;P&quot; &quot;T&quot; # rows 2 and 4 mat[c(T, F, T, F), ] ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;A&quot; &quot;E&quot; &quot;I&quot; &quot;M&quot; &quot;Q&quot; ## [2,] &quot;C&quot; &quot;G&quot; &quot;K&quot; &quot;O&quot; &quot;S&quot; Task: Your turn. Use logical subsetting to get row 4 of the first two columns of mat. mat[c(F, F, F, T), c(T, T, F, F, F)] ## [1] &quot;D&quot; &quot;H&quot;   You can even COMBINE the two ways! mat[4, c(T, T, F, F, F)] ## [1] &quot;D&quot; &quot;H&quot;   And as if vectors weren’t enough, you can even use matrices of logical values to subset matrices and data frames. Task: Recreate the df data frame from last time (see below). Don’t copy the code, type it all up again so that your brain gets used to basic R syntax and commands. Remember, the id and dpt variables should be factors, degree should be ordered, and age should be numberic. Hint: data.frame(variable1_name = vector1, variable2_name = vector2...). df &lt;- data.frame(id = factor(1001:1006), age = c(23, 25, 33, 26, 25, 24), eye_col = factor(c(&quot;blue&quot;, &quot;brown&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;brown&quot;)), dpt = factor(c(&quot;psy&quot;, &quot;cog&quot;, &quot;lel&quot;, &quot;psy&quot;, &quot;phi&quot;, &quot;lel&quot;)), degree = ordered(c(3, 1, 1, 1, 2, 2), labels = c(&quot;1&quot;, &quot;2:1&quot;, &quot;2:2&quot;))) df ## id age eye_col dpt degree ## 1 1001 23 blue psy 2:2 ## 2 1002 25 brown cog 1 ## 3 1003 33 brown lel 1 ## 4 1004 26 green psy 1 ## 5 1005 25 blue phi 2:1 ## 6 1006 24 brown lel 2:1 Task: Create a matrix called mat_logic that looks like the one below. There are multiple ways of doing this but a reasonably efficient one would be to bind 5 columns. These columns can easily be constructed with the rep() function (if you’re not familiar with it yet, look it up using ?rep). mat_logic &lt;- cbind(rep(F, 6), rep(F, 6), rep(c(F, T), each = 3), rep(F, 6), rep(c(T, F), each = 3)) mat_logic ## [,1] [,2] [,3] [,4] [,5] ## [1,] FALSE FALSE FALSE FALSE TRUE ## [2,] FALSE FALSE FALSE FALSE TRUE ## [3,] FALSE FALSE FALSE FALSE TRUE ## [4,] FALSE FALSE TRUE FALSE FALSE ## [5,] FALSE FALSE TRUE FALSE FALSE ## [6,] FALSE FALSE TRUE FALSE FALSE Task: Now use mat_logic to subset df. Since you are using a 2D object to subset another 2D object, you don’t need to worry about dimensions within the []s. df[mat_logic] ## [1] &quot;green&quot; &quot;blue&quot; &quot;brown&quot; &quot;2:2&quot; &quot;1&quot; &quot;1&quot;   Notice, however, that the output is a vector so two things happened: first, the rectangular structure has been erased and second, since vectors can only contain elements of the same class (see above), the numbers got converted into character strings (hence the &quot;&quot;s). Nevertheless, this method of subsetting using logical matrices can be useful for replacing several values in different rows and columns with another value: # replace with NAs df[mat_logic] &lt;- NA df ## id age eye_col dpt degree ## 1 1001 23 blue psy &lt;NA&gt; ## 2 1002 25 brown cog &lt;NA&gt; ## 3 1003 33 brown lel &lt;NA&gt; ## 4 1004 26 &lt;NA&gt; psy 1 ## 5 1005 25 &lt;NA&gt; phi 2:1 ## 6 1006 24 &lt;NA&gt; lel 2:1   To use a different example, take the function lower.tri(). It can be used to subset a matrix in order to get the lower triangle (with or without the diagonal). Consider matrix mat2 which has &quot;L&quot;s in its lower triangle, &quot;U&quot;s in its upper triangle, and &quot;D&quot;s on the diagonal: ## [,1] [,2] [,3] [,4] ## [1,] &quot;D&quot; &quot;U&quot; &quot;U&quot; &quot;U&quot; ## [2,] &quot;L&quot; &quot;D&quot; &quot;U&quot; &quot;U&quot; ## [3,] &quot;L&quot; &quot;L&quot; &quot;D&quot; &quot;U&quot; ## [4,] &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;D&quot;   Task: Create mat2 just like the one above and use it as an argument of lower.tri(). # elegant way mat2 &lt;- matrix(rep(&quot;U&quot;, 16), ncol = 4) # first, create a matrix of &quot;U&quot;s mat2[col(mat2) == row(mat2)] &lt;- &quot;D&quot; # replace the diagonal elements with &quot;D&quot;s mat2[lower.tri(mat2)] &lt;- &quot;L&quot; # replace lower triangle with &quot;L&quot;s mat2 # et voila! ## [,1] [,2] [,3] [,4] ## [1,] &quot;D&quot; &quot;U&quot; &quot;U&quot; &quot;U&quot; ## [2,] &quot;L&quot; &quot;D&quot; &quot;U&quot; &quot;U&quot; ## [3,] &quot;L&quot; &quot;L&quot; &quot;D&quot; &quot;U&quot; ## [4,] &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;D&quot; lower.tri(mat) ## [,1] [,2] [,3] [,4] [,5] ## [1,] FALSE FALSE FALSE FALSE FALSE ## [2,] TRUE FALSE FALSE FALSE FALSE ## [3,] TRUE TRUE FALSE FALSE FALSE ## [4,] TRUE TRUE TRUE FALSE FALSE Task: Now, you can use this command inside the []s to subset out the elements in the lower triangle of mat2. Try it now! You should only get &quot;L&quot;s. mat2[lower.tri(mat2)] ## [1] &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot;   Task: Adding the , diag = T argument to the lower.tri() function will return the lower triangle along with the diagonal. Edit the command above to include this argument. # we get only &quot;L&quot;s and &quot;D&quot;s mat2[lower.tri(mat2, diag = T)] ## [1] &quot;D&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;D&quot; &quot;L&quot; &quot;L&quot; &quot;D&quot; &quot;L&quot; &quot;D&quot;   So what does the function actually do? What is this sorcery? Let’s look at the output of the function again: lower.tri(mat2) ## [,1] [,2] [,3] [,4] ## [1,] FALSE FALSE FALSE FALSE ## [2,] TRUE FALSE FALSE FALSE ## [3,] TRUE TRUE FALSE FALSE ## [4,] TRUE TRUE TRUE FALSE So the function produces a matrix of logicals, the same size as out mat2, with TRUEs in the lower triangle and FALSEs elsewhere. What we did above is simply use this matrix to subset mat2[]. If you’re curious how the function produces the logical matrix then, first of all, that’s great, keep it up and second, you can look at the code wrapped in the lower.tri object (since functions are only objects of a special kind with code inside instead of data): lower.tri function (x, diag = FALSE) { x &lt;- as.matrix(x) if (diag) row(x) &gt;= col(x) else row(x) &gt; col(x) } &lt;bytecode: 0x0000000015a39ab0&gt; &lt;environment: namespace:base&gt; Right, let’s see. If we set the diag argument to TRUE the function returns row(x) &gt;= col(x). If we leave it set to FALSE (default), it returns row(x) &gt; col(x). Let’s substitute x for our mat2 and try it out:   row(mat2) ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 2 2 2 2 ## [3,] 3 3 3 3 ## [4,] 4 4 4 4 col(mat2) ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 1 2 3 4 ## [3,] 1 2 3 4 ## [4,] 1 2 3 4 # diag = TRUE case row(mat2) &gt;= col(mat2) ## [,1] [,2] [,3] [,4] ## [1,] TRUE FALSE FALSE FALSE ## [2,] TRUE TRUE FALSE FALSE ## [3,] TRUE TRUE TRUE FALSE ## [4,] TRUE TRUE TRUE TRUE # use it for subsetting mat2 mat2[row(mat2) &gt;= col(mat2)] ## [1] &quot;D&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;D&quot; &quot;L&quot; &quot;L&quot; &quot;D&quot; &quot;L&quot; &quot;D&quot; # diag = FALSE case row(mat2) &gt; col(mat2) ## [,1] [,2] [,3] [,4] ## [1,] FALSE FALSE FALSE FALSE ## [2,] TRUE FALSE FALSE FALSE ## [3,] TRUE TRUE FALSE FALSE ## [4,] TRUE TRUE TRUE FALSE mat2[row(mat2) &gt; col(mat2)] ## [1] &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot;   MAGIC!   Take home message: When subsetting using logical vectors, the vectors must be the same length as the vectors you are subsetting. The same goes for logical matrices: they must be the same size as the matrix/data frame you are subsetting. 3.1.1 Complementary subsetting Both ways of asking for subsets of data can be inverted. For indices, you can simply put a - sign before the vector: # elements 3-6 of x x[3:6] ## [1] &quot;l&quot; &quot;o&quot; &quot;v&quot; &quot;e&quot; # invert the selection x[-(3:6)] ## [1] &quot;I&quot; &quot; &quot; &quot; &quot; &quot;R&quot; #equivalent to x[c(1, 2, 7, 8)] ## [1] &quot;I&quot; &quot; &quot; &quot; &quot; &quot;R&quot;   For logical subsetting, you need to negate the values. That is done using the logical negation operator ‘!’ (AKA “not”): y &lt;- T y ## [1] TRUE # negation !y ## [1] FALSE Task: You can also negate a matrix of logicals. Use the complement (negation) of mat_logic to subset our df. mat_logic ## [,1] [,2] [,3] [,4] [,5] ## [1,] FALSE FALSE FALSE FALSE TRUE ## [2,] FALSE FALSE FALSE FALSE TRUE ## [3,] FALSE FALSE FALSE FALSE TRUE ## [4,] FALSE FALSE TRUE FALSE FALSE ## [5,] FALSE FALSE TRUE FALSE FALSE ## [6,] FALSE FALSE TRUE FALSE FALSE !mat_logic ## [,1] [,2] [,3] [,4] [,5] ## [1,] TRUE TRUE TRUE TRUE FALSE ## [2,] TRUE TRUE TRUE TRUE FALSE ## [3,] TRUE TRUE TRUE TRUE FALSE ## [4,] TRUE TRUE FALSE TRUE TRUE ## [5,] TRUE TRUE FALSE TRUE TRUE ## [6,] TRUE TRUE FALSE TRUE TRUE df[!mat_logic] ## [1] &quot;1001&quot; &quot;1002&quot; &quot;1003&quot; &quot;1004&quot; &quot;1005&quot; &quot;1006&quot; &quot;23&quot; &quot;25&quot; ## [9] &quot;33&quot; &quot;26&quot; &quot;25&quot; &quot;24&quot; &quot;blue&quot; &quot;brown&quot; &quot;brown&quot; &quot;psy&quot; ## [17] &quot;cog&quot; &quot;lel&quot; &quot;psy&quot; &quot;phi&quot; &quot;lel&quot; &quot;1&quot; &quot;2:1&quot; &quot;2:1&quot;   Bear in mind that all data cleaning and transforming ultimately boils down to using either or both of these two ways of subsetting elements! 3.2 Sorting Sorting is often an important step in cleaning or otherwise processing your data. It is also something that should never be done in Excel6. SO let’s have a look at how to sort vectors and data frames! There are two main base R functions that are useful for sorting, sort() and order() Task: Create an object lttrs containing vector of 5 randomly ordered letters of the alphabet. Use the LETTERS object (created by default when you open R) and the sample() function to do this. Note. Your vector of 5 letters will differ from the below due to random sampling. lttrs &lt;- sample(LETTERS, 5) lttrs ## [1] &quot;U&quot; &quot;T&quot; &quot;G&quot; &quot;N&quot; &quot;S&quot; Task: Call the sort() function on the object (i.e., provide the object as an argument to the function). sort(lttrs) ## [1] &quot;G&quot; &quot;N&quot; &quot;S&quot; &quot;T&quot; &quot;U&quot; Task: What if you want to reverse the order? Check out the help file for the function (?sort) and get a vector of reverse-ordered lttrs. sort(lttrs, decreasing = T) ## [1] &quot;U&quot; &quot;T&quot; &quot;S&quot; &quot;N&quot; &quot;G&quot;   OK, so this is easy, sort() is used to, well, sort vectors of numbers or letters. I can be useful in situations when you want to know if, let’s say, a vector of numbers contains all the numbers within a given range. Imagine we want to know if the command sample(20) gives us all the numbers between 1 and 20, albeit in a random order: sample(20) ## [1] 8 1 7 15 4 2 19 5 16 10 17 13 9 11 14 3 12 20 6 18 sort(sample(20)) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # compare the sort(sample(20)) vector with the 1:20 vector element by element sort(sample(20)) == 1:20 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [15] TRUE TRUE TRUE TRUE TRUE TRUE # are all of the resulting logicals TRUE? all(sort(sample(20)) == 1:20) ## [1] TRUE   Task: So much for sort(). Let’s call order() on lttrs and try to figure out what the output means. order(lttrs) ## [1] 3 4 5 2 1   As you may have figured out, order() gives you the indices of each element’s position in an ordered vector consisting of the same elements. That means that if you use the output of order() to index out lttrs, you’ll get the same result as with sort(lttrs): lttrs[order(lttrs)] ## [1] &quot;G&quot; &quot;N&quot; &quot;S&quot; &quot;T&quot; &quot;U&quot; Task: Just like sort(), order() can be reversed. Use order() to sort df$id in decreasing order like this: df$id[order(df$id, decreasing = T)] ## [1] 1006 1005 1004 1003 1002 1001 ## Levels: 1001 1002 1003 1004 1005 1006   You might be wondering what the point of order() is when there is sort(). Well, since order() produces a vector of indices that is independent of the input to the function, you can use it to order one vector according to the order of another: order(df$age) # order indices (ranks) of elements of age ## [1] 1 6 2 5 4 3 df$id[order(df$age)] # sort id from youngest to oldest ## [1] 1001 1006 1002 1005 1004 1003 ## Levels: 1001 1002 1003 1004 1005 1006 Task: Try ordering the entire df in order of decreasing age. Hint: don’t forget that data frames are 2-dimensional. df[order(df$age, decreasing = T), ] ## id age eye_col dpt degree ## 3 1003 33 brown lel &lt;NA&gt; ## 4 1004 26 &lt;NA&gt; psy 1 ## 2 1002 25 brown cog &lt;NA&gt; ## 5 1005 25 &lt;NA&gt; phi 2:1 ## 6 1006 24 &lt;NA&gt; lel 2:1 ## 1 1001 23 blue psy &lt;NA&gt; Task: A bit of a challenge now. Try sorting the columns of df according to the alphabetical order of their names. Hint: the names() (orcolnames()`) function produces a vector of column names of its argument. You can use it to get the rank indices. are 2-dimensional. df[ , order(names(df))] ## age degree dpt eye_col id ## 1 23 &lt;NA&gt; psy blue 1001 ## 2 25 &lt;NA&gt; cog brown 1002 ## 3 33 &lt;NA&gt; lel brown 1003 ## 4 26 1 psy &lt;NA&gt; 1004 ## 5 25 2:1 phi &lt;NA&gt; 1005 ## 6 24 2:1 lel &lt;NA&gt; 1006 3.3 Subsetting with commands You will have noticed that, when subsetting objects, we are not restricted to raw numeric and logical vectors but we can also use certain functions and operations. That is because R doesn’t distinguish between vectors provided manually and those returned by functions. For that reason, you should get into the habit of always thinking about functions and objects in terms of their output! This is an extremely important point when it comes to programming in R, hence the exclamation point. It’s so important that I’m tempted to repeat it in capital letters, but I don’t think I could live with myself if I did. Any function that returns either a numeric vector, a logical vector of the right length, or a logical matrix of the right size can be used for subsetting (think back to df[ , order(names(df))] or mat[lower.tri(mat)]. Let’s explore a few ways of using functions in this way. Comparative operators are useful for querying your dataset for rows for which some variable is equal to (==), larger than (&gt;), smaller than (&lt;), at least (&gt;=), or at most (&lt;=) some desired value. df$age == 25 ## [1] FALSE TRUE FALSE FALSE TRUE FALSE df[df$age == 25, ] ## id age eye_col dpt degree ## 2 1002 25 brown cog &lt;NA&gt; ## 5 1005 25 &lt;NA&gt; phi 2:1 Task: Why does this command for getting the subset of df for LEL students not work? Read the error and try to understand it. df[df$dpt == lel, ] ## Error in `[.data.frame`(df, df$dpt == lel, ): object &#39;lel&#39; not found   The error tells us that R is interpreting lel as a name of an object. Of course, lel is not an object, it is a level of the df$dpt factor and thus R fails to find it and throws an error. What do you need to do to make R understand that lel is a string? Task: Go on, correct the error in the code above. df[df$dpt == &quot;lel&quot;, ] ## id age eye_col dpt degree ## 3 1003 33 brown lel &lt;NA&gt; ## 6 1006 24 &lt;NA&gt; lel 2:1   Believe it or not, getting the error is actually a lucky case. Imagine you make the same mistake but – by coincidence – the level of the factor has the same name as some existing object in your environment. R might be able to run that command and quietly do something completely different than you expected. Coding is a lot about attention to details like these! Task: Get a subset of df of students who are 25 or older. df[df$age &gt;= 25, ] ## id age eye_col dpt degree ## 2 1002 25 brown cog &lt;NA&gt; ## 3 1003 33 brown lel &lt;NA&gt; ## 4 1004 26 &lt;NA&gt; psy 1 ## 5 1005 25 &lt;NA&gt; phi 2:1   You can combine and modify any operations that produce logicals using boolean operators (&amp;, |, xor(), !). Let’s say we only want to subset those students who are older than 25 OR younger than 24: df[df$age &gt; 25 | df$age &lt; 24, ] ## id age eye_col dpt degree ## 1 1001 23 blue psy &lt;NA&gt; ## 3 1003 33 brown lel &lt;NA&gt; ## 4 1004 26 &lt;NA&gt; psy 1 Task: Get a subset of those students who do psychology, philosophy, or cognitive science. df[df$dpt != &quot;lel&quot;, ] ## id age eye_col dpt degree ## 1 1001 23 blue psy &lt;NA&gt; ## 2 1002 25 brown cog &lt;NA&gt; ## 4 1004 26 &lt;NA&gt; psy 1 ## 5 1005 25 &lt;NA&gt; phi 2:1 # same df[!df$dpt == &quot;lel&quot;, ]   Task: If you asked for pdt equal to &quot;psy&quot; OR &quot;phi&quot; OR &quot;cog&quot;, that’s fine: it works. However, sometimes it’s easier to flip things around. In this case, the same result can be achieved by asking for dpt NOT equal to &quot;lel&quot;. Try it now!   Just like there’s no need to limit yourself to one operation, there’s also no need to limit yourself to a single variable. We can get a subset of students older than 24 who at the same time either study psychology or don’t have green eyes. Why? Because we can! df[df$age &gt; 24 &amp; xor(df$dpt == &quot;psy&quot;, df$eye_col != &quot;green&quot;), ] ## id age eye_col dpt degree ## 2 1002 25 brown cog &lt;NA&gt; ## 3 1003 33 brown lel &lt;NA&gt; ## NA &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## NA.1 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Notice something weird? Yeah, there are a couple of rows of NAs. The reason for that is that the result of the command we used for subsetting is a vector that contains 2 NAs. That’s because any comparative operation (==, &gt;=, etc.) performed on an NA will return NA: df$age &gt; 24 &amp; xor(df$dpt == &quot;psy&quot;, df$eye_col != &quot;green&quot;) ## [1] FALSE TRUE TRUE NA NA FALSE NA &lt; 100 ## [1] NA To prevent the subset from including the NA rows, we can get R to only give us indices of those rows for which the statement is TRUE: which(df$age &gt; 24 &amp; xor(df$dpt == &quot;psy&quot;, df$eye_col != &quot;green&quot;)) ## [1] 2 3 df[which(df$age &gt; 24 &amp; xor(df$dpt == &quot;psy&quot;, df$eye_col != &quot;green&quot;)), ] ## id age eye_col dpt degree ## 2 1002 25 brown cog &lt;NA&gt; ## 3 1003 33 brown lel &lt;NA&gt; Finally, you can subset cases based on how they compare to the output of some function. Again, that’s because R doesn’t care if you say &gt; 24 or &gt; function_that_returns_24(). Task: what is the mean age in our df? Hint: The function to use here is, rather conveniently, called mean(). mean(df$age) ## [1] 26 Task: . Get the IDs of those students whose age is less or equal to the mean age. df[df$age &lt;= mean(df$age), &quot;id&quot;] ## [1] 1001 1002 1004 1005 1006 ## Levels: 1001 1002 1003 1004 1005 1006 We’ve covered several things here. You’ve learned most there is to learn about subsetting. This is an extremely useful skill for all data processing. You also saw that you can peer into the code of functions to understand what they do (or, more often, why they don’t do what you’d like them to do). We Strongly encourage this kind of curiosity. Picking things apart and putting them back together is a very good way to learn! Finally, you learned to think of functions in terms of their output and understood that functions and operations can be used for subsetting. Understand these basic principles and you’re sorted! The only other thing you will ever need is a repertoire of functions to use. But that will come with experience… 3.4 Creating variables Sometimes – quite often, actually – you will want to create new variables in your data. Once you realise that variables are just vectors, there’s nothing easier than to add new ones to a data frame: # let&#39;s say all of the students in df are in Group 1 in our study df$group &lt;- 1 # the 1 will get recycled the right number of times df ## id age eye_col dpt degree group ## 1 1001 23 blue psy &lt;NA&gt; 1 ## 2 1002 25 brown cog &lt;NA&gt; 1 ## 3 1003 33 brown lel &lt;NA&gt; 1 ## 4 1004 26 &lt;NA&gt; psy 1 1 ## 5 1005 25 &lt;NA&gt; phi 2:1 1 ## 6 1006 24 &lt;NA&gt; lel 2:1 1 Indexing can be very helpful for creating new variables in your data. Imagine, we want to categorise our students into younger and older according to a cut-off point of 25 years (those older than 25 are in the “older” group). The way to do this is to create a new variable – age_group, for instance – that equals, say, &quot;younger&quot; for everyone. Task: go ahead and create this variable. Make sure your df ends up looking something like this: df$age_group &lt;- &quot;younger&quot; df ## id age eye_col dpt degree group age_group ## 1 1001 23 blue psy &lt;NA&gt; 1 younger ## 2 1002 25 brown cog &lt;NA&gt; 1 younger ## 3 1003 33 brown lel &lt;NA&gt; 1 younger ## 4 1004 26 &lt;NA&gt; psy 1 1 younger ## 5 1005 25 &lt;NA&gt; phi 2:1 1 younger ## 6 1006 24 &lt;NA&gt; lel 2:1 1 younger OK, now use indexing to assign the value &quot;older&quot; to those cases of our new variable for whom age &gt; 25. This should be the result: df$age_group[df$age &gt; 25] &lt;- &quot;older&quot; df ## id age eye_col dpt degree group age_group ## 1 1001 23 blue psy &lt;NA&gt; 1 younger ## 2 1002 25 brown cog &lt;NA&gt; 1 younger ## 3 1003 33 brown lel &lt;NA&gt; 1 older ## 4 1004 26 &lt;NA&gt; psy 1 1 older ## 5 1005 25 &lt;NA&gt; phi 2:1 1 younger ## 6 1006 24 &lt;NA&gt; lel 2:1 1 younger   No problem, right? Good! Task: To practice a little more, create another variable in the data frame called has_col that will tell us, whether or not the given student has a record of their eye colour. Hint: Check out the is.na() function. df$has_col &lt;- !is.na(df$eye_col) df ## id age eye_col dpt degree group age_group has_col ## 1 1001 23 blue psy &lt;NA&gt; 1 younger TRUE ## 2 1002 25 brown cog &lt;NA&gt; 1 younger TRUE ## 3 1003 33 brown lel &lt;NA&gt; 1 older TRUE ## 4 1004 26 &lt;NA&gt; psy 1 1 older FALSE ## 5 1005 25 &lt;NA&gt; phi 2:1 1 younger FALSE ## 6 1006 24 &lt;NA&gt; lel 2:1 1 younger FALSE The combination of indexing and calculating new variables is an immensely powerful tool in your data-processing arsenal!   Task: Finally, save and exit the project..   3.4.0.1 That’s it for today! Feedback on this, and whether you feel you learnt anything, very welcome. For a quick reference, check out this short account of useful functions that return logical vectors If the plot disappears after a split-second, just click inside the RStudio plotting window and it should pop back up.↩ "],
["lab-4.html", "Chapter 4 Lab 4 4.1 Introduction 4.2 Simple Plots 4.3 Scatterplots: Visualizing associations 4.4 Boxplots: Visualizing group differences 4.5 Descriptive Statistics 4.6 More advanced plotting 4.7 Multiple Plots in One 4.8 Let’s put all this into practice 4.9 Lab 4 GGPLOT", " Chapter 4 Lab 4 4.1 Introduction In this lab, you are going to begin to become accustomed to randomly generating data of different types in R, generating appropriate plots for different types of data, reading in real data and checking data sets, summarising variables, and plotting real data. The last sections of this lab are intended to be completed as homework, however if you get it all done in the 2 hours - good for you. So let’s get started. Before we begin, set up a project for today’s lab! 4.2 Simple Plots 4.2.1 Categorical Variables Let’s start with the visualization of binary variables. First, let’s randomly generate a single binary variable. # Randomly Generate a Single Binary Variable x_bin &lt;- rbinom(100, 1, .6) Take a look at ?rbinom and try to work out what frequency of 1s we might expect to see in x_bin? We will come back to this shortly. Now let’s look to make a plot of this variable. plot(x_bin) What has gone wrong here? plot() is generally quite a clever function in that it will produce appropriate plots for the type of data we give it. But we need to make sure the variables we give plot() are of the type we would like. Let’s check whether the variable we created is being treated as a factor (R language for categorical variables): is.factor(x_bin) #Asking R whether x_bin is being treated as a factor. ## [1] FALSE So R is not recognising our variable as a categorical variable. What is R treating the xbin as; class(x_bin) ## [1] &quot;integer&quot; We want this to be a factor (nominal categories), so We can change the status of this variable as follows: x_bin &lt;- as.factor(x_bin) is.factor(x_bin) ## [1] TRUE Excellent! R now recognises the variable as a categorical variable. Now let’s see what plot() does: plot(x_bin) plot() has now given us a very basic frequency bar chart of 0s and 1s, for the variable x_bin. Let’s briefly return to the question above - what is the frequency of 1s in x_bin? From looking at the plot above, we can see there are more 1s than 0s, but as it stands the plot is not much use to us in seeing the exact frequency (we will deal with editing plots shortly). Let’s ask R to give us the actual frequencies of 0s and 1s: table(x_bin) # this produces a frequency table for our variable ## x_bin ## 0 1 ## 34 66 Task: : So, thinking back to how we specified the creation of our random variable (rbinom(100, 1, .6)), why is it we do not have 60 1s? Also, why is it that you may have a different number of 1s to the person sat next to you? Hopefully you can see that this is your first taste of probability theory and random variables (although you may not have quite phrased it that way), which we will discuss lots more next week. For now, lets continue looking at plots. Note from this point on your plots will look different to mine (and your neighbours) because you have different random variables. Now let’s create a second binary variable and create a mini data set with both variables: y_bin &lt;- rbinom(100, 1, .75) # look at the frequencies of 0 and 1 table(y_bin) ## y_bin ## 0 1 ## 20 80 # tell R y_bin is a binary categorical variable y_bin &lt;- as.factor(y_bin) # cbind() combines two columns into a single object, here called data data &lt;- cbind(x_bin, y_bin) # shows the column and row names and the first few lines of data head(data) ## x_bin y_bin ## [1,] 2 2 ## [2,] 2 1 ## [3,] 1 2 ## [4,] 1 2 ## [5,] 1 2 ## [6,] 2 1 Task: : Look at the first few lines carefully. Why is R showing the values of x_bin and y_bin to be 1 and 2 instead of 0 and 1? More importantly, if we assume this is a nominal variable, does this matter? A very quick refresher on indexing. R uses 2-dimensional indexing with [ ] when indexing rows, columns and individual cells in a data frame or matrix. The numbers appear in the order [row, column]. So; [1, ] = the first row [ , 1] = the first column [1, 1] = the value in the first row, first column Indexing is a very useful tool. We can use it to highlight individual elements, perform tasks on single rows or columns, subset data etc. Indexing will appear in lots of the code that follows. You can use the basic rules here to help you “read R”. As an example: data[3, 2] #calls the value in the third row, second column of our data set ## y_bin ## 2 head(data) # which we can check by looking the top few rows of data ## x_bin y_bin ## [1,] 2 2 ## [2,] 2 1 ## [3,] 1 2 ## [4,] 1 2 ## [5,] 1 2 ## [6,] 2 1 You can play around with this a little if you are not yet convinced. For example, we could use indexing to give us the total number of 1s in the first column; sum(data[ , 1] == 1) ## [1] 34 We can then check this is correct using the table() function again. table(data[ , 1]) #then we can check the answer using table() ## ## 1 2 ## 34 66 Task: What is the code line sum(data[ , 1] == 1) doing? Although in principle this is quite simple, how R does something of this sort takes a bit of getting used to. Hint: Think logical vectors. One last thing before we get back to plots. I want to give our variables names so we can pretend we have collected real data (instead of just simulating). So, here we use colnames() to change the names of the variables. We create a vector of new names, here &quot;programme&quot; and &quot;over18&quot;, and then we assign this to data: colnames(data) &lt;- c(&quot;programme&quot;, &quot;over18&quot;) head(data) # we can then check the names have changed ## programme over18 ## [1,] 2 2 ## [2,] 2 1 ## [3,] 1 2 ## [4,] 1 2 ## [5,] 1 2 ## [6,] 2 1 This is good. But now we need our numerical labels to mean something consistent with the variable names. So we need to assign verbal labels to the numerical values. # To manipulate our data, we first make it a data frame data &lt;- as.data.frame(data) # Now we make sure our variable is recognised as a factor data$programme &lt;- as.factor(data$programme) is.factor(data$programme) ## [1] TRUE OK, all looking good so far. Remember you looked up logical vectors earlier, well you should recognise that when we use is.factor() (asking R a direct question of whether the variable is a factor or not) it returns a logical vector. Before we assign verbal labels to the numerical values, we should first check how many levels (in R speak, levels are the number of unique numerical labels) we have: levels(data$programme) ## [1] &quot;1&quot; &quot;2&quot; And we can see we have 2 levels with numerical labels 1 and 2. So now we assign new informative verbal labels: levels(data$programme) &lt;- c(&quot;Psychology&quot;, &quot;Linguistics&quot;) Task: Now do the same for the over18 variable but, this time, use indexing in our code. data[, 2] &lt;- as.factor(data[, 2]) is.factor(data[, 2]) ## [1] TRUE levels(data[, 2]) ## [1] &quot;1&quot; &quot;2&quot; levels(data[, 2]) &lt;- c(&quot;Yes&quot;, &quot;No&quot;) Task: Back to plotting (finally)! Let’s look at the bar plots of the frequencies for both variables: plot(data$programme) plot(data[, 2]) # indexing works too but notice axis labels Now, if we want to consider the ratio of psychologists and linguists who are over 18, we can compute a mosaic plot. To do this we simply give plot() two categorical variables. Task: Why don’t you try it yourself! plot(data$programme, data$over18) Time to Think: What do you think would happen if we used data[, 1] instead of data$programme? What can we conclude from this plot? Are there proportionally more psychologists than linguists 18+? Again, why does your plot look different to mine and your neighbour? We can extend all the principles above to considering multiple nominal variables. Let’s add a variable to our data called sport. Let’s suppose our sample of 100 people play five sports (football, running, hockey, golf &amp; swimming). Task: Let’s create this variable. sample() is going to come in handy here. data$sport &lt;- factor(sample(c(&quot;football&quot;, &quot;running&quot;, &quot;hockey&quot;, &quot;golf&quot;, &quot;swimming&quot;), 100, replace = T)) Task: Look at the bar chart for the sport variable. plot(data$sport) Task: Now ask for a mosaic plot of sport by programme: plot(data[, 1], data[, 3]) Time to think: Look at the help for ?sample. Look at the third argument, prob in the help and think about the lecture on probability. Can you work out from the help what the default settings for the weights are? How do you think the mosaic plot may change if we changed these weights? If you want to play with the weights you can. Add the argument prob = c() to the code for creating the sport variable. Inside the parentheses, list 5 numbers which sum to 1. For example: prob = c(.1, .5, .2, .1, .1). Note: You could try to make these meaningful by estimating the actual popularities of these sports. 4.2.1 Plotting order With all of the plots in this section, it really doesn’t matter what order we plot the variables in as they are nominal variables. There is no order of presentation for programme which makes more logical sense than any other. However, the same is not true for ordered categorical variables. 4.2.2 Ordinal Categorical Variables For ordered categorical data, our primary tool for visualization is the same as for the binary and nominal variables - the bar plot. However, here, order matters for reasons that should be clear from the lecture. Unlike nominal categorical variables, order does matter and we want to preserve this in our visualizations. Remember, with an ordered categorical variable we often assume that the data we observe come from a truly normally distributed variable, but that our measurements are coarsely categorized. For the purpose of our generated data, let’s create a variable, fitness, that is individuals self-rated fitness level on a scale of 1-8: norm &lt;- rnorm(100, 0, 1) # start by creating a random normal variable initial_fitness &lt;- cut(norm, 8) initial_fitness &lt;- as.numeric(initial_fitness) Here we first cut our normally distributed variable into 8 categories, and then told R that this new variable initial_fitness was numeric. However, in R, we can easily put functions within other functions, so we could have written the same thing as: initial_fitness &lt;- as.numeric(cut(norm, 8)) Look out for this as we go through the practical. In the second line, we have used the cut() function to create 8 equally sized groups from our normally distributed variable. These represent our coarse divisions of an underlying normal variable. We use as.numeric() to tell R we want to treat this variable as if it were numeric, not as a factor. Remember: There is much debate about whether coarsely categorized variables should be treated as continuous or not. This is an important decision you will have to make in your own analyses and you will need to make sure you have told R what type of variable each variable in your data set is. As an aside, if you want to check what R thinks each variable is, you can use: class(initial_fitness) ## [1] &quot;numeric&quot; We can see how the distribution of our variables degrades when we coarsely categorize norm by using histograms. Task: Look at the histograms of norm and initial_fitness. If you’ve been using plot() until now, this time use hist()) hist(norm) hist(initial_fitness) Task: With 8 categories, this does not look too bad, but what would it look like if we used say 4 categories? Check this out, create a new variable with a different name, and use the cut() function and the continuous variable norm to categorize it. Then plot it on a bar chart. barplot(table(as.numeric(cut(norm, 4))), xlab = &quot;Categorised fitness&quot;) Now let’s add initial_fitness to the data set. We could just attach it as it is, but to show you another method to manipulate your data, let’s jumble up the order a bit using sample(). fitness &lt;- sample(initial_fitness, 100, replace = F) Time to Think: We have used sample() before. Have a go at describing what we are doing with sample here. Hint: use ?sample and focus on size we have requested (given sample size), and what it might mean to not replace values. We can then compare the first few lines to see we have jumbled it up, and the cell frequencies to show this is all we have done; head(initial_fitness) ## [1] 4 5 6 3 7 6 head(fitness) ## [1] 4 6 3 4 8 6 table(initial_fitness) ## initial_fitness ## 1 2 3 4 5 6 7 8 ## 5 7 15 22 22 20 8 1 table(fitness) ## fitness ## 1 2 3 4 5 6 7 8 ## 5 7 15 22 22 20 8 1 Task: Let’s add it to our other data. data$fitness &lt;- fitness # add fitness 4.2.3 Continuous Variables In some cases our variables will truly be continuously measured on a ratio scale. When this is the case, we can visualize the distribution of the variable in a number of ways. We can use a histogram, but in doing so we need to decide how many bins to use. Alternatively, we can use a kernel density plot. We have gone over a number of examples of this slowly, so let’s do this in one quick hit of code. Task: Create a normally distributed variable weight (let’s say in kg, with mean = 81 and SD = 12) in data and plot it on a histogram. data$weight &lt;- sample(rnorm(100, 81, 12), replace = F) hist(data$weight) Task: Now try creating a density plot of the variable. plot(density(data$weight)) 4.3 Scatterplots: Visualizing associations Sometimes we might want to use plots to look at the relations between variables. We will cover lots of different types of plots to do this as the course progresses (particularly plots which help check assumptions in GLM). As a simple case, consider looking at the relationship between two continuous variables. We already have two variables we are treating as continuous in our data, Fitness and Weight. This time, instead of giving plot a single variable, we give it two. Again, it is smart enough to know what to do with these data: plot(data$fitness, data$weight) Time to Think: Although we are treating fitness as continuous, this scatterplot looks odd. Why do you think this is? Task: Instead of using the categorized variable fitness, why don’t we use the norm instead. Though not in your data set, this is still in your R environment (look down the values list in the top right). plot(norm, data$weight) 4.4 Boxplots: Visualizing group differences So, the scatterplot gives us the association between two variables, but what about comparing the mean score on a continuous variable given a categorical variable. This type of plot is very useful if you are conducting a t-test (next week!). You have in fact already done this in the first lab, so this code should seem familiar and we wont linger on it: # I am using this here to remind myself how each of the variables is named in data names(data) ## [1] &quot;programme&quot; &quot;over18&quot; &quot;sport&quot; &quot;fitness&quot; &quot;weight&quot; boxplot(weight~programme, data = data) 4.5 Descriptive Statistics As well as the basics of plotting, it is also useful to be able to produce basic descriptive statistics for the variables in your data set. So let’s look at the options for this. My preference for descriptive statistics is to use the describe() function in the psych package. This is simple to use. An aside on packages We talked about how functions in R are just objects with a bunch of code inside. Therefore, there must be some code somewhere that creates these objects. Packages are basically libraries of functions that define these objects. Many of them come as part of basic installation of R and even more are available on the internet (the repository of all tested R packages is called CRAN). If you want to see a list of the packages currently installed on your computer, type installed.packages(). To install a package, use the install.package() function. It takes as the only required argument the name of the package in double quotes, e.g., install.package(&quot;psych&quot;). You only have to install a package once so if you want this command in your script, comment it out after first use (by potting # at the beginning of the line). This way it won’t run every time you run your code! Once a package is installed on your computer, the files that define the functions contained in the package will be made available to R. However, if you want to use them yourself, you need to load the package. To load an installed package, use the library() function, giving it the name of the package as argument: library(psych) Once you load a package, you will be able to use its functions just like any other function you’ve already used. # install.packages(&quot;psych&quot;) library(psych) describe(data) ## vars n mean sd median trimmed mad min max range ## programme* 1 100 1.66 0.48 2.00 1.70 0.00 1.00 2.00 1.00 ## over18* 2 100 1.80 0.40 2.00 1.88 0.00 1.00 2.00 1.00 ## sport* 3 100 2.84 1.50 3.00 2.80 1.48 1.00 5.00 4.00 ## fitness 4 100 4.46 1.61 5.00 4.51 1.48 1.00 8.00 7.00 ## weight 5 100 79.84 11.88 78.99 79.62 10.42 53.85 112.97 59.12 ## skew kurtosis se ## programme* -0.67 -1.57 0.05 ## over18* -1.48 0.19 0.04 ## sport* 0.20 -1.38 0.15 ## fitness -0.26 -0.55 0.16 ## weight 0.23 -0.12 1.19 However, as you can see, psych reports the arithmetic mean, SD, and SE for all variables, even those which are factors. We can tell which psych and R are treating as factors as a * appears after the name of the variable on the left. psych does give us the median, minimum and maximum values, range, skew and kurtosis - so it is doing pretty well in describing our data. But we may want a couple of other things. # as we have seen, this gives frequencies table(data$programme) ## ## Psychology Linguistics ## 34 66 # we can use quantile to give us the values at a given percentile of our variable # here we select the 25th and 75th quantile(data$weight, c(.25, .75)) ## 25% 75% ## 72.01751 86.04928 # we could also use this to get the inter-quartile range lower &lt;- as.numeric(quantile(data$weight, c(.25))) upper &lt;- as.numeric(quantile(data$weight, c(.75))) IQR &lt;- upper-lower IQR ## [1] 14.03177 # we can also get the mode. For multiple category variables, # we just want the largest value from the 2nd row of table() table(data[, 4]) ## ## 1 2 3 4 5 6 7 8 ## 5 7 15 22 22 20 8 1 Sometimes we may want to create our own table of results that includes the appropriate statistics given our variables. Let’s have a go. # first job, lets create an empty 2x2 matrix empty &lt;- matrix(nrow = 2, ncol = 2) colnames(empty) &lt;- c(&quot;Mean&quot;, &quot;Skew&quot;) # and then give the columns names rownames(empty) &lt;- c(&quot;Weight&quot;, &quot;Fitness&quot;) # and the rows names empty # and lets take a look ## Mean Skew ## Weight NA NA ## Fitness NA NA Now let’s put some values in. First we need to compute the statistics (here I will use describe(), save the output as an object, and then use indexing to isolate the values of interest and enter them into our empty table: descriptive &lt;- describe(data) descriptive # so lets take the mean of weight. ## vars n mean sd median trimmed mad min max range ## programme* 1 100 1.66 0.48 2.00 1.70 0.00 1.00 2.00 1.00 ## over18* 2 100 1.80 0.40 2.00 1.88 0.00 1.00 2.00 1.00 ## sport* 3 100 2.84 1.50 3.00 2.80 1.48 1.00 5.00 4.00 ## fitness 4 100 4.46 1.61 5.00 4.51 1.48 1.00 8.00 7.00 ## weight 5 100 79.84 11.88 78.99 79.62 10.42 53.85 112.97 59.12 ## skew kurtosis se ## programme* -0.67 -1.57 0.05 ## over18* -1.48 0.19 0.04 ## sport* 0.20 -1.38 0.15 ## fitness -0.26 -0.55 0.16 ## weight 0.23 -0.12 1.19 descriptive[5, 3] # which is in the fifth row, third column. ## [1] 79.84445 # now we can add this to our matrix empty[1, 1] &lt;- descriptive[5, 3] empty ## Mean Skew ## Weight 79.84445 NA ## Fitness NA NA Task: Finish this table by adding the other three values. If you want an extra challenge, edit the code to add a third column for the standard deviation of the mean (sd column in the describe() output). Put this column between the Mean and Skew columns of empty. # we can do empty[2, 1] &lt;- descriptive[4, 3] empty[1, 2] &lt;- descriptive[5, 12] empty[2, 2] &lt;- descriptive[4, 12] empty &lt;- cbind(empty[ , 1], descriptive$sd[5:4], empty[ , 2]) colnames(empty) &lt;- c(&quot;Mean&quot;, &quot;SD&quot;, &quot;Kurtosis&quot;) empty # output is a matrix ## Mean SD Kurtosis ## Weight 79.84445 11.875848 -0.1181091 ## Fitness 4.46000 1.610697 -0.5537514 # this also works empty &lt;- descriptive[5:4, c(3, 4, 12)] names(empty) &lt;- c(&quot;Mean&quot;, &quot;SD&quot;, &quot;Kurtosis&quot;) row.names(empty) &lt;- c(&quot;Weight&quot;, &quot;Fitness&quot;) empty # output is a data frame ## Mean SD Kurtosis ## Weight 79.84 11.88 -0.12 ## Fitness 4.46 1.61 -0.55 4.6 More advanced plotting OK, now we have, at some length in both labs 1 and 2, looked at the basic forms of plots in R and some basic skills of manipulating and using objects in R. Now let’s start adding a bit of finesse to our plots and some additional features. The examples below are in no way exhaustive of the HUGE variety of things you can do with plots in R, but they should give you a feel for what is possible. Let’s start with our scatterplot from above and add some basic options. # Let&#39;s start with labels plot(norm, data[, 5], main = &quot;Association Between \\nFitness and Weight&quot;, xlab = &quot;Fitness Normal Distribution&quot;, ylab = &quot;Weight&quot;) As something new, we can use the \\n to split titles across multiple lines. Now how about adding bold and italics to labels? We can do this with expression; plot(norm, data[, 5], main = &quot;Association Between \\nFitness and Weight&quot;, xlab = expression(bold(Fitness~Normal~Distribution)), ylab = expression(italic(Weight))) Note, when using expression(), we need to use a ~ to connect words. Given the plot above, use what you learnt last week to change the colour, size and symbol used for all the points. 4.6.1 Pinpointing data OK, now let’s see how we can pick out single points, and include plots for multiple groups. So, to identify a single point (or multiple points), we want to create an object which gives the x and y coordinates of the points; y_coord &lt;- data$weight[90] #take 90th element in weight x_coord &lt;- norm[90] # take 90th element in norm # create a vector of weight values for elements 1 to 10 y2_coord &lt;- data$weight[1:10] x2_coord &lt;- norm[1:10] # and the same for norm plot(norm, data[, 5], main = &quot;Association Between \\nFitness and Weight&quot;, xlab = expression(bold(Fitness~Normal~Distribution)), ylab = expression(italic(Weight))) # we can then use points to adjust particular points give the co-ordinates above points(x_coord, y_coord, pch = 15, col = &quot;red&quot;) points(x2_coord, y2_coord, pch = 15, col = &quot;green&quot;) Task: Using xy.coords can be a little tricky at times. Look up the function identify() and see if you can do the same thing.   So that is individual points, now how about 2 groups on the same plot. Let’s have the points for psychologists and linguists coloured differently on our scatterplot. plot(norm, data[, 5], main = &quot;Association Between \\nFitness and Weight&quot;, xlab = expression(bold(Fitness~Normal~Distribution)), ylab = expression(italic(Weight)), pch = 15, col = c(&quot;blue&quot;, &quot;red&quot;)[data$programme]) Here we use data$programme to index the colour vector. Provided the variable we use is a factor, and the length of the col vector matches the number of levels, this will work for us. Now we have different coloured points on the plot, we had better add a legend to explain what they are. So; plot(norm, data[, 5], main = &quot;Association Between \\nFitness and Weight&quot;, xlab = expression(bold(Fitness~Normal~Distribution)), ylab = expression(italic(Weight)), pch = 15, col = c(&quot;blue&quot;, &quot;red&quot;)) legend(&quot;topleft&quot;, legend = c(&quot;Psychology&quot;, &quot;Linguistics&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = c(15, 15)) Task: Look at ?legend, and think through why we have put in all the elements we have. Particularly think about why we have used c() to create the different column vectors. As a practice task, change the points symbols and colours in the plot and legend, and move the legend to a different location. plot(norm, data[, 5], main = &quot;Association Between \\nFitness and Weight&quot;, xlab = expression(bold(Fitness~Normal~Distribution)), ylab = expression(italic(Weight)), pch = 20, col = c(&quot;purple&quot;, &quot;darkgreen&quot;)) legend(&quot;bottomright&quot;, legend = c(&quot;Psychology&quot;, &quot;Linguistics&quot;), col = c(&quot;purple&quot;, &quot;darkgreen&quot;), pch = 20, bty=&quot;n&quot;) # gets rid of the box around legent (box type = none) Hint: Look up the help for the function locator(). This has multiple uses, much like identify(), and may be of use when producing your own plots.   One last thing, lets add some lines to our scatterplot. There are multiple reasons why we may want to add lines. Below I add regression lines for psychologists and linguists (more about this in the GLM lectures), vertical and horizontal lines, and curved loess fit lines. Clearly, we would never want all these on one plot, but this is simply to show what can be done. So; plot(norm, data[, 5], main = &quot;Association Between \\nFitness and Weight&quot;, xlab = expression(bold(Fitness~Normal~Distribution)), ylab = expression(italic(Weight)), pch = 15, col = c(&quot;blue&quot;, &quot;red&quot;)) legend(&quot;topleft&quot;, legend = c(&quot;Psychology&quot;, &quot;Linguistics&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = c(15, 15)) abline(v = 0, col = &quot;green&quot;, lwd = 2) # vertical line at norm = 0 # horizontal line at weight = 100 abline(h = 100, col = &quot;black&quot;, lty = 5, lwd = 2) # regression line linguists abline(lm(data[, 5]~norm, subset = data$programme==&quot;Linguistics&quot;), col = &quot;red&quot;, lwd = 2) # regression line psychologists abline(lm(data[, 5]~norm, subset = data$programme==&quot;Psychology&quot;), col = &quot;blue&quot;, lwd = 2) # lowess line of best fit lines(lowess(norm, data[, 5]), col = &quot;purple&quot;, lwd = 2) 4.6.2 Customising plots As noted above, many of the plotting features are general across types of plot. One of the last things we should look at is changing the axes on a plot. hist(data$weight, main = &quot;Histogram for Weight&quot;, xlab = &quot;Weight&quot;, ylab = &quot;Frequency&quot;, col = &quot;red&quot;) So this plot now has some pretty colours, but our axes don’t look very good. The x and y axes don’t join for a start. So let’s get rid of these and draw them ourself: hist(data$weight, main = &quot;Histogram for Weight&quot;, xlab = &quot;Weight&quot;, ylab = &quot;Frequency&quot;, col = &quot;red&quot;, axes = F) # this line removes the axes from the plot And now we can add some new ones using axis(): hist(data$weight, main = &quot;Histogram for Weight&quot;, xlab = &quot;Weight&quot;, ylab = &quot;Frequency&quot;, col = &quot;red&quot;, axes = F) # this puts an L shaped box (2 sides) onto our plot box(bty = &quot;L&quot;) # we can use axis for each side to put the default check marks back axis(side = 1, xpd = T) axis(side = 2, xpd = T)   If we want to plot two density plots for two groups on the same plot, there is a useful function in the sm package to help us (we can use all the normal graphical parameters here: Task: Install and load the package sm. #install.packages(&quot;sm&quot;) library(sm)   This is how it’s done: sm.density.compare(data$weight, data$programme, col = c(&quot;blue&quot;, &quot;red&quot;), lty = c(1, 2)) legend(&quot;topleft&quot;, levels(data$programme), lty = c(1, 2), col = c(&quot;blue&quot;, &quot;red&quot;))   4.7 Multiple Plots in One So we have now done some fancy things with the basic plots for different types of data. There is one more step that is important to highlight to you with respect to plotting in R. Not only can we have quite fine control over the plots themselves, we can also control the space that the plots are drawn in. The way in which we do this is by adjusting the basic graphical parameters. So let’s follow through a simple example of plotting two plots on top of each other. We will use the code developed above for the plots, so all we need to worry about here is adjusting the space. # first we use the mfrow command to tell R we want to divide the plotting space par(mfrow = c(2, 1)) # then we give it the two plots. # Automatically the first set of code plots the top plot, # the second the lower plot sm.density.compare(data$weight, data$programme, col = c(&quot;blue&quot;, &quot;red&quot;), lty = c(1, 2)) legend(&quot;topleft&quot;, levels(data$programme), lty = c(1, 2), col = c(&quot;blue&quot;, &quot;red&quot;)) hist(data$weight, main = &quot;Histogram for Weight&quot;, xlab = &quot;Weight&quot;, ylab = &quot;Frequency&quot;, col = &quot;red&quot;) 4.7 Plot margins too small? If you get an error complaining about plot margins being too small, that is beause the plotting window in the bottom right panel of your RStudio is, well, too small for the plot to be rendered. Making it bigger should help. Task: Use everything you’ve learnt today to create this figure with four plots. # first we use the mfrow command to tell R we want to divide the plotting space par(mfrow = c(2, 2)) # then we give it the two plots. # Automatically the first set of code plots the top plot, # the second the lower plot sm.density.compare(data$weight, data$programme, col = c(&quot;blue&quot;, &quot;red&quot;), lty = c(1, 2)) legend(&quot;topleft&quot;, levels(data$programme), lty = c(1, 2), col = c(&quot;blue&quot;, &quot;red&quot;)) hist(data$weight, main = &quot;Histogram for Weight&quot;, xlab = &quot;Weight&quot;, ylab = &quot;Frequency&quot;, col = &quot;darkred&quot;) plot(data$programme, main = &quot;Bar chart for Programme&quot;, xlab = &quot;Weight&quot;, ylab = &quot;Frequency&quot;, col = &quot;darkblue&quot;) boxplot(data$weight ~ data$programme , main = &quot;Boxplot for Weight by Programme&quot;, xlab = &quot;Weight&quot;, ylab = &quot;Frequency&quot;, col = &quot;darkgreen&quot;) par(mfrow = c(1, 1)) Sometimes we may want more space on one side of our plot. If this is the case, we need to change the margins in the plotting space. We do this within par() as well. First though, it can be useful to see what our actual margins are. We can see this by simply typing par(). This produces quite a lot of output as it lists all the graphical parameters settings. But we can just look at the margins using; check &lt;- par() check$mar ## [1] 5.1 4.1 4.1 2.1 Here you have four numbers corresponding to the sides of the plot 1 = bottom = 5.1 2 = left = 4.1 3 = top = 4.1 4 = right = 2.1 To change the margins we simple add; # first we use the mfrow command to tell R we want to divide the plotting space # then change margins par(mfrow = c(2, 1), mar = c(3, 1, 1, 1)) # once graphical parameters are changed, you can create plots sm.density.compare(data$weight, data$programme, col = c(&quot;blue&quot;, &quot;red&quot;), lty = c(1, 2)) legend(&quot;topleft&quot;, levels(data$programme), lty = c(1, 2), col = c(&quot;blue&quot;, &quot;red&quot;)) hist(data$weight, main = &quot;Histogram for Weight&quot;, xlab = &quot;Weight&quot;, ylab = &quot;Frequency&quot;, col = &quot;red&quot;) You can see that these are not very good settings. We have lost the title on the top plot, and we have lost the axes labels on the left hand side. However, this does shows you the basics of how to adjust the margins. We will want to set out plotting space back to the default. We do this by simply clicking the “Clear All” option on the plotting screen. That wraps up the tutorial part of the lab for now. We will do more with plotting in the course of future labs, but for now, this will do us. 4.8 Let’s put all this into practice Start a new project called something like “Lab 4 Real Data”. This will make it easier to reference back to your project code (above) when answering the questions. In all the examples above, we have been using randomly generated data. Now let’s (assume!!) we have got some real data (i.e., you have not seen me simulate it). So, let’s get practising, describing and visualizing our data. First, read the data into R. Look back at the instructions from week 2 if you can not remember how to do this. The data is a .csv file, so you can also look at ?read.csv. The data set is saved on LEARN in the Week 4 folder. OK, so now I have a list of tasks. All of these should be achievable by looking at the code above and adjusting it for the new names of variables, size of the data set, etc. First, here is some information about the variables. You will notice there is very little useful labelling in the data set. 4.8.1 Code Book Variables in the order they appear in the data set: Daily energy expenditure (vigorous activity) Daily energy expenditure (light activity) Programme (0 = Linguistics; 1 = Psychology) BDNF-alpha genotype (0 = Risk Allele Not Present ; 1 = Risk Allele Present) Attitude to exercise (1-8; 1 = lowest, 8 = highest) 4.8.2 Questions Add an ID to the data set. Hint: see Week 2 lab Name all variables. Add labels to the programme and BDNF-alpha variables. Produce a descriptives table with appropriate measures of central tendency and a column for sample size. Produce a frequency plot for Attitude to Exercise and a density plot for Daily energy expenditure (vigorous). Plot the association between Daily energy expenditure vigorous activity and light activity. Produce a plot to show the relative frequencies of BDNF-alpha risk alleles among psychologists and linguists. Produce a plot comparing the densities of daily energy expenditure (light activity) in psychologists and linguists. Combine these four plots into one figure.   Good luck!     4.9 Lab 4 GGPLOT 4.9.1 Introduction Hopefully by now you are starting to see the power of R and the benefits of using such a tool compared to something like SPSS. Once you have mastered some basics you have the skills to go on and complete any number of tasks that wouldn’t be possible without having a full programming environment to work in. The real beauty of R is a combination of it being free to use and the package system for accessing ready-made functions/procedures for completing tasks. We rarely have to start from scratch if we have a complicated task we want to carry out using R (there is most probably a package for it), and if we don’t like the way some things are done by default we don’t have to accept it (there is most probably a package that does it differently). One such example of the second situation is visualising data using the plot() family of functions that is part of the base R installation. While plot can be made to do almost anything you could want it to do, there are alternatives. A package that provides an alternative option for plotting data is ggplot2 by Hadley Wickham. 4.9.2 ggplot2 The syntax for building a graphic in ggplot2 is different from the basic plot() method. Some of you may prefer it, some may not. The point is to use this is an example of how - by using R - we are not forced to do things a certain way but instead can pick from different options (or create our own). Go ahead and install ggplot2 and load the library. # install.packages(&quot;ggplot2&quot;) library(ggplot2) Now, some data to work with. # install.packages(&quot;gcookbook&quot;) # install.packages(&quot;Ecdat&quot;) library(gcookbook) library(Ecdat) # We can now access a bunch of datasets that are included in these packages. data() # provides a list of datasets currently accessible in your R environment You should see a list of usable datasets under a tab in the editor section of Rstudio (top-left by default). You can use the standard help method to get information on these datasets e.g., ?aapl ?Star. 4.9.3 qplot() We will start our exploration of ggplot2 with the qplot() function. qplot() works just like plot() in that it is usually very smart and creates the type of plot you want based on the input. For example, when we pass qplot() a single continuous variable it assumes we want a histogram. qplot(data = Star, tmathssk, binwidth = 25) If we give qplot() two numeric variables then it produces a scatterplot. qplot(data = Star, tmathssk, treadssk) If we provide a single factor variable we get a ‘bar chart’ showing the counts for each level of the factor. qplot(data = Star, classk) Many of the optional arguments we have played with so far in our plot() figures have an equivalent in ggplot2. Take this plot for example where we use variable mapping to colour points based on levels of a factor: qplot(data = Star, tmathssk, treadssk, colour = sex) There are plenty of other options and qplot() is clever enough to figure out what king of plot you probably have in mind depending on which arguments you specify. Take for example the fill option: if you specify the fill colour, it will figure out that you are probably looking for a barchart. qplot(data = Star, classk, fill = sex) How about time-series type data such as the apple stock price information in the ‘aapl’ data. Notice the use of the geom = argument. Try it without this argument to see what qplot will do by default and why we might want to set geom = &quot;line&quot;. qplot(data = aapl, date, adj_price, geom = &quot;line&quot;) 4.9.4 Your Turn Using qplot() create the following plots: Use the ‘mtcars’ dataset to produce a scatter plot visualising the relationship between horsepower and miles per gallon. Also make the points a different colour based on whether the car is an automatic or manual transmission. think about types of variable and conversion methods Use the ‘worldpop’ dataset to plot a similar time-series illustration for the growth in world population over the timespan provided. Make your plot show a line describing the relationship but also include the specific points at each time point. You need to think about how you might provide two values to one of the arguments. qplot(data = mtcars, hp, mpg, colour = factor(am, labels = c(&quot;auto&quot;,&quot;manual&quot;))) + labs(colour = &quot;Transmission&quot;) qplot(data = worldpop, Year, Population, geom = c(&quot;line&quot;,&quot;point&quot;)) There are many other features that can be used in conjunction with qplot() to get the plot needed. Feel free to use the help ?qplot as well as other online resources to explore these more. Don’t be afraid to just try out changing some of the default values or by specifying extra arguments to simply observe the changes it creates. There is no better way to learn how to get the most out of ggplot than to execute code and observe the results. This section has served as a little introduction to qplot() but ggplot() is the meatier function in the ggplot2 package. And it is to ggplot() that we now turn. 4.9.5 Using ggplot() Using the ggplot() function creates a ggplot object within the R environment that is then parsed to create the plot we have defined. By using ggplot() rather than qplot() we can do a lot more. As an introduction to the syntax, let’s look at the code necessary to reproduce some of the plots shown above using ggplot() instead of qplot(). # Scatter plot between math and reading scores in the Star dataset. ggplot(data = Star) + geom_point(aes(x = tmathssk, y = treadssk)) # Bar chart showing frequencies of each level of the classk variable. ggplot(data = Star) + geom_bar(aes(x = classk)) # Specifying fill within aes() to be the sex variable plots a barchart of classk by sex. ggplot(data = Star) + geom_bar(aes(x = classk, fill = sex)) # Adding the position = &quot;dodge&quot; unstacks the bars. ggplot(data = Star) + geom_bar(aes(x = classk, fill = sex), position = &quot;dodge&quot;) # Reproduce the scatter plot but colour points based on the sex variable. ggplot(data = Star) + geom_point(aes(x = tmathssk, y = treadssk, colour = sex)) # Time-Series view of the apple stock price ggplot(data = aapl) + geom_line(aes(x = date, y = adj_price)) Try these in your R window and see that they produce the same plots as above. By using ggplot() we are now starting to use the layering approach to making our graphics. For the vast majority of our needs the following formula will work for us: ggplot(data) + geom_xxxx(aes()) + extra_option_1() + extra_option_n() We start by creating a ggplot object that contains the data we are going to be using ggplot(data). We then need to tell ggplot what type of ‘thing’ we want to add to the plot in the form of a ‘geom’. We have already encountered geom_point(), geom_line(), and geom_bar() in the examples above. For a full list of available geoms see the documentation here. Within the geom_xxxx() call we map variables to aesthetics within the aes() command. Tip: Use ?geom_xxxx to see what aesthetics can be set (some required, some optional) Then we can build on the basic plot those steps create by adding other ‘geoms’ or options on top of that. Let’s put that in to action. The ‘movies’ dataset included in the ggplot2movies package contains information on over 50,000 films. Install the package if you haven’t aleady done so and load it. Use the help page (?movies) to see what some of the included variables are. Let’s use this dataset to create some plots summarising some of this information. # install.packages(&quot;ggplot2movies&quot;) mov &lt;- ggplot2movies::movies # puts movies in my environment so I can &#39;see&#39; it mov$cyear &lt;- cut(mov$year,5) mov$cyear &lt;- factor(mov$cyear, labels = 1:5) # Here, we have added a categorical variable that reflects the age of the movies. # The movies span a period of approx 100 years (range(mov$year)) so each level of # the new cyear variable reflects approximately a 20-year window where 1 # represents the oldest movies and 5 the newest movies. # define our initial ggplot object with the data to use. epic_plot &lt;- ggplot(data = mov) epic_plot + geom_point(aes(x = cyear, y = rating), position = &quot;jitter&quot;) # confused by position = &quot;jitter&quot;, run the command without it and compare. # We have a LOT of films in the more recent categories. We can use # the alpha option to add transparency to the points to make it easier to see. epic_plot + geom_point(aes(x = cyear, y = rating), position = &quot;jitter&quot;, alpha = 0.1) # A boxplot might be more informative for summarising this data....but we # want to keep the raw data points plotted also. We can add an additional # layer to the plot: epic_plot + geom_point(aes(x = cyear, y = rating), position = &quot;jitter&quot;, alpha = 0.1) + geom_boxplot(aes(x = cyear, y = rating)) # Notice that we have added to the plot we previously had, the addition # of the geom_boxplot layer hasn&#39;t interfered with what came before it. # Re-do that plot with some additional options to the boxplot_geom epic_plot + geom_point(aes(x = cyear, y = rating), position = &quot;jitter&quot;, alpha = 0.1) + geom_boxplot(aes(x = cyear, y = rating), fill = NA, outlier.colour = NA, colour = &quot;green&quot;) # Let&#39;s tidy up the labels so we can show off our plot epic_plot + geom_point(aes(x = cyear, y = rating), position = &quot;jitter&quot;, alpha = 0.1) + geom_boxplot(aes(x = cyear, y = rating), fill = NA, outlier.colour = NA, colour = &quot;green&quot;) + labs(title = &quot;Film Quality Over Time&quot;, x = &quot;Year Quintile&quot;, y = &quot;IMDB User Rating&quot;) Phew! That may have felt a bit long-winded because of the repeating code but in practice we wouldn’t have needed to repeat so much, we did it in more stages than necessary to learn from it. The principle of building up a graphic in layers is evident from this example. 4.9.5 Aesthetics Note that when we are mapping variables to aesthetics we put that code inside the aes(), but when we are setting an aesthetic property to a static value it goes outside the aes(). 4.9.6 Your Turn Let’s have a closer look at whether film length might be related to user ratings. # First inspect the length variable library(psych) describe(mov$length) ## vars n mean sd median trimmed mad min max range skew ## X1 1 58788 82.34 44.35 90 84.63 17.79 1 5220 5219 31.07 ## kurtosis se ## X1 3341.86 0.18   Some films in the database have extreme lengths (both short and long, look at the min and max). For the purposes of this exercise we will only look at standard film lengths and we will also narrow the data down to popular films (based on the number of ratings made). Execute the following commands: mov &lt;- subset(mov, length &gt; 75 &amp; length &lt; 200) mov &lt;- subset(mov, votes &gt; 2000) # use ?subset if you are unsure how this command works. describe(mov$length) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 3005 109.81 19.12 106 107.82 16.31 76 198 122 1.21 2.17 ## se ## X1 0.35 describe(mov$votes) ## vars n mean sd median trimmed mad min max range ## X1 1 3005 9844.78 13204.39 5376 6984.08 4133.49 2001 149494 147493 ## skew kurtosis se ## X1 4.35 26.73 240.88   Now that the mov dataset is restricted to only movies within our acceptable range of values, produce the following plots exactly as seen here: epic_plot_2 &lt;- ggplot(data = mov) epic_plot_2 + geom_point(aes(x = cyear, y = length), position = &quot;jitter&quot;, alpha = 0.5) + geom_boxplot(aes(x = cyear, y = length), fill = NA, outlier.colour = NA, colour = &quot;pink&quot;) + labs(title = &quot;Movie Length Over Time&quot;, x = &quot;Year Quintile&quot;, y = &quot;Movie Length (minutes)&quot;) mov2 &lt;- subset(mov, year &gt; 1980) epic_plot_3 &lt;- ggplot(data = mov2) epic_plot_3 + geom_point(aes(x = year, y = length), position = &quot;jitter&quot;, alpha =0.5) + labs(title = &quot;Film Length by Year (1980-2005)&quot;, x = &quot;Year Released&quot;, y = &quot;Film Length (Minutes)&quot;) epic_plot_4 &lt;- ggplot(data = mov) epic_plot_4 + geom_point(aes(x = length, y = rating)) + geom_smooth(aes(x = length, y = rating)) + labs(title = &quot;Relationship Between Movie Length and User Rating&quot;, x = &quot;Movie Length (minutes)&quot;, y = &quot;Mean IMDB User Rating&quot;) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Tip: For the final plot look up ?geom_smooth   That’s it for now regarding ggplot2 although we’ve only really scratched the surface of its potential. You are encouraged to keep exploring!       "],
["lab-5.html", "Chapter 5 Lab 5 5.1 Finally stats 5.2 Let’s stats!", " Chapter 5 Lab 5 5.1 Finally stats Welcome to lab 5! Today, we will dip our toes in the wonderful world of statistical analysis. We’ll use R to gain some experience analysing data. Task: You know what’s coming, don’t you? That’s right, create a new R project. TIP: You really want to work from both the script and the console here. Any commands that modify the data or create plots and output analysis that you want to have access to belong in the script. Any other commands (one-off stuff) should be left for the console. 5.1.1 Data set Big brother has been collecting some data on you. Go on LEARN, fetch the ‘Lab5_data.csv’ file in the Week 5 folder and save it into your project directory. Here is the code book for the data set id: categorical; student ID number lab: categorical with 2 levels (&quot;Thursday&quot;, &quot;Friday&quot;); student’s lab group programme: categorical with 3 levels (1 = &quot;Psychology&quot;, 2 = &quot;Linguistics&quot;, 3 = &quot;CogScience&quot;); programme of study lab_time: numeric, integer; average time in minutes spent on the lab sheets across last 4 labs tasks3: numeric, integer; number of tasks completed in lab 3, maximum 18 tasks4: numeric, integer; number of tasks completed in lab 4, maximum 18 5.1.2 Read in and clean data Task: Read the data into R. Use the read.csv() function to save the data set into an object called data. Hint. If you save the file somewhere else than your project directory, you will have to specify the path to it so that R knows where to find it. data &lt;- read.csv(&quot;data/Lab5_data.csv&quot;)   As a first step in analysis, you need to familiarise yourself with the data. looking at the first few rows is a good way of doing this. Task: Use the head() function to see the first few rows of data. head(data) ## V_1 V_2 V_3 V_4 V_5 V_6 ## 1 1 Thursday 1 100 10 12 ## 2 2 Friday 3 NA NA NA ## 3 3 Thursday 1 NA NA 4 ## 4 4 Friday 2 115 11 13 ## 5 5 Friday 2 104 6 12 ## 6 6 Friday 3 75 5 2   OK, you can probably see that the data set isn’t quite ready for analysis. This is often the case with real data. You might get your hands on all sorts of data sets collected by people with varying degrees of methodological prowess. Getting data into shape for analysis can be a time-consuming and frustrating process and the ability to do it is a crucial skill. Task: Look, the data set doesn’t have proper variable names, just some V labels. Let’s remedy this. Name the columns according to the code book. Hint. The names() function will be handy here names(data) &lt;- c(&quot;id&quot;, &quot;lab&quot;, &quot;programme&quot;, &quot;lab_time&quot;, &quot;task3&quot;, &quot;task4&quot;)   Now that our variables have informative names, let’s look at some descriptive statistics. This is a quick way of spotting obvious errors in the data set. Task: Use the describe() function from package psych to cast a first glance on your data. Hint. Don’t forget that to use functions from packages that are not pre-loaded at the start of the R session, you have to load them using library(). library(psych) describe(data) ## vars n mean sd median trimmed mad min max range skew ## id 1 167 84.00 48.35 84 84.00 62.27 1 167 166 0.00 ## lab* 2 167 2.39 1.50 1 2.36 0.00 1 5 4 0.16 ## programme 3 167 1.96 0.82 2 1.95 1.48 1 3 2 0.08 ## lab_time 4 155 103.79 90.92 100 98.71 14.83 -78 1200 1278 11.24 ## task3 5 157 6.32 3.89 6 6.00 4.45 1 21 20 0.84 ## task4 6 166 7.83 3.59 8 7.93 3.71 -2 16 18 -0.21 ## kurtosis se ## id -1.22 3.74 ## lab* -1.96 0.12 ## programme -1.53 0.06 ## lab_time 133.42 7.30 ## task3 0.67 0.31 ## task4 -0.30 0.28   This should be raising some red flags! Check out the minima and maxima of the variables. Is this what you’d expect based on what the code book says? No, I didn’t think so… :) Task: What class is id? It’s a categorical variable so let’s tell R to treat is as such. Hint. You want to turn it into a factor. data$id &lt;- factor(data$id) Task: Can you spot what’s wrong with the lab variable? How many levels does it have? How many is it supposed to have? Can you fix it? Read on for a more step-by-step guide if you’re feeling a bit lost. table(data$lab) # we have some typos! ## ## Friday Fryday Thrusday Thursday Tuesday ## 89 1 1 75 1 Hint. table() is a useful function for categorical variables/factors. It let’s you know what different values there are in the data and what their frequencies are.   You might have noticed that lab is already a factor. That is because the read.csv() function turns variables with character strings. You can easily change factors into character vectors using as.character() but, as a general rule, it might be worth reading data in the rawest possible format. Task: Can you figure out how to tell read.csv() not to turn strings into factors? Hint. Look up the help file for the function and read through the list of arguments it takes. read.csv(..., stringsAsFactors = F)   The nice thing about the script (you have been using the script, haven’t you?) is that if you realise you should have done something slightly different earlier in the process of cleaning and analysing your data, you can just edit and re-run the code. Task: Edit the script code to read in the data without turning strings into factors and re-run the remainder of the code you’ve written thus far. # only these 3 lines are required data &lt;- read.csv(&quot;data/Lab5_data.csv&quot;, stringsAsFactors = F) names(data) &lt;- c(&quot;id&quot;, &quot;lab&quot;, &quot;programme&quot;, &quot;lab_time&quot;, &quot;task3&quot;, &quot;task4&quot;) data$id &lt;- factor(data$id)   Now that’s done, you can do something about the lab variable. Task: Change the inappropriate values of lab. Hint. This is where logical subsetting comes in handy so good thing you spilled so much sweat over it! # only these 3 lines are required data$lab[data$lab == &quot;Thrusday&quot;] &lt;- &quot;Thursday&quot; data$lab[data$lab == &quot;Fryday&quot;] &lt;- &quot;Friday&quot; data$lab[data$lab == &quot;Tuesday&quot;] &lt;- NA # I think it&#39;s best to get rid of this one Want more hints? To refresh your memory, something like data$variable[data$variable == &quot;value&quot;] &lt;- &quot;different value&quot; does the trick.   Analytical decisions You will have noticed that one of the weird values in data$lab is not really a typo. What to do about it is sort of your call. You might argue that it is a typo and change it to the value it resembles. However, it is possible that the data is self-reported, in which case you might legitimately ask if including data from some space cadet who doesn’t know what day it is is useful. I would probably exclude this person’s entry for the lab variable (by replacing it with NA). I would not recommend ever deleting entire rows from your data frames. Task: Good, now you can change the variable into a factor. data$lab &lt;- factor(data$lab)   So that’s lab sorted. Let’s move on to programme. As you can see, it only has numeric values. It is also not a factor. It would be good to do something about this, don’t you think? Task: First things first though! Have a closer look at the variable. Does it look fine? Task: If you’re happy with it, tell R it’s a factor with labels as per the code book. Hint. There are several ways to do this. For instance, you can use the factor(..., labels = ...) approach, or opt for the recode() function from package car. data$programme &lt;- factor(data$programme, labels = c(&quot;Psychology&quot;, &quot;Linguistics&quot;, &quot;CogScience&quot;))   Grand! Moving on to lab_time. Definitely something fishy going on here. Can you spot what it is? Task: Fix the lab_time so that it conforms to the criteria in the code book. Hint. There are 3 problems with the values: there are impossible ones, improbable ones, and the format of the numbers suggests that there are also non-integer ones. Dealing with the third issue is going to be a bit of a puzzle. 5.1.3 Impossible values It stands to reason that average time spent on the lab sheets should not be negative. Task: Get rid of the negative values by replacing them with NAs. Hint. again, logical subsetting is the way to go! data$lab_time[data$lab_time &lt; 0] &lt;- NA   5.1.4 Improbable values Task: Have a look at the distribution of lab_time. Use a histogram or a box plot to visualise it. boxplot(data$lab_time) You should hopefully see that there’s an improbably large value in the variable. Task: Choose a safe cut-off point and use it to remove the offending data point. data$lab_time[data$lab_time &gt; 200] &lt;- NA # only one point larger than 200   Analytical decisions Should we have really removed the poor person who spends on average 20 hours on a lab sheet? Is it a typo? Probably… But what if not? Well, if not then, first of all, I sincerely apologise to this, brave, tenacious hero! That said, they are probably quite an extreme outlier. I think it’s safe to remove this value, what do you think? 5.1.5 Non-integer values Have a look at this: class(data$lab_time) ## [1] &quot;numeric&quot; class(data$task3) ## [1] &quot;integer&quot; You see that task3 is a vector of integers, just like we’d expect it to be. lab_time, however, is not. The only reason for that is that at least one of the values of lab_time is a decimal number and so the entire vector must be decimal. Since the code book above tells us that lab_time is supposed to be an integer, any decimal values must be typos. To get rid of them, we first need to identify them Task: How can we programmatically spot non-integers?   Well, non-integers will – by definition – not be divisible by 1 without a remainder. That means that if we can somehow ask R to give us the remainder of division by 1, any numbers that are not zero will identify non-integers. As luck would have it, the mathematical operation we are looking for is called modulo and is implemented in R as the %% operator: 32.452 %% 1 ## [1] 0.452 Putting it all together, lab_time modulo 1 should be 0. If it’s not, we should get rid of the value. Task: Use %% 1 and logical subsetting to get rid of non-integer values. data$lab_time[data$lab_time %% 1 != 0] &lt;- NA   Task: If we want, we can now safely turn lab_time into integer. Hint. as.integer(). data$lab_time &lt;- as.integer(data$lab_time)   Two more variables to go. These are fairly straight-forward so I’ll let you deal with them unaided. Just apply the same reasoning and principles as we did above. Task: Clean the tasks3 and tasks4 variables so that they conform to the code book rules. data$task3[data$task3 &lt; 0 | data$task3 &gt; 18] &lt;- NA data$task4[data$task4 &lt; 0 | data$task4 &gt; 18] &lt;- NA   I think this is us, we have ourselves some nice and clean data. If you’ve done everything with me, you should see this output if you ask for describe(data) again: describe(data) ## vars n mean sd median trimmed mad min max range skew ## id* 1 167 84.00 48.35 84 84.00 62.27 1 167 166 0.00 ## lab* 2 166 1.46 0.50 1 1.45 0.00 1 2 1 0.17 ## programme* 3 167 1.96 0.82 2 1.95 1.48 1 3 2 0.08 ## lab_time 4 152 98.45 12.44 100 98.91 14.83 73 119 46 -0.27 ## task3 5 155 6.15 3.59 6 5.91 4.45 1 15 14 0.50 ## task4 6 163 7.99 3.40 8 8.04 2.97 1 16 15 -0.04 ## kurtosis se ## id* -1.22 3.74 ## lab* -1.98 0.04 ## programme* -1.53 0.06 ## lab_time -0.98 1.01 ## task3 -0.58 0.29 ## task4 -0.56 0.27 The take-home message is that, in order to produce sensible analyses, you must make sure you work with sensible data. Cleaning data requires a great deal of attention to detail, thinking, and, well, coding. Having done that, we can move on to finding out about the world… 5.2 Let’s stats! In this section, we will ask and answer questions about our variables and relationships between them. What I’d like you to do is to run at least one test “by hand”, i.e., without using a built-in R function. Doing this will help you understand the basic principle common to all statistical tests we will cover on this course. 5.2.1 Categorical variables Let’s have a look at lab again. This time, we want to know if the number of students attending the Thursday lab is the same as the number attending the Friday lab. In other words, \\[H_0: P(Thursday) = P(Friday)\\] To test this null hypothesis, let’s first simply look at the frequency table. Task: Ask R to dive you a table of the lab variable. Save this table into an object called lab_table since it will come in handy later. Once that’s done, look at it in the console. lab_table &lt;- table(data$lab) lab_table ## ## Friday Thursday ## 90 76   If you want, you can visualise this distribution in a plot, such as this one: barplot(lab_table) We can test our hypothesis using a Chi-square test (specifically a goodness-of-fit test). In other words, we want to see if the distribution of lab is significantly different from uniform. To do that, we first need to know what frequencies we expect under a uniform distribution. Task: Create an object exp with the expected frequencies. Try doing this programmatically rather than manually typing in the numbers. Hint. The expected frequency of each category is half of all the observations. exp &lt;- rep(sum(lab_table)/2, 2) exp ## [1] 83 83   Now we want a measure of just how much our observed distribution deviates from the expected one. Squares of differences are a good measure (if we just took differences they would cancel each other out), however, a difference of 1 if the expected value is 100 isn’t the same as a difference of 1 if the expected value is 10. So to get a more proportional measure, let’s scale the squared differences between the observed and expected counts by the expected counts (\\((obs - exp)^2/exp\\)) Task: Do this in R! Create an object sq_scaled that includes these scaled squared differences sq_scaled &lt;- (lab_table - exp)^2/exp sq_scaled ## ## Friday Thursday ## 0.5903614 0.5903614 We still have as many numbers as we have categories. To get a single measure of the departure from expectation, let’s just add the numbers up. Since this is our test statistic, let’s call it chi_sq. Task: Calculate the chi_sq statistic. chi_sq &lt;- sum(sq_scaled) chi_sq ## [1] 1.180723   We know that this test statistic follows the \\(\\chi^2\\)-distribution. We also know that this distribution changes shape depending on the number of degrees of freedom. So our test statistic will have a different associated p-value under each of these distributions: pchisq(chi_sq, 3, lower.tail = F) ## [1] 0.7576314 pchisq(chi_sq, 8, lower.tail = F) ## [1] 0.9968287 pchisq(chi_sq, 147, lower.tail = F) ## [1] 1 Let’s calculate the degrees of freedom. If you know what the marginal value is, i.e., how many people are there altogether, how many entries in lab_table are free to vary? Task: OK, now that we have the number of dfs figured out, let’s get the p-value associated with our observed chi_sq! p &lt;- pchisq(chi_sq, 1, lower.tail = F) p ## [1] 0.2772089   As a sanity check, we can run the test using R’s chisq.test() function. If you pull up its documentation, you’ll see that for a Chi-square test such as this one, it requires a table as the only argument. Task: Run the test using the R function. chisq.test(lab_table) ## ## Chi-squared test for given probabilities ## ## data: lab_table ## X-squared = 1.1807, df = 1, p-value = 0.2772   Hopefully, you will have got the same results from the by-hand calculation. A Chi-square test showed that the distribution of lab group attendance did not significantly depart from uniform, \\(\\chi^2(1) = 1.18,\\ p=.277\\). Task: Perform a Chi-square test on the distribution of programme separately for the Thursday and Friday labs. The null hypothesis assumes that both are uniform. Write up the results. chisq.test(table(data$programme[data$lab == &quot;Thursday&quot;])) ## ## Chi-squared test for given probabilities ## ## data: table(data$programme[data$lab == &quot;Thursday&quot;]) ## X-squared = 9.0263, df = 2, p-value = 0.01096 chisq.test(table(data$programme[data$lab == &quot;Friday&quot;])) ## ## Chi-squared test for given probabilities ## ## data: table(data$programme[data$lab == &quot;Friday&quot;]) ## X-squared = 2.8667, df = 2, p-value = 0.2385 A Chi-square test showed that the distribution of lab group attendance was significantly different from uniform in the Thursday group (\\(\\chi^2(2) = 9.03,\\ p=.011\\)) but not the Friday group (\\(\\chi^2(2) = 2.87,\\ p=.239\\)). This is the kind of multiple testing that inflates Type I error, so don’t do it in practice! :) Instead, let’s see if there is a relationship between the day on which the lab takes place and who attends it. Task: First, create a contingency table cont, cross-tabulating lab and programme. You can do this in several ways but two of the more convenient ones are using table() or xtabs(). The latter produces a table that looks like this: cont &lt;- xtabs(~ lab + programme, data) cont ## programme ## lab Psychology Linguistics CogScience ## Friday 23 31 36 ## Thursday 37 23 16   Above, we used a Chi-square goodness-of-fit test to see how well a uniform distribution fit our data. We can also use a Chi-square test to compare two (or more) observed distributions in a Chi-square test of independance. To perform a Chi-square test, we need the observed counts, the expected counts and the degrees of freedom. We just got the first one in our contingency table. The latter two can be derived using observed and marginal counts. Marginal counts are called marginal because they are the row and column sums of the contingency table and are, thus, in its margins. Task: Add margins to cont. There is a function that does that. Hint. If you don’t know its name, maybe try asking the internet how to add margins to a table in R. addmargins(cont) ## programme ## lab Psychology Linguistics CogScience Sum ## Friday 23 31 36 90 ## Thursday 37 23 16 76 ## Sum 60 54 52 166   If you want, you can calculate the expected counts by hand. However, what I’d like you to do is figure out how many degrees of freedom there are in this table. If the marginal counts are fixed, how many independent values do you need to know before you know what the others are? Start by fixing the number of the cell [1, 1] to some number. Does that tell you anything about some other values in other cells, given that the marginal counts are fixed? Fix another one, say [1, 2], what about now? Carry on in the same vein until you can solve the table. In a way, it’s a bit like sudoku. In fact, a sudoku with only one solution is basically a table with 0 degrees of freedom! Task: How many dfs do we have?   # you can solve the table once you know 2 values: ## programme ## lab Psychology Linguistics CogScience Sum ## Friday 23 31 90 ## Thursday 76 ## Sum 60 54 52 166 Great! Now, you should understand why the formula for df for Chi-square tests is what it is (\\((r-1)(c-1)\\) where r is the number of rows and c is the number of columns). Let’s run the test itself! There are, yet again, several ways of telling the chisq.test() function that you want to perform the test of independence. You can give it a contingency table (without marginal counts!) or two variables. Consult the function documentations (?chisq.test) for examples and details. Task: Run the Chi-square test of independence to see if the day of the lab is related to the distribution of programme. Write up the results. chisq.test(data$lab, data$programme) ## ## Pearson&#39;s Chi-squared test ## ## data: data$lab and data$programme ## X-squared = 11.042, df = 2, p-value = 0.004002 # equivalent ways # chisq.test(table(data$lab, data$programme)) # chisq.test(xtabs(~ lab + programme, data)) According to a Chi-square test of independence, there was a relationship between the day of the lab and the distribution of attending students with respect to study programme, \\(\\chi^2(2) = 11.04,\\ p=.004\\). Have a think: Was the test we used appropriate for the kind of data we had? In other words, were the assumptions of the test met? Yes, the variables are both categorical, with expected cell counts of \\(&gt; 5\\), and the observations are random and independent. 5.2.2 Analysing continuous data 5.2.2.1 Known \\(\\sigma\\) Let’s look at the average time spent on a lab sheet. What if I told you, that – as far as undergraduate psychology students are concerned – the mean time spent on these kinds of labs sheets is 103.7 minutes with the population std. deviation \\(\\sigma = 14.6\\)? Could you use this knowledge to test whether USMR students are a random sample from the psych undergrad population or not? Task: Try testing the null hypothesis using a 2-tailed z-test. Do it “by hand” (there’s no built-in R function for the z-test anyway). Remember, all you’re doing is calculating the z-statistic and getting the p-value of this or more extreme statistic under the standard normal distribution. Hint: The formula for the z-test is \\(z = \\frac{\\bar{x} - \\mu}{SE}\\), with \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\). The function you need to get the p-value is pnorm(). Think about which tail of the normal distribution you’re interested in and how to get the 2-tailed p-value. 5.2.2.1 Warning Watch out for missing values! These are the results you should have got: mu &lt;- 103.7 sigma &lt;- 14.6 n &lt;- length(na.omit(data$lab_time)) z &lt;- (mean(data$lab_time, na.rm = T) - mu) / sqrt(sigma^2/n) p &lt;- round(pnorm(abs(z), lower.tail = F) * 2, 7) z &lt;- round(z, 3) paste(&quot;z =&quot;, z) ## [1] &quot;z = -4.43&quot; paste(&quot;p =&quot;, p) ## [1] &quot;p = 9.4e-06&quot; Task: Write up the results of the z-test. The mean time spent on lab sheets in the sample of USMR students was significantly less than in the population of Psychology undergraduates, \\(z=-4.43,\\ p&lt;10^{-5}\\). 5.2.2.2 Unknown \\(\\sigma\\) As you know, the z-test required the population variance to be known and so I told you what it is so that you could run the test. Let’s try a more realistic scenario, where we don’t know \\(\\sigma^2\\). Let’s keep the hull hypothesis the same tough, \\(H_0: \\mu = 103.7\\). Task: Run the one-sample t-test on the same variable and write up the results. Take note of the similarities and differences between the results of the z- and the t-test. Hint. By default, the test assumes that \\(\\mu = 0\\). The documentation to the t.test() function will help you find out how to change this value. t.test(data$lab_time, mu = mu) ## ## One Sample t-test ## ## data: data$lab_time ## t = -5.2003, df = 151, p-value = 6.375e-07 ## alternative hypothesis: true mean is not equal to 103.7 ## 95 percent confidence interval: ## 96.46076 100.44713 ## sample estimates: ## mean of x ## 98.45395 A one-sample t-test revealed a significant difference in time spent on lab sheets between USMR students (Mean = 98.45, 95% CI [96.46, 100.45]) and the population of Psychology undergraduates, \\(t(151)=-5.2,\\ p&lt;10^{-6}\\). The result of the t-test should be very close to that of the z-test. However, the smaller the sample size, the larger the effect of random statistical fluctuations. Task: Re-run the t-test on a random sample of 20 elements of lab_time to see that sometimes size really does matter! t.test(data$lab_time[sample(n, 20)], mu = mu) ## ## One Sample t-test ## ## data: data$lab_time[sample(n, 20)] ## t = -0.64608, df = 16, p-value = 0.5274 ## alternative hypothesis: true mean is not equal to 103.7 ## 95 percent confidence interval: ## 95.66647 107.98059 ## sample estimates: ## mean of x ## 101.8235   5.2.3 More analysis! Thinking about it, it doesn’t seem to me like we’ve explored lab_time enough yet. Let’s see what else hides within! To encourage some healthy rivalry (for when has that gone wrong?!), let’s compare the Thursday and the Friday folks to see who’s fastest with the lab sheets. We can assume, that people attend either one or the other group. Task: Write down the null hypothesis. Hint. We are looking at differences in means of average time spent on lab sheets in the two groups. \\(H_0: \\mu_{Thursday} = \\mu_{Frisday}\\) Task: OK, let’s test the null with a Welch’s t-test. That’s the one R does by default when you ask it to perform a t-test on independent data. Write up the results. Hint. The function documentation should help you find out how to specify the test formula. If it doesn’t, ask the Internet. t.test(lab_time ~ lab, data) ## ## Welch Two Sample t-test ## ## data: lab_time by lab ## t = -4.5055, df = 143.12, p-value = 1.365e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -12.249376 -4.778753 ## sample estimates: ## mean in group Friday mean in group Thursday ## 94.43038 102.94444 A Welch’s t-test showed that the time spent on lab sheets in the Thursday group was significantly longer than that in the Friday group, Mean diff = \\(-8.51\\), 95% CI [\\(-12.25,\\ -4.78\\)], \\(t(143.12)=-4.51,\\ p&lt;10^{-4}\\). Great! Finally, let’s see if there’s been a change in the number of tasks students are able to complete in the labs between week 3 and 4. Task: What is the null hypothesis and which test is appropriate to test it? \\(H_0: \\mu_{wk3} = \\mu_{wk4}\\) The paired sample t-test is the one to pick as the observations are paired (and therefore not independent) – one observation per week from each student. Task: Run the test and write up the results. t.test(data$task3, data$task4, paired = T) ## ## Paired t-test ## ## data: data$task3 and data$task4 ## t = -7.5341, df = 152, p-value = 4.097e-12 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.582219 -1.509284 ## sample estimates: ## mean of the differences ## -2.045752 “The students spent on average less time on the lab sheet in week 4 compared to week 3, Mean diff = \\(-2.05\\), 95% CI [\\(-2.58,\\ -1.51\\)], \\(t(152)=-7.53,\\ p&lt;10^{-11}\\). You can now do statistical analysis of several types of data. Very well done indeed!   5.2.4 Extra: Get loopy It’s important to understand where p-values come from. Simulating the sampling distribution of the test statistic is the best way of getting your head around the concept. So let’s do that! Given what the sampling distribution is, simulating it involves taking many samples from the population. Every time you encounter a repetitive task you should start thinking about how to delegate as much of the legwork as possible to the computer. This is where loops come in very handy. They are used to repeat a block of code an arbitrary number of times. There are other ways of implementing this in R, you could try using the replicate() function instead. Let’s look at the anatomy of a loop in R: for (i in n) { # code } The for () {} bit is just syntax and its sole function is to tell R that this is a loop. The bit in () defines the iterator variable i and the vector n over which the loop iterates. So for (i in 1:10) will be a loop that runs through 10 times. First, it will assign i the value of 1 and run the rest of the code. Then, it will assign it the next value in the n vector, e.g., 2 and run the code again. It will repeat until it runs out of elements of n. To demonstrate: for (i in 1:5) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 As you can see, the loop printed the value of i for each of the 5 iterations. The code in the body of the loop {} can do whatever you want. you can use the current value of i to create, modify, or subset data structures, to do calculations with it, or simply not use it at all. The world’s your oyster! Finally, loops can be nested, if needed: for (i in n) { for (j in m) { # code } } This will run all the iterations of the inner loop for each iteration of the outer one. Task: Have a go at writing a loop that will print data[1, 1], then data[2, 2], then data[3, 3], etc., for as many times as there are columns of data. Hint. ncol() is a good function to use to specify the vector over which to loop. The output should look like this: for (i in 1:ncol(data)) { print(data[i, i]) } ## [1] 1 ## 167 Levels: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... 167 ## [1] Friday ## Levels: Friday Thursday ## [1] Psychology ## Levels: Psychology Linguistics CogScience ## [1] 115 ## [1] 6 ## [1] 2 Now, often you will want to save the output of each iteration of the loop. There are again many ways to do this but a straight-forward one is to first create an empty object and then start combining it with the output of the loop: out &lt;- c() # empty vector for (i in 1:10) { # append the result of some operation to to the vector out &lt;- c(out, data$lab_time[i * 10]) } out ## [1] NA 93 114 102 107 81 94 88 NA 87 Task: Combine the two tasks above and create a vector out that includes data[1, 1], data[2, 2], data[3, 3], etc. out &lt;- c() for (i in 1:ncol(data)) { out &lt;- c(out, data[i, i]) } out ## [1] 1 1 1 115 6 2 Now that you have an inkling about how to use loops, let’s see if we can make use of them to simulate our sampling distribution Task: First, let’s create an empty vector that will store each estimate of out t-statistic. Call it t_est, why not! t_est &lt;- c() Task: Next, let’s write the core structure of a loop. Say we want the loop to run 104 times. Don’t worry about the code inside {} just yet. for (i in 1:10^4) { # code } Task: Now, inside of the loop, you want to sample the same number of observation as we have in our data from a normally distributed population with \\(\\mu=103.7\\) and SD of sd(data$lab_time, na.rm = T). Save this sample into an object called samp. for (i in 1:10^4) { # I defined both n and mu earlier, you might not have! samp &lt;- rnorm(n, mu, sd(data$lab_time, na.rm = T)) } Tip: You want to make sure each of the constituent commands within the loop run on it own, otherwise the entire loop won’t run either. You can trial-run things in the console. Task: Now, we want to get the t-statistic for this sample and add it to our for now empty t_est vector. You can calculate it by hand if you wish but maybe you can somehow extract it from t.test()? Hint: t.test() returns a list of values. This list, just like any list, can be queried using $. The Value section of the function documentations will tell you the rest. 5.2.4 t.test() Don’t forget to specify the mu = parameter if you are using t.test()! for (i in 1:10^4) { samp &lt;- rnorm(n, mu, sd(data$lab_time, na.rm = T)) t_est &lt;- c(t_est, t.test(samp, mu = mu)$statistic) } Task: That’s it! You can run the loop now and sit back Task: Let’s look at the histogram of t_est. To see it in a higher resolution, let’s ask for 50 breaks. hist(t_est, breaks = 50)   Beautiful, isn’t it? OK, final stretch! We have successfully simulated the sampling distribution of the t-statistic under the null hypothesis. Let’s use it to get a p-value for our observed t (-5.2003) Task: Calculate the 2-tailed p-value using our sampling distribution. Basically, you want to know what proportion of the individual t estimates in the distribution have values that are more extreme than the observed one. ### step by step ## observed t t_val &lt;- t.test(data$lab_time, mu = mu)$statistic t_val ## t ## -5.200296 ## logical condition # abs(t_est) to consider both tails of the distribution # abs(t) to allow for negative t # this will be a LONG vector of TRUEs and FALSEs cond &lt;- abs(t_est) &gt;= abs(t_val) ## how many TRUEs are there? sum(cond) ## [1] 0 ## p-value is the proportion sum(cond)/length(t_est) ## [1] 0   Since none of our simulated estimates met the condition – at least when I did it! –, the simulated p-value is 0. However, this is just an approximation. To say, that there’s zero chance that you would find this or a more extreme result if the null is true, is incorrect! We can safely say, that the p-value is very small, though. Nicely done! This should bring the principle of NHST home.   "],
["lab-6.html", "Chapter 6 Lab 6 6.1 Some more R skills 6.2 Your Turn 6.3 Real Data", " Chapter 6 Lab 6 In today’s lab we are going to start working with some data that looks a little more “real”. What do I mean by real? Well, messy! Up until now we have largely been working with complete data sets where all the values for every variable are within the range we expect and there are no unusual cases. But for any of you who have previously had your hands on real raw data, you will know it rarely looks so neat. So our aims for today are to show you some more R skills for exploring your data to highlight potentially problematic values, and then to put these, and all your other skills to the test to answer some simple research questions using some real (messy) data. So let’s get started. 6.1 Some more R skills 6.1.1 Two types of subscripting Task: Create a vector called “vec” with 10 numbers in it (you could use one of the following functions, sample(), or seq(), for example). vec = seq(2, 20, 2) Task: What does the next command do? vec[8] ## [1] 16 Task: What about the following command? vec[c(2, 8)] ## [1] 4 16 Task: Now try: vec[c(F, T, F, F, F, F, F, T, F, F)] ## [1] 4 16 So what is going on here? In one case the subscripting is being done by index; in the other, it’s being done by truth values (these are called logical subscripts). Having both gives R enormous flexibility when it comes to manipulating data. In general, indexes are what you type in if you want to target specific values, but logical subscripts are the results of tests. Try the following and think about what the output represents: vec2 &lt;- c(1, 2, NA, 4, 5, 6) is.na(vec2) ## [1] FALSE FALSE TRUE FALSE FALSE FALSE vec2 &gt; 4 ## [1] FALSE FALSE NA FALSE TRUE TRUE vec2/2 == 2 ## [1] FALSE FALSE NA TRUE FALSE FALSE What will the next two commands do? Think about it first and then try them out: vec2[vec2 &gt; 4] ## [1] NA 5 6 vec2[vec2 &gt; 4] &lt;- NA For matrices, R indexes by row, then by column in the format matrix[row, column]. Task: Let’s create a matrix: mat &lt;- matrix(rnorm(20, 100, 15), ncol = 5, nrow = 4) # Here we specify some values (20 from a normal(100, 15) distribution) # and then tell R how to organise the matrix (col and row sizes) Task: What does the next command do (you have seen this before in previous labs - think about adding values to the empty matrix)? mat[2, 3] ## [1] 101.6555 Task: What about the next one? mat[ , 3] ## [1] 106.7639 101.6555 67.9537 101.8156 Task: What’s the difference between the folllowing two commands? mat[2, 3] ## [1] 101.6555 mat[c(2, 3), ] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 81.60815 93.44354 101.6555 107.10758 124.3886 ## [2,] 90.11979 94.36973 67.9537 94.17529 106.6617 Note that logical tests respect the shape of the input, as much as possible. To see what this means, try the following and think about what you expect the output to look like first: mat &gt; 100 ## [,1] [,2] [,3] [,4] [,5] ## [1,] FALSE TRUE TRUE TRUE TRUE ## [2,] FALSE FALSE TRUE TRUE TRUE ## [3,] FALSE FALSE FALSE FALSE TRUE ## [4,] TRUE TRUE TRUE TRUE TRUE You can use the matrix of truth values like so (the rows and columns are implicit because the truth values come as a matrix): mat[mat &gt; 100] &lt;- 100 # cap maximum score at 100 throughout 6.2 Your Turn If you’ve followed all this, you should be able to have a good go at the following: Task: Create a 20-row, 2-column matrix called ‘mat2’ filled with random numbers drawn from a normal population with mean 100 and sd 15. mat2 &lt;- matrix(rnorm(40, 100, 15), nrow = 20, ncol = 2) Task: Change the value at the 14th and 18th rows of the first column to 200 (in one command). mat2[c(14, 18), 1] &lt;- 200 Task: Change the value of the 3rd row of the second column to 200. mat2[3, 2] &lt;- 200 Task: Find the mean and the sd of all the values (using mean() and sd()). mean.mat2 &lt;- mean(mat2) sd.mat2 &lt;- sd(mat2) Task: A typical task we want to carry out when ‘cleaning’ data prior to carrying out our formal analysis is to look for outliers and follow a protocol for dealing with them. For this particular analysis, exclude any values that are 2 standard deviations above the mean. You need to convert any values in the matrix that fit this criteria to NA. This is a natural extension to some of the examples given above and while the code you produce may look scary it is just a case of joining together multiple parts. cutoff &lt;- mean.mat2 + (2 * sd.mat2) mat2[mat2 &gt; cutoff] &lt;- NA Task: Check that your code has worked by looking at the matrix either in the console or in the editor view in RStudio. Imagine you had a matrix with 10,000 rows and had just completed the same task and now wanted to check which values had been converted to NA. It would be difficult to do this by simply looking at the matrix. Try: which(is.na(mat2[ , 1])) Question: What does which() do? Hint: Try is.na(mat2[ , 1]) on its own. which() returns the index values where the evaluation of the given condition is TRUE. Hence in this case we definitely get 14 and 18 as we set those values to 200. You may also have got other values depending on the random values selected initially. Task: If you have time: Repeat the exercise above (create a new matrix called mat3). This time, replace the values greater than 2 sds above the mean in column 1 only. Hint: Your command will start with something like “mat3[ , 1][mat3[ , 1] &gt; .]” mat3 &lt;- matrix(rnorm(40, 100, 15), nrow = 20, ncol = 2) mat3[c(14, 18), 1] &lt;- 200 mat3[3, 2] &lt;- 200 cutoff &lt;- mean(mat3) + (2 * sd(mat3)) mat3[ , 1][mat3[ , 1] &gt; cutoff] &lt;- NA mat3 ## [,1] [,2] ## [1,] 89.40008 60.90289 ## [2,] 116.62812 115.50316 ## [3,] 130.22246 200.00000 ## [4,] 91.52784 101.77641 ## [5,] 97.01684 94.22977 ## [6,] 89.00230 92.78804 ## [7,] 95.49700 101.85408 ## [8,] 91.93028 105.84998 ## [9,] 93.37270 92.47380 ## [10,] 92.94904 79.24207 ## [11,] 98.47387 105.40844 ## [12,] 90.37563 66.20067 ## [13,] 92.16715 101.59119 ## [14,] NA 103.80897 ## [15,] 124.81714 79.55675 ## [16,] 88.33719 117.35854 ## [17,] 88.88372 130.10749 ## [18,] NA 95.75119 ## [19,] 116.19086 98.57315 ## [20,] 92.57845 98.29681 # Notice the 3rd value in column 2 is still 200 and not NA, so the code worked. 6.3 Real Data OK, now let’s have a look at analysing some real data to answer some research questions. The questions will require you to use your previously learnt R skills (cleaning data, describing data, plotting etc.). Though some elements may be repetitive, it is good practice to think through your analyses from start to finish each time. Think about what the question is asking and what is needed to answer it - basic descriptives of the variables, plots, tests of assumptions, etc. 6.3.1 The data The data set is saved on LEARN as a .csv file. You can open this file by downloading it, saving it, and using the read.csv() to open the file. We have have used read.csv() previously so if you can’t recall how it works, use ?read.csv. The data for this lab come from a study of exercise and cognitive ability in early and later adulthood. All participants were male and randomly sampled within age groups. Data were collected on 325 individuals, 151 from a young adulthood group (aged 20-28 years) and 174 from a later adulthood group (aged 50-58 years). Each person was asked to categorise the regularity of their exercise, and were given an IQ test. The data are in the file called healthIQ.csv. The variables are as follows: ID AgeGroup: 1 = Young adulthood; 2 = Later adulthood ExGroup: 1 = no exercise; 2 = moderate exercise; 3 = intense exercise IQ score: IQ score In addition to this data, the researchers were given some seperate data from a colleague which used the same IQ test on a sample of approximately the same age as the later adulthood sample. This group had been tested at 2 points in time 3 years apart. The data file repeatIQ.csv contatins this data.The variables are: ID IQ score wave 1: IQ score IQ score wave 2: IQ score 6.3.2 The questions 6.3.2.1 Data Inspection/Cleaning You are going need to use your newly acquired indexing skills, as well as some of your already established skills to check all variables ahead of the analyses. What you should do here is use some tools (perhaps describe() in the psych package as a starting point?) to get a feel for the data. Are there any strange values, such as impossible values based on the codebook, for the variables given above? Any outliers? Is R treating each variable as the appropriate class (type) of data? e.g. factor or numeric… This should take you a little bit of time, it is an open ended exercise where you need to use your R skills and judgement to assess the data and fix any problems you encounter before moving on to the following questions: # reading in and checking the data (my data files are in my project folder) health = read.csv(&quot;data/healthIQ.csv&quot;, header = T) repIQ = read.csv(&quot;data/repeatIQ.csv&quot;, header = T) # Basic eyeball library(psych) describe(health) ## vars n mean sd median trimmed mad min max range ## ID 1 325 163.00 93.96 163.00 163.00 120.09 1 325.00 324.00 ## Agegroup 2 325 1.54 0.52 2.00 1.55 0.00 1 4.00 3.00 ## ExGroup 3 325 4.72 55.33 2.00 1.63 0.00 1 999.00 998.00 ## IQ 4 325 100.00 15.34 99.82 100.18 14.74 20 135.92 115.92 ## skew kurtosis se ## ID 0.00 -1.21 5.21 ## Agegroup 0.16 -0.54 0.03 ## ExGroup 17.86 317.95 3.07 ## IQ -0.46 1.70 0.85 This tells us a few things: There are no * so we can see that at the moment, AgeGroup and ExGroup are not being treated as factors. Second, we can see that the max value for AgeGroup is 4, yet we know from the code book that there are only 2 groups. So we have a coding error here. Third, there is a max value of 999 for ExGroup, which is again out of range (we have 3 groups). Lastly, our minimum value for IQ is 20. This is very low. On an IQ scale, we would not expect to see a value this low. This suggests we have a data entry problem. So let’s sort these out. # check factor versus numeric is.factor(health$Agegroup) ## [1] FALSE is.factor(health$ExGroup) ## [1] FALSE health$Agegroup &lt;- as.factor(health$Agegroup) health$ExGroup &lt;- as.factor(health$ExGroup) # Now they are factors let&#39;s deal with the levels levels(health[ , 2]) ## [1] &quot;1&quot; &quot;2&quot; &quot;4&quot; # so we have got a 4 where we don&#39;t need it. So let&#39;s give that NA # NA is R generic code for missing data levels(health[ , 2]) &lt;- c(&quot;Young&quot;, &quot;Older&quot;, NA) levels(health[ , 2]) ## [1] &quot;Young&quot; &quot;Older&quot; # And the same for Exercise Group levels(health[ , 3]) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;999&quot; levels(health[ , 3]) &lt;- c(&quot;None&quot;, &quot;Mod&quot;, &quot;Intense&quot;, NA) levels(health[ , 3]) ## [1] &quot;None&quot; &quot;Mod&quot; &quot;Intense&quot; # Let&#39;s have a look at the effect of using NA describe(health) ## vars n mean sd median trimmed mad min max range ## ID 1 325 163.00 93.96 163.00 163.00 120.09 1 325.00 324.00 ## Agegroup* 2 324 1.54 0.50 2.00 1.55 0.00 1 2.00 1.00 ## ExGroup* 3 324 1.65 0.58 2.00 1.63 0.00 1 3.00 2.00 ## IQ 4 325 100.00 15.34 99.82 100.18 14.74 20 135.92 115.92 ## skew kurtosis se ## ID 0.00 -1.21 5.21 ## Agegroup* -0.15 -1.98 0.03 ## ExGroup* 0.20 -0.71 0.03 ## IQ -0.46 1.70 0.85 What we can see here is that we now have * and we have an n of 324 for Age group and Exercise group. In other words, R is now recognising that we have some missing data. We can now do exactly the same for the low IQ value and then the second data set. # For the IQ data, we can use what you have learnt above health$IQ[health$IQ &lt; 25] &lt;- NA summary(health$IQ) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 55.78 90.19 99.83 100.24 110.51 135.92 1 # And the second data set describe(repIQ) ## vars n mean sd median trimmed mad min max range ## ID 1 80 250.26 45.90 256.00 250.69 60.05 174.00 324.00 150.00 ## wave1_IQ 2 80 99.33 14.53 98.51 99.36 14.28 64.61 135.83 71.21 ## wave2_IQ 3 80 96.55 14.70 96.39 96.56 14.86 63.78 148.10 84.32 ## skew kurtosis se ## ID -0.10 -1.35 5.13 ## wave1_IQ 0.07 0.08 1.62 ## wave2_IQ 0.24 0.74 1.64 # This one looks OK You may have made different decisions, or encountered different potential issues. As long as you engaged with the data and highlighted that it wasn’t perfectly fit for analysis as it was, and took steps to fix it, then you passed the Quest! For the following questions, you need to think about what statistical procedure you can use to answer each question, and then implement it using R. Q1. Are age category (young versus old) and health activity category independent? Chi-square test of independance # Look at cell counts using xtabs: xtabs(~ Agegroup + ExGroup, data = health) ## ExGroup ## Agegroup None Mod Intense ## Young 38 100 11 ## Older 90 78 6 Some counts (Intense) are quite low so we will run the test with small N correction library(lsr) results = chisq.test(health[ , 2], health[ , 3], correct = T) results ## ## Pearson&#39;s Chi-squared test ## ## data: health[, 2] and health[, 3] ## X-squared = 23.521, df = 2, p-value = 7.808e-06 # And look at some residuals results$residuals ## health[, 3] ## health[, 2] None Mod Intense ## Young -2.738935 1.974118 1.127668 ## Older 2.534547 -1.826803 -1.043518 Q2. A researcher has a hypothesis that extreme exercise is quite rare and no/moderate exercise are equally probable. Construct a single test for exercise volume with appropriate probabilities to test this hypothesis. This is a Chi-square goodness-of-fit test observed = table(health$ExGroup) chisq.test(x = observed, p = c(.45, .45, .10)) ## ## Chi-squared test for given probabilities ## ## data: observed ## X-squared = 16.367, df = 2, p-value = 0.0002792 The test is significant, suggesting that the proposed distribution is not reflected in the data Q3. Do younger and older individuals differ in IQ score? This is an independent sample t-test bygroup = describeBy(health, health$Agegroup) # describes the data for each factor level seperately as.data.frame(bygroup$Young)[4, c(2:4, 11:12)] # narrow selection to what we are interested in ## n mean sd skew kurtosis ## IQ 149 105.2388 14.45316 -0.2760457 0.1754786 as.data.frame(bygroup$Older)[4, c(2:4, 11:12)] # narrow selection to what we are interested in ## n mean sd skew kurtosis ## IQ 174 95.99017 13.61018 -0.02139273 -0.1367622 # plot it boxplot(health$IQ ~ health$Agegroup, main = &quot;Boxplot of Test Means&quot;, xlab = &quot;Age Group&quot;, ylab = &quot;IQ&quot;) # Or using ggplot2 library(ggplot2) iq_plot &lt;- ggplot(data = na.omit(health), aes(x = Agegroup, y = IQ)) # use na.omit() to remove the case with NA value for Age Group. # Otherwise we get a box plot for the NA group.... iq_plot + geom_boxplot() #Assumptions qqnorm(y = health$IQ) shapiro.test(health$IQ) ## ## Shapiro-Wilk normality test ## ## data: health$IQ ## W = 0.99598, p-value = 0.5807 library(car) leveneTest(health$IQ ~ health$Agegroup) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.5643 0.4531 ## 321 # Test q3 = t.test(formula = IQ ~ Agegroup, data = health, var.equal = T) q3$statistic ## t ## 5.916355 q3$parameter ## df ## 321 q3$p.value ## [1] 8.436336e-09 Q4. Did IQ scores significantly increase across time points in the later adulthood sample? This is a paired-sample t-test describe(repIQ[ , c(2, 3)]) # Descriptive look at the variables ## vars n mean sd median trimmed mad min max range skew ## wave1_IQ 1 80 99.33 14.53 98.51 99.36 14.28 64.61 135.83 71.21 0.07 ## wave2_IQ 2 80 96.55 14.70 96.39 96.56 14.86 63.78 148.10 84.32 0.24 ## kurtosis se ## wave1_IQ 0.08 1.62 ## wave2_IQ 0.74 1.64 Essentially, follow the solution for the previous question except make sure to set the paired value to TRUE in t.test() and take note of the directional hypothesis using the alternative = &quot;greater&quot; argument. q4 = t.test(repIQ$wave2_IQ, repIQ$wave1_IQ, alternative = &quot;greater&quot;, paired = TRUE) q4$statistic ## t ## -2.088266 q4$parameter ## df ## 79 q4$p.value ## [1] 0.9800024 "],
["lab-7.html", "Chapter 7 Lab 7 7.1 Merging and missing values 7.2 Writing a function 7.3 Some data manipulation 7.4 A note on correlation 7.5 Some statistics!", " Chapter 7 Lab 7 This week we’ll be merging data collected from a number of sources before running a correlation and a linear regression. You should end up knowing a bit more about data manipulation, as well as about running statistical tests. As always, create a new project in RStudio. 7.1 Merging and missing values Task: First of all, generate a data frame with IQ measurements for 5 people (Amy, Jill, Kim, Dave, and Earl). IQ should be a normally distributed variable with \\(Mean = 100\\) and \\(SD = 15\\). iq.f &lt;- data.frame(name = factor(c(&#39;Amy&#39;, &#39;Jill&#39;, &#39;Kim&#39;, &#39;Dave&#39;, &#39;Earl&#39;)), iq = rnorm(n = 5, mean = 100, sd = 15)) ## name iq ## 1 Amy 126.82867 ## 2 Jill 93.30838 ## 3 Kim 86.73938 ## 4 Dave 102.51666 ## 5 Earl 110.01594 Task: Next, create another data frame with height measurements for 5 people (Jill, Dave, Earl, Fred, and Jen). ht.f &lt;- data.frame(name = factor(c(&#39;Jill&#39;, &#39;Dave&#39;, &#39;Earl&#39;, &#39;Fred&#39;, &#39;Jen&#39;)), height = rnorm(5, 170, 10)) ## name height ## 1 Jill 161.3152 ## 2 Dave 174.6044 ## 3 Earl 174.0020 ## 4 Fred 176.4006 ## 5 Jen 164.8891   You’ll notice that the data frames you’ve created contain some overlapping information. If you wanted to test for a relationship between IQ and height it would be useful to create one merged data frame. The R function for this is merge(). Task: Try the following commands. merge(iq.f, ht.f) ## name iq height ## 1 Dave 102.51666 174.6044 ## 2 Earl 110.01594 174.0020 ## 3 Jill 93.30838 161.3152 merge(iq.f, ht.f, all = T) ## name iq height ## 1 Amy 126.82867 NA ## 2 Dave 102.51666 174.6044 ## 3 Earl 110.01594 174.0020 ## 4 Jill 93.30838 161.3152 ## 5 Kim 86.73938 NA ## 6 Fred NA 176.4006 ## 7 Jen NA 164.8891 What’s the difference between these two commands? What would you do if the columns in each data frame had different names? (Look at ?merge). Task: Now create a new merged data frame with no missing values for IQ. Hint: Look at the all.x argument in ?merge. new.f &lt;- merge(iq.f, ht.f, all.x = T) new.f ## name iq height ## 1 Amy 126.82867 NA ## 2 Dave 102.51666 174.6044 ## 3 Earl 110.01594 174.0020 ## 4 Jill 93.30838 161.3152 ## 5 Kim 86.73938 NA Task: Calculate the mean of the IQ column. Revision: These two commands do the same thing (why?) mean(new.f$iq) ## [1] 103.8818 mean(new.f[ ,2]) ## [1] 103.8818 Task: Now calculate the mean height. mean(new.f$height) ## [1] NA   7.1.1 Missing Values mean(new.f$height) doesn’t give you a mean; instead it returns NA. If you completed last week’s worksheet you will have encountered NA values and perhaps learned to deal with them. The fact that R returns NA rather than just ignoring any missing values may seem odd at first, but in fact it’s based on an important principle: You should know that there are missing values, and tell R. If it automatically dealt with missing values that you didn’t know about, you could end up with misleading results. For functions (like mean(), sd(), etc.), the only way to get R to ignore NAs is as follows (na.rm means “NA remove”). mean(new.f$height, na.rm = T) ## [1] 169.9739 For functions like lm(), which we’ll see below, an argument na.action is set to na.omit to remove missing values.   7.2 Writing a function You may have come across writing functions before, and you have learnt about logical subsetting (where items in a vector or matrix are selected according to the logical values T or F). Can we put these together here? Write a function called outliers() to test whether observed values are greater than, or less than, x standard deviations from the mean (where x is a value passed to the function). You should use a template something like the following. outliers &lt;- function(obs, x = 2.5) { # code goes here } Tip: Your function takes two values: obs (a vector or matrix of observations), and x, as above. x = 2.5 passes a default value to x, so that, if you don’t specify a value, the default of 2.5 will be used. This means that you can run outliers(vec), which would use the default value; or you could run outliers(vec, 2) if you wanted to use 2 sds instead. Task: Now write the code to complete the function. Hint: Suppose you have a matrix mat. Code that replace elements in mat that are more than 2.5 sds above the mean with NAs could look something like this: mat[mat &gt; mean(mat) + 2.5 * sd(mat)] &lt;- NA. ## one possible solution outliers &lt;- function(obs, x = 2.5) { # the following line returns TRUE if outlier, FALSE otherwise (for each element of &#39;obs&#39;) return(abs(obs - mean(obs, na.rm = T)) &gt; (sd(obs, na.rm = T) * x)) } Here, you’re being asked to produce a function that does something similar to the code inside the [] above. You want to use x rather than 2.5 so that the number of standard deviations can be specified. You will need to take into account that your input could include NAs. You also need to think about detecting values that are ‘x’ standard deviations below the mean as well. Pro tip: A neat solution to this problem might involve using the abs() function which converts values to their absolute value. See ?abs, and look here for more on what an absolute value is. Task: Test your outliers function with the following code: Does it correctly identify the 2 outliers? vec &lt;- rnorm(20, 100, 15) vec[sample(length(vec), 2)] &lt;- 250 # create two outliers at random vec # inspect vector to find outliers ## [1] 89.22605 119.27710 83.13218 99.03670 75.09153 103.36543 102.95576 ## [8] 130.21001 96.07631 250.00000 250.00000 113.37671 111.64968 114.56312 ## [15] 103.16879 112.40882 112.67822 74.78100 134.96395 91.31800 which(outliers(vec)) # check outliers function ## [1] 10 11 7.3 Some data manipulation The aim of this exercise is for you to load some data, get it into a suitable format for analysis, and perform a correlation, and a linear regression. Note: Most of this exercise is about getting data into shape before you do the stats! Task: Download the data from the LEARN (lab7.Rdata), and load it into R. Tip: This data is in R format, not .csv (because it contains several data frames). Save it into your project folder. Use the command below to load it into R. load(&#39;data/lab7.Rdata&#39;) # if your data is in your project folder Tip: The first thing you should do is look at the Environment tab in Rstudio (top right), or type ls(), to find out what new objects you’ve loaded. Pro tip: ls() returns a list of all the objects (variables and functions) in your workspace. That means it’s very useful if you want to delete everything and start again: You can use rm(list = ls()) (or click on the broom in RStudio’s Environment tab). rm() is the function to remove things. You have data from three different Universities on students in their statistics classes. Each University (or School) has provided you with the same information; unfortunately, they have provided it in slightly different formats. Your task is to assemble all of the information into one data frame called schools, suitable for further analysis. The data set should consist of a unique student identifier and, for each student, the school they’re in, their IQ, an exam score, and their gender. Unfortunately, the records are not all complete (in particular, some exam scores are missing), and there may be other errors. Tip: You will definitely want to use merge() to tackle this (be careful with the all arguments!). You may also be able to use rbind() to bind rows for some (but not all) merges. You should also be thinking about using your indexing skills. Below are some things you might want to think about: are the observations complete? are there any typos? do the column names match? are there any unlikely values, or outliers? The general approach is probably to merge data from each individual school, before merging the complete dataset together (Note: you can only merge two things at a time…) #### SOME STEPS YOU MIGHT TAKE summary(schoolA) ## id school gender iq exam ## s023 : 1 a: 1 female:25 Min. : 68.0 Min. : 3.0 ## s028 : 1 A:62 male :38 1st Qu.: 90.5 1st Qu.:43.0 ## s038 : 1 Median : 99.0 Median :53.0 ## s059 : 1 Mean :101.3 Mean :52.8 ## s103 : 1 3rd Qu.:110.5 3rd Qu.:65.0 ## s105 : 1 Max. :250.0 Max. :95.0 ## (Other):57 NA&#39;s :14 # maximum IQ of 250 looks suspect! Is it an outlier? schoolA$iq[which(outliers(schoolA$iq))] ## [1] 250 # yes it is; fix it schoolA$iq[which(outliers(schoolA$iq))] &lt;- NA # exam range looks very broad; is the 3 unusual? which(outliers(schoolA$exam)) ## integer(0) # actually there are no outliers at 2.5sd; just a wide spread, see: hist(schoolA$exam) # or: ggplot2::qplot(schoolA$exam, binwidth = 10) ## Warning: Removed 14 rows containing non-finite values (stat_bin). # look at the levels of school: &#39;a&#39; and &#39;A&#39;... # there&#39;s an &#39;a&#39; where there should be an &#39;A&#39; schoolA$school[schoolA$school == &#39;a&#39;] &lt;- &#39;A&#39; # (can be useful to drop levels, although merge() should cope with this later) schoolA &lt;- droplevels(schoolA) summary(schoolB.IQ) ## id iq ## s001 : 1 Min. : 58.00 ## s039 : 1 1st Qu.: 92.25 ## s078 : 1 Median :102.00 ## s089 : 1 Mean : 99.83 ## s133 : 1 3rd Qu.:109.00 ## s136 : 1 Max. :130.00 ## (Other):58 summary(schoolB.exam) ## id school gender exam ## s001 : 1 B:48 female:24 Min. : 15.00 ## s039 : 1 male :24 1st Qu.: 39.75 ## s078 : 1 Median : 51.00 ## s133 : 1 Mean : 53.29 ## s136 : 1 3rd Qu.: 66.00 ## s144 : 1 Max. :100.00 ## (Other):42 # there are more IQ entries than exam entries (data frames are different lengths); # we just have to exclude IQ info that doesn&#39;t have corresponding exam info. schoolB &lt;- merge(schoolB.IQ, schoolB.exam) summary(schoolB) ## id iq school gender exam ## s001 : 1 Min. : 58.00 B:48 female:24 Min. : 15.00 ## s039 : 1 1st Qu.: 92.25 male :24 1st Qu.: 39.75 ## s078 : 1 Median :102.00 Median : 51.00 ## s133 : 1 Mean : 99.77 Mean : 53.29 ## s136 : 1 3rd Qu.:108.00 3rd Qu.: 66.00 ## s144 : 1 Max. :130.00 Max. :100.00 ## (Other):42 # everything else looks fine # can now merge schools A&amp;B: all = T is important, think about why! schools &lt;- merge(schoolA, schoolB, all = T) summary(schoolC) ## id school gender IQ exam ## s007 : 1 C:40 female:20 Min. : 64.00 Min. : 5.00 ## s010 : 1 male :20 1st Qu.: 94.75 1st Qu.: 48.75 ## s011 : 1 Median : 99.00 Median : 64.50 ## s035 : 1 Mean : 99.95 Mean : 62.95 ## s041 : 1 3rd Qu.:108.75 3rd Qu.: 75.50 ## s055 : 1 Max. :135.00 Max. :104.00 ## (Other):34 # column name is &quot;IQ&quot; rather than &quot;iq&quot; colnames(schoolC)[4] &lt;- &#39;iq&#39; # there&#39;s quite a low exam score which(outliers(schoolC$exam)) ## [1] 35 schoolC$exam[which(outliers(schoolC$exam))] ## [1] 5 # the low score is an outlier, but there was a low score (not an outlier) for SchoolA # so I&#39;ll wait until the scores are merged. # However, there&#39;s an impossible exam mark of 104! schoolC$exam[schoolC$exam &gt; 100] &lt;- NA # now merge again schools &lt;- merge(schools, schoolC, all = T) ## check exam one more time for outliers hist(schools$exam) which(outliers(schools$exam)) ## [1] 42 schools$exam[which(outliers(schools$exam))] ## [1] 3 ### OK, the 3 in SchoolA *is* low according to the 2.5sd default schools$exam[which(outliers(schools$exam))] &lt;- NA   7.4 A note on correlation x = seq(50, 150, 4) y = x + rnorm(n = length(x), m = 0, sd = 5) # add some random noise to x and store it in y y2 = 2 * x + rnorm(n = length(x), m = 0, sd = 10) y3 = 4 * x + rnorm(n = length(x), m = 0, sd = 20) Take a look at the plot generated below where the black, blue, and red values (and their ‘line of best fit’) represent different variables and their relationship with the variable x. Take a second to think about what you expect the correlations between x and the three different variables representing y (black, blue, and red) will be. plot(x, y, ylim = c(0, 650)) points(x, y2, col = &quot;blue&quot;) points(x, y3, col = &quot;red&quot;) abline(lm(y ~ x)) abline(lm(y2 ~ x), col = &quot;blue&quot;) abline(lm(y3 ~ x), col = &quot;red&quot;) You may have been tempted to suggest that the blue line indicates a higher correlation than the black line, and the red line indicates a higher correlation than both the blue and black lines, etc. This is a common mistake when thinking about correlations. All these correlations are actually pretty much identical (almost 1). cor(x, y) # Black ## [1] 0.9916637 cor(x, y2) # Blue ## [1] 0.983024 cor(x, y3) # Red ## [1] 0.9894178 Remember that correlation tells you how well the line fits the data (how close the points are to the line), not how steep it is. Looking at all the lines again - you can see that in each case knowing the value of x for any one participant allows you to say, with almost certainty, what their y value would be. The steepness of the gradient is determined by the increase in y units for each one unit increase in x - this is the coefficient from the linear model.   7.5 Some statistics! Task: Using cor.test() and lm(), first run a correlation, and then a linear regression, to examine the relationship between IQ and exam performance in the schools dataset you’ve created. What can you conclude from your analyses? Tip: cor.test() is a simple function that prints out information immediately. lm() is a bit more complex: You will need to assign its output to an object (e.g. model &lt;- lm(...)) and then use summary() on that object to get useful information about it (summary(model)). Note: Results may differ slightly depending on the merging decisions you made. with(schools, cor.test(iq, exam)) ## ## Pearson&#39;s product-moment correlation ## ## data: iq and exam ## t = 3.015, df = 132, p-value = 0.003083 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.08802564 0.40593935 ## sample estimates: ## cor ## 0.2538249 There’s a significant positive correlation between iq and exam - Higher IQ is associated with better exam score model &lt;- lm(exam~iq, data = schools) summary(model) ## ## Call: ## lm(formula = exam ~ iq, data = schools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -52.742 -14.319 -1.098 12.805 49.294 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.7961 11.4757 1.899 0.05970 . ## iq 0.3423 0.1135 3.015 0.00308 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.82 on 132 degrees of freedom ## (17 observations deleted due to missingness) ## Multiple R-squared: 0.06443, Adjusted R-squared: 0.05734 ## F-statistic: 9.09 on 1 and 132 DF, p-value: 0.003083 The model is better than the null model (F(1, 132) = 9.09; p &lt; .05) For each point increase in iq, exam score goes up by 0.34 (b coefficient) "],
["lab-8-the-general-linear-model.html", "Chapter 8 Lab 8 - The General Linear Model 8.1 A brief note on with() 8.2 Linear Models 8.3 Model prediction and visualisation 8.4 Model criticism 8.5 Multiple regression 8.6 Contrast coding", " Chapter 8 Lab 8 - The General Linear Model This week we’ll run a linear model and scale our predictors, learn how to plot the fitted (or predicted) values, and get some practice with model criticism and multiple regression. 8.1 A brief note on with() You may have noticed a really useful function in the solutions for last couple of weeks - the with() function. This function allows you to specify a data frame that an expression uses. This means you don’t have to keep typing mydataframe$variable. Instead, you can use with(data, function(variable, ....)). See the following example: plot(schools$exam, schools$revision_hours) This command can be written as: with(schools, plot(exam, revision_hours)) This can: * make your code more readable * save time and effort when you are referring to the same dataframe multiple times. 8.2 Linear Models Task: Download the sharks1 dataset from LEARN, create a new script file, and load in the data using the load() function. This will load in a dataframe called sharks. Task: Create a linear model and inspect it by typing the code below. model &lt;- lm(shark_attacks ~ ice_cream_sales, data = sharks) summary(model) ## ## Call: ## lm(formula = shark_attacks ~ ice_cream_sales, data = sharks) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.6352 -4.8057 -0.9863 4.8565 17.7059 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.54859 5.28649 0.860 0.392 ## ice_cream_sales 0.33444 0.05921 5.648 2.33e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.21 on 81 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.2826, Adjusted R-squared: 0.2737 ## F-statistic: 31.9 on 1 and 81 DF, p-value: 2.329e-07 Task: Which is the intercept? Which is the slope? How can you interpret each of them? Task: Create a new model, model2, using scale(ice_cream_sales) instead of ice_cream_sales. model2 &lt;- lm(shark_attacks ~ scale(ice_cream_sales), data = sharks) summary(model2) ## ## Call: ## lm(formula = shark_attacks ~ scale(ice_cream_sales), data = sharks) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.6352 -4.8057 -0.9863 4.8565 17.7059 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.0723 0.7914 43.052 &lt; 2e-16 *** ## scale(ice_cream_sales) 4.4974 0.7962 5.648 2.33e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.21 on 81 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.2826, Adjusted R-squared: 0.2737 ## F-statistic: 31.9 on 1 and 81 DF, p-value: 2.329e-07 Task: What does the intercept of model2 tell us? And the slope? (Take a look at the slides from this weeks lecture on standardisation for some guidance with this) #the intercept is predicted exam score when scale(ice_cream_sales) is 0. #the scale() function centres a variable around its mean, and standardises it so #that we can talk in terms of +/-1 standard deviation. #This means that: #- the intercept is the predicted shark attacks for the mean number of ice cream sales. #- the slope now represents the change in shark attacks units for 1 standard deviation increase in ice cream sales. 8.3 Model prediction and visualisation Coefficients can sometimes get a little tricky to interpret, and visualising the model can be really helpful. We’ve seen in an earlier lab one way of visualising a linear model, using the abline() function (Hint: See Week 7 solutions). Task: Run the code below to create a plot, and then use abline() to add the regression line from the linear model which we named model earlier. The code below also sets the colours and aesthetics of the plots - feel free to try changing these to see how they work! with(sharks, plot(ice_cream_sales, shark_attacks, xlab=&quot;- Monthly ice cream sales -&quot;,ylab=&quot;Monthly shark attacks&quot;, main=&quot;Shark attacks!&quot;, col=c(&quot;slateblue&quot;), bty=&quot;n&quot;, pch=20)) with(sharks, plot(ice_cream_sales, shark_attacks, xlab=&quot;- Monthly ice cream sales -&quot;,ylab=&quot;Monthly shark attacks&quot;, main=&quot;Shark attacks!&quot;, col=c(&quot;slateblue&quot;), bty=&quot;n&quot;, pch=20)) abline(model) Task: Do the same for model2. What do you think should be on the x-axis of your plot? with(sharks, plot(scale(ice_cream_sales), shark_attacks, xlab=&quot;- Monthly ice cream sales -&quot;,ylab=&quot;Monthly shark attacks&quot;, main=&quot;Shark attacks!&quot;, col=c(&quot;slateblue&quot;), bty=&quot;n&quot;, pch=20)) abline(model2) 8.3.1 Predict() We can get at the y-values for these lines by using the predict() function. head(predict(model)) # head() will just print out the first 6 rows ## 1 3 4 5 6 7 ## 29.96629 34.98294 36.32072 38.99626 40.66848 38.66182 The predict() function is especially useful as we can ask for model predictions given some new data (e.g., new numbers of monthly ice cream sales). Task: What do you think the code below tells us? new_months &lt;- data.frame(ice_cream_sales=c(70, 120)) predict(model, newdata=new_months) ## 1 2 ## 27.95963 44.68180 # They are the number of shark attacks predicted by our model for months in which 70 and 120 ice creams are sold. Task: We can also ask the predict() function to return a confidence interval. Try the following code: p.sharks &lt;- predict(model, interval = &quot;confidence&quot;) head(p.sharks) ## fit lwr upr ## 1 29.96629 27.82816 32.10442 ## 3 34.98294 33.37592 36.58997 ## 4 36.32072 34.55807 38.08336 ## 5 38.99626 36.65359 41.33894 ## 6 40.66848 37.86161 43.47535 ## 7 38.66182 36.40498 40.91866 There are three columns: fit is the fitted (predicted) value of shark_attacks for each number of ice cream sales; lwr and upper are the upper and lower bounds of the model confidence interval for each value of sharks$ice_cream_sales. Note that the row names seem to miss out number 2? In fact, there are actually only 83 values in p.sharks, but we had 84 months in our sharks dataset. If you look carefully at the output of summary(model) above, it tells us that 1 observation has been deleted due to “missingness”. The row names of p.sharks are actually the original row names of the sharks dataset (the one which we fit our model on). Task: Merge the sharks data with p.sharks, and call it sharks_pred. You will need to use by = &quot;row.names&quot;. sharks_pred &lt;- merge(sharks, p.sharks, by=&quot;row.names&quot;) Task: We can now plot the confidence intervals. You can see this below using the plot() and lines() functions, and with ggplot(). You can also see below how to change some of the aesthetics in ggplot(). Note that to make this work using plot() and lines(), we need to order the dataframe first. Otherwise R will join points to make a line in the order they appear in the data. If you’re keen to understand this more, try running plot(x=c(5,1,3,4), y=c(1,2,3,4), type=&quot;l&quot;) to see what happens! sharks_pred_ordered &lt;- sharks_pred[order(sharks_pred$ice_cream_sales),] with(sharks, plot(ice_cream_sales, shark_attacks, xlab=&quot;- Monthly ice cream sales -&quot;,ylab=&quot;Monthly shark attacks&quot;, main=&quot;Shark attacks!&quot;, col=c(&quot;slateblue&quot;), bty=&quot;n&quot;, pch=20)) with(sharks_pred_ordered, lines(ice_cream_sales, fit)) with(sharks_pred_ordered, lines(ice_cream_sales, lwr)) with(sharks_pred_ordered, lines(ice_cream_sales, upr)) library(ggplot2) ggplot(sharks_pred, aes(x=ice_cream_sales))+ # we want ice_cream_sales on the x-axis geom_point(aes(y=shark_attacks), col=&quot;slateblue&quot;)+ # for the points, we want the y to be the actual (observed) shark attacks geom_line(aes(y=fit))+ # for the line, we want the fitted model values (the &quot;fit&quot; column) geom_ribbon(aes(ymin=lwr,ymax=upr), fill=&quot;orange&quot;, alpha=.4) + # the &quot;ribbon&quot; is the upper and lower bounds of the confidence interval. the alpha bit just makes it slightly transparent. theme_minimal()+ labs(x=&quot;- Ice cream sales - &quot;,y=&quot;Shark attacks&quot;) 8.4 Model criticism You’ll also notice that in the sharks data there is information capturing the temperature of each month. Task: Construct a linear model to see if monthly temperature predicts the number of ice cream sales. model.b &lt;- lm(ice_cream_sales ~ temperature, data = sharks) summary(model.b) ## ## Call: ## lm(formula = ice_cream_sales ~ temperature, data = sharks) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.704 -5.914 2.341 7.295 17.415 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.206 5.720 8.951 9.91e-14 *** ## temperature 1.975 0.298 6.627 3.51e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.9 on 81 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.3516, Adjusted R-squared: 0.3436 ## F-statistic: 43.92 on 1 and 81 DF, p-value: 3.505e-09 Task: Plot the regression line and confidence interval, as seen above. icecream_pred &lt;- merge(sharks, predict(model.b, interval = &quot;confidence&quot;), by=&quot;row.names&quot;) ggplot(icecream_pred, aes(x=temperature))+ geom_point(aes(y=ice_cream_sales, col=temperature))+ geom_line(aes(y=fit))+ geom_ribbon(aes(ymin=lwr,ymax=upr), fill=&quot;red&quot;, alpha=.4)+ theme_minimal()+ labs(x=&quot;- Temperature -&quot;, y=&quot;Ice cream sales&quot;) Task: Use some of the model criticism tools you saw in the lecture to examine your model. Try using plot() on your model. Here are some quick explanations of the plots you get out: Residuals vs Fitted: Can show if the residuals have non-linear patterns. A “good” model will show equally spread residuals around a horizontal line. Normal Q-Q: This is a way of plotting the residuals against the equivalent quantiles of a standard normal distribution (mean = 0, sd = 1). A “good” model will have residuals close to the straight dashed line. Scale-Location: (also called Spread-Location). This shows if the residuals are spread equally along the range of the predictors (This is the assumption of equal variance, or homoscedaticity). Ideally, we want a horizontal line with points spread equally from across the plot. Residuals vs Leverage: This helps to find influential points in the regression. Cook’s Distance is shown on these plots by red dashed lines, and points which fall outside of these lines are influential to the model. A really good illustration of influence can be found here. # Model Check (Visually): par(mfrow = c(2, 2)) plot(model.b, which = c(1:4)) # There might be some issues: # There may be a slight non-linear pattern to the data. # QQ-Plot shows a small deviation at the lower end. # Homogeneity of Variance also potentially problematic # A couple of quite influential points 8.5 Multiple regression Think about the relationship we found earlier between ice cream sales and shark attacks. Does this make sense to you? (see also the Spurious Correlations page). What might explain the relationship? Task: Does the number of ice cream sales predict shark attaacks over and above what temperature predicts? Tip: You’ll want to test whether adding a new predictor improves on chance (using anova()) as part of your answer… Part 2 of Lecture 7 will help you here. model.c &lt;- lm(shark_attacks ~ temperature + ice_cream_sales, data = sharks) anova(model.c) ## Analysis of Variance Table ## ## Response: shark_attacks ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## temperature 1 3000.00 3000.00 86.8115 2.093e-14 *** ## ice_cream_sales 1 104.96 104.96 3.0374 0.08521 . ## Residuals 80 2764.61 34.56 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(model.c) ## ## Call: ## lm(formula = shark_attacks ~ temperature + ice_cream_sales, data = sharks) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2533 -2.6712 -0.0836 2.9513 16.6902 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.60047 4.35313 0.138 0.8906 ## temperature 1.29170 0.19966 6.469 7.27e-09 *** ## ice_cream_sales 0.10448 0.05995 1.743 0.0852 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.879 on 80 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.529, Adjusted R-squared: 0.5172 ## F-statistic: 44.92 on 2 and 80 DF, p-value: 8.339e-14 Task: Think about what we know about our predictors (ice_cream_sales and temperature) - they are correlated. This will make the standard error of the coefficient bigger (you can see the standard errors in the SE column in the summary() output for a model). Use the vif() function from the car package to see how much the SE of the effect of ice_cream_sales on shark_attacks has been inflated by the inclusion of temperature. Is it problematic? Hint: See slide 36 of Lecture 7. library(car) vif(model.c) ## temperature ice_cream_sales ## 1.542181 1.542181 #VIFs of less than 4 are usually not too bad, so we&#39;re okay here. #square root of VIF tells you inflation of SE. sqrt(1.542181) ## [1] 1.241846 8.6 Contrast coding Task: The data we’ve been working with has captured great white shark attacks. But what about other types of shark? Load in the sharks_extra data set from learn. It will load in a dataset called sharks2. It contains data for hammerheads and basking shark attacks as well as the attacks by the great whites. Task: Use orthogonal contrasts to answer the following questions: - Do basking sharks attack fewer people than great whites and hammerheads? - Do numbers of great whites attacks differ from hammerhead attacks? load(&quot;data/sharks_extra.Rdata&quot;) contrasts(sharks2$shark) ## basking hammerhead ## great_white 0 0 ## basking 1 0 ## hammerhead 0 1 contrasts(sharks2$shark)&lt;-cbind(&quot;GWHvB&quot;=c(-1/3, 2/3, -1/3), &quot;GWvH&quot;=c(1/2, 0, -1/2)) shark_model&lt;-lm(shark_attacks ~ shark, data = sharks2) summary(shark_model) ## ## Call: ## lm(formula = shark_attacks ~ shark, data = sharks2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.5357 -7.2738 -0.2738 7.4643 18.4643 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.9365 0.5639 45.994 &lt;2e-16 *** ## sharkGWHvB -24.9940 1.1962 -20.894 &lt;2e-16 *** ## sharkGWvH -0.5357 1.3813 -0.388 0.698 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.952 on 249 degrees of freedom ## Multiple R-squared: 0.6369, Adjusted R-squared: 0.634 ## F-statistic: 218.4 on 2 and 249 DF, p-value: &lt; 2.2e-16 "],
["lab-9-interactions-and-glm.html", "Chapter 9 Lab 9 - Interactions and GLM&quot; 9.1 Interactions 9.2 Generalised Linear Model (GLM) 9.3 Your turn (Less guidance)", " Chapter 9 Lab 9 - Interactions and GLM&quot; Today’s lab is primarily focused on extending our knowledge/experience of conducting and interpreting linear models. 9.1 Interactions 9.1.1 Specifying interactions in R In the lecture last week, you were introduced to interactions in linear models. Therefore we need to know how to tell R to also fit interaction terms in our calls to lm(). lm(y ~ x1) lm(y ~ x1 + x2) So far, these are the types of linear models we have been asking R to fit. In the first example, the command tells R we want to predict y using the explanatory variable x1. The second example extends this by also using x2 as an explanatory variable. To also ask for R to fit the interaction between x1 and x2, we can execute: lm(y ~ x1 + x2 + x1:x2) We use a colon to separate two terms to indicate we want to fit the interaction between these terms. An alternative method is: lm(y ~ x1 * x2) Using the * tells R to fit the full model using these variables: each of the variables individually and the interaction term.   9.1.2 Simulating data Today’s exercise is a little different: We’re going to analyse a dataset, but we’re going to build it ourselves. The purpose if this is because we can define the relationships between the variables, and so link this more clearly to our regression formula (\\(y_i=b_0+b_1x_{1i}+b_2x_{2i}+{}...{}+b_nx_{ni}+\\epsilon_i\\)) and model output. Task: Create a data frame to hold your data with participant labels for 100 people. To do this, you can simple use the colon : to make a sequence from 1 to 100, and wrap the whole thing in factor() to make sure R knows that it is categorical. my.data &lt;- data.frame(ppt = factor(1:100)) Task: Now let’s add in some information about these participants. Give some values for their age, and their musicality (some measure of how interested the participant is in music generally). Make up some values that vary plausibly for each column. So, for example, if you thought that age was best expressed in years and our participants’ ages should vary uniformly from 20 to 60, you might type: my.data$age &lt;- runif(100, 20, 60) but this is only an example. You might assume that the age of participants is normally distributed; you might want to express it in months; and so on. Similarly, you can define musicality however you like: You could use rnorm() or some other random function to assign values to it; or you might want to sample() (with replacement) from a strict set of numbers such as 1:7, for example. ## Here are some simple versions based on the hints above: my.data$musicality &lt;- sample(c(1:7), 100, replace = TRUE) Task: Have a look at the dataset you have created: my.data. Have a look at a summary of the variables you have created: summary(my.data). Do they look sensible? Most likely yes, but make sure nothing stands out as strange. ## here&#39;s the summary based on the commands above: summary(my.data) ## ppt age musicality ## 1 : 1 Min. :20.42 Min. :1.00 ## 2 : 1 1st Qu.:28.72 1st Qu.:2.00 ## 3 : 1 Median :39.59 Median :4.00 ## 4 : 1 Mean :39.93 Mean :3.89 ## 5 : 1 3rd Qu.:50.11 3rd Qu.:6.00 ## 6 : 1 Max. :59.62 Max. :7.00 ## (Other):94 Task: Create a variable called spotify_hours in your data frame. This will represent the number of hours each participant spent listening to spotify over the course of a year. We want this to be a function of musicality and age. In other words, you’re being asked: `What is the (fictional) way in which age and musicality predict the numbers of hours that people listen to spotify over the course of a year?’ To do this, you’ll need to think about regression equations in general, and in particular, about interaction terms. How do age and musicality interact? Will old musical people listen to spotify less than young musical people? And so on. The exact formula is up to you but, it’s a linear equation, so it should be something like: my.data$spotify_hours &lt;- ... + (... * my.data$age) + (... * my.data$musicality) + (... * my.data$age * my.data$musicality) In this equation, the …’s represent spaces where \\(b_0\\), \\(b_1\\), \\(b_2\\), and so on (see slides 36-37 from lecture 8). For our example, the following might work: my.data$spotify_hours &lt;- 300 - 1 * my.data$age + 2 * my.data$musicality + .2 * my.data$age * my.data$musicality #this would mean that someone who is 20, and who is very musical (scores 7), #listens to spotify for 300-(1*20)+(2*7)+(.2*20*7) = 322 hours #(that&#39;s a little under an hour a day). #We&#39;ve said that for every year older, people listen 1 hour less, and for every #unit of musicality, people listen 2 hours more. #We&#39;ve also stated that for every year older, there is an extra .2 effect of musicality. #(maybe older musical people have more time to listen? younger people are all stuck in statistics courses!)   9.1.3 Visualising interactions Task: Have a look at your data. There are various ways of doing this! For a fancy 3D-plot you can install the plotly package and try the code below7, and then move the resulting graph around with your mouse. library(plotly) # if you don&#39;t include the &#39;~&#39;s, the axes will be labelled &#39;X&#39;, &#39;y&#39;, and &#39;z&#39;. with(my.data, plot_ly(x = ~age, y = ~musicality, z = ~spotify_hours)) If you are having problems with plotly or want to stay within 2-dimensions, try a flat plot, like this: plot(my.data[, c(&#39;age&#39;, &#39;musicality&#39;, &#39;spotify_hours&#39;)]) Task: Does everything look relatively plausible? If not, you could start again at Task 4 (or earlier), changing values or the formula to create spotify_hours until you’re happy with the result. The variable spotify_hours we’ve created above is a bit like fitted values - based on specific choice of \\(b_0\\), \\(b_1\\), etc, we’ve perfectly explained spotify hours in terms of age and musicality. But real data is noisy - even when the relationship is extremely strong the real values will not lie perfectly on the ‘line of best fit’. Task: Add some noise to spotify_hours hint: ‘noise’ is random, and the linear model assumes that residual (\\(\\epsilon_i\\)) is normally distributed. my.data$spotify_hours &lt;- my.data$spotify_hours + rnorm(100, 0, 5)   9.1.4 Modelling interactions Task: Test how age, musicality, and their interaction predict spotify_hours. Examine your model. How close are the coefficients to the \\(b\\) values that you initially entered into the equation? model &lt;- lm(spotify_hours ~ age * musicality, data = my.data) summary(model) ## ## Call: ## lm(formula = spotify_hours ~ age * musicality, data = my.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.3758 -2.7977 0.1205 3.2512 10.9172 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 301.34010 4.30198 70.047 &lt; 2e-16 *** ## age -1.04024 0.10544 -9.866 2.89e-16 *** ## musicality 1.42275 1.01520 1.401 0.164 ## age:musicality 0.21569 0.02412 8.942 2.79e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.144 on 96 degrees of freedom ## Multiple R-squared: 0.942, Adjusted R-squared: 0.9402 ## F-statistic: 519.8 on 3 and 96 DF, p-value: &lt; 2.2e-16 Task: Check the model assumptions. Are the residuals normal? Do the other diagnostics look sensible? hist(resid(model)) par(mfrow = c(2, 2)) # to plot all of the below in one window plot(model) 9.1.4 Optional Task: Re-run the equation you created at Task 4 (this resets the spotify_hours variable to the exact fitted values without the noise (\\(\\epsilon\\))). Instead of adding a normally distributed residual (like you did in task 6), try adding one which is not. Then re-run the regression and check the model assumptions. What has changed? Task: If you selected a continuous measure of musicality you were working with a continuous\\(\\times\\)continuous interaction. A good way to visualise this is to ‘bin’ the data based on one of the explanatory variables. In the example above we might create a grouped age variable that split age into 4 or 5 equal sized categories (the function cut() will help you to achieve this). Construct a plot showing the relationship of spotify_hours and musicality for 4 different age-groups. This could be a different line for each age-group, or a different ‘facet’ for each. The code below introduces two new things we might not have come across within ggplot2: geom_smooth() and facet_grid(). *Tip:* Remember plotting the confidence interval around the regression line in last week’s lab? geom_smooth() saves so much time! # A few different examples of illustrating this: my.data$cutAge &lt;- cut(my.data$age, 4) #a ggplot library(ggplot2) ggplot(data = my.data, aes(x = musicality, y = spotify_hours, col=cutAge, fill=cutAge)) + geom_smooth(method=&quot;lm&quot;, se=T)+ theme_minimal() ggplot(data = my.data, aes(x = musicality, y = spotify_hours)) + geom_point()+ facet_grid(~cutAge)+ theme_minimal()   9.2 Generalised Linear Model (GLM) Okay, moving on from interactions, we’re now going to take a look at the generalised linear model, which you were introduced to in this weeks lecture (lecture 9). Task: Load in the titanic data using the following code: load(url(&quot;https://is.gd/rYCGR3&quot;)) In the lecture, we saw how to model the odds that an alien would be splatted by a disco ball given their quality of singing. Along similar lines, we’re now going to construct a model of the odds of someone surviving the sinking of the titanic, given factors such as age, sex, class of ticket etc. Variable Description passenger_id ID of passenger survived Categorical indicating whether (1) or not (0) the passenger survived pclass Class of ticket (1,2,3) name Name of passenger sex Sex of passenger (‘male’/‘female’) age Age in year sib_sp Number of spouses and siblings on board parch Number of parents or children on board ticket Ticket code fare Ticket fare cabin Cabin number embarked Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) Task: Plot the survived variable against the pclass variable, and use abline() to add to this the line for a simple regression of survival by class of ticket. with(titanic, plot(survived ~ pclass)) abline(lm(survived~pclass, data=titanic)) Task: Probability p(x) can’t be linear. Odds, ranging from 0 to Infinity, are calculated as \\(p(x)/(1-p(x))\\). Log-odds are more linear, and range from -Infinity to +Infinity. If the probability of someone surviving is 0.6. What are the odds, and what are the log odds? p=0.6 (p/(1-p)) # odds ## [1] 1.5 log(p/(1-p)) # log odds ## [1] 0.4054651 Task: Use glm() to model survival predicted by pclass. Remember to set family=binomial, and take a look at the coefficients using coef(). logit_mod &lt;- glm(survived ~ pclass, data = titanic, family=binomial) coef(logit_mod) ## (Intercept) pclass ## 1.4467895 -0.8501067 9.2.1 Log-odds and predicted probabilities Task: Where the equation for our linear model was \\(y=b_0+b_1x\\), for our logit model it is \\(ln(\\frac{p}{(1-p)})=b_0+b_1x\\). For the model we just ran, it is \\(ln(\\frac{p}{(1-p)})=1.45-0.85*pclass\\). We can construct these fitted values using the code below: titanic$fit = 1.4467895 - 0.8501067*titanic$pclass #or titanic$fit = coef(logit_mod)[1] + coef(logit_mod)[2]*titanic$pclass Task: Remembering that these numbers are on the scale of log-odds, we can, get out the predicted probability of survival using the inverse logit \\(p_i=\\frac{e^L_i}{(1+e^L_i)}\\) (this is the inverse of the log-odds formula \\(L_i=log(\\frac{p_i}{1-p_i}))\\). predicted_probs &lt;- exp(titanic$fit) / (1 + exp(titanic$fit)) And this is exactly what we get when we use the predict() function! Note: we specify type=&quot;response&quot; here, but if you use type=&quot;link&quot; you get the log-odds which we just calculated. titanic$p_surv &lt;- predict(logit_mod, type=&quot;response&quot;) #and check that they are the same using the all.equal() function all.equal(predicted_probs, titanic$p_surv) ## [1] TRUE Maximum likelihood estimation How does the model find the coefficients? We’re no longer talking about minimising sums of squares, like we were with a continuous variable and lm(). This is also why we don’t have things like \\(R^2\\) for logit regression, because \\(SS_\\text{regression}/SS_\\text{total}\\) no longer makes sense in the context of 0s and 1s. Instead, models are fitted use Maximum Likelihood Estimation (MLE) to work out the coefficients which maximise the likelihood of the observed data. (There’s a lot of info on MLE if you want to google it!) 9.2.2 Predicted vs Observed Task: Now that we have our fitted values in the p_surv variable, we can compare those to the observed data. Use the ifelse() function to assign values of 0 and 1 to the p_surv predictions based on whether they are above or below 0.5. titanic$p_surv &lt;- ifelse(titanic$p_surv &gt; 0.5, 1, 0) Task: Create a table of these predictions against the survived variable (the observed data). table( actual = titanic$survived, predicted = titanic$p_surv ) ## predicted ## actual 0 1 ## 0 469 80 ## 1 206 136 9.3 Your turn (Less guidance) Task: What factors contribute to increased odds of survival for passengers on the titanic? titanic_model &lt;- glm(survived ~ age + sex + pclass + sib_sp + parch, data = titanic, family=binomial) Task: How does a passenger being male (relative to female) influence the odds of survival? exp(coef(titanic_model)) ## (Intercept) age sexmale pclass sib_sp parch ## 275.2051697 0.9565700 0.0715638 0.2682218 0.6947680 0.9636555 # 0.07 odds of survival for males relative females Task: Extra: Try doing a bit of googling to find the function which can quickly give you some confidence intervals for your betas. # remember to exp()! exp(confint(titanic_model)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 97.35567063 832.4627665 ## age 0.94104651 0.9716845 ## sexmale 0.04603557 0.1089289 ## pclass 0.20199683 0.3512246 ## sib_sp 0.53799311 0.8843912 ## parch 0.75718485 1.2146246 If the plot disappears after a split-second, just click inside the RStudio plotting window and it should pop back up.↩ "],
["lab-10.html", "Chapter 10 Lab 10 10.1 apply functions 10.2 Lab 10 10.2.1 An Example Analysis", " Chapter 10 Lab 10 This lab is a little less guided, and a little more like the take-home exam. The main thing to remember when carrying out an exercise like this is to make it clear why you made the decisions you made, either as a write-up or as comments (beginning with #) in the R code. There are no absolutely right or wrong answers, just sensible and less sensible things to try out! Take some time to read this document fully before getting started with any analyses. Think about each variable in the dataset in terms of what it measures and the type of data it provides. Tip: Don’t be tempted to immediately run a certain model. Analysing a collected dataset is a process and these labs have been structured to illustrate many of the steps taken when analysing data ‘for real’. Think about some of the exercises from past labs to generate ideas about the steps you want to take. 10.1 apply functions There is one more bit of programming we’d like you to be familiar with – the apply() family functions. These function take some data structure and apply any function to each element of that structure. To illustrate, let’s look at apply(). Imagine I have a data frame like this: mat &lt;- matrix(rnorm(100), ncol = 5) df &lt;- data.frame(id = 1:20, mat) head(df) ## id X1 X2 X3 X4 X5 ## 1 1 1.6363745 -0.83842519 0.3728223 0.006180753 -0.6092358 ## 2 2 -0.8760820 0.37245830 -0.1414563 2.046797547 0.5266528 ## 3 3 1.8021205 -0.01405419 -1.0615777 1.905091329 2.9117237 ## 4 4 -0.5493263 0.23579541 1.1208409 1.659623084 0.2307443 ## 5 5 0.3001265 0.48284934 -0.1680932 -0.845594145 0.3560563 ## 6 6 1.0421078 0.42371533 0.1735486 -0.175874533 -2.0643879 let’s say I want to know which of the X1-X5 columns contains the largest value for each participant. If we only take the first row, for example, then the task is easy, right? df[1, 2:6] ## X1 X2 X3 X4 X5 ## 1 1.636374 -0.8384252 0.3728223 0.006180753 -0.6092358 which.max(df[1, 2:6]) ## X1 ## 1 If we want to know this bit of information for every row of the data frame, doing it manually is quite tedious. We could instead use a for loop: max_col &lt;- c() for (i in 1:nrow(df)) max_col &lt;- c(max_col, which.max(df[i, 2:6])) max_col ## X1 X4 X5 X4 X2 X1 X3 X5 X5 X4 X2 X3 X3 X2 X2 X4 X2 X1 X1 X5 ## 1 4 5 4 2 1 3 5 5 4 2 3 3 2 2 4 2 1 1 5 This loop iterates over the rows of df (for (i in 1:nrow(df))) and for each cycle it adds the output of the which.max() function to the max_col variable. There is nothing wrong with this approach but it is a little wordy to write and not the fastest when it comes to computation time. The apply() function can be used to achieve the same outcome faster and using less code: max_col2 &lt;- apply(X = df[ , 2:6], MARGIN = 1, FUN = which.max) max_col2 ## [1] 1 4 5 4 2 1 3 5 5 4 2 3 3 2 2 4 2 1 1 5 all(max_col == max_col2) # result is the same ## [1] TRUE So how does this work? apply() requires 3 arguments: - X – a matrix (remember that data frames can be treated as matrices) - MARGIN – 1 for by row, 2 for by column - FUN – name of function to apply (notice no ()s) Other arguments can be provided, for example na.rm = T if the function applied takes the na.rm = argument (e.g., mean()). The function takes the matrix provided to the X = argument and applies the function passed to FUN = along the margin (rows/columns) of the matrix, specified by the MARGIN = argument. This is useful, don’t you think? The function becomes even more useful when you realise you can apply your own functions. Let’s say, we want to know the standard error estimate based on the 5 values for each participant: # define function std.error &lt;- function(x, na.rm) sd(x, na.rm = na.rm)/sqrt(length(na.omit(x))) apply(df[ , 2:6], 1, std.error, na.rm = T) # na.rm value will get passed on to std.error ## [1] 0.4374026 0.4821450 0.7189106 0.3850806 0.2439404 0.5251694 0.3409391 ## [8] 0.3206761 0.5697642 0.2644058 0.1898708 0.4080338 0.6083222 0.4635421 ## [15] 0.5036622 0.3922462 0.5230088 0.3590511 0.4263829 0.3458455 You don’t even have to create a function object! # same thing, different way apply(df[ , 2:6], 1, function(x) sd(x, na.rm = T)/sqrt(length(na.omit(x)))) ## [1] 0.4374026 0.4821450 0.7189106 0.3850806 0.2439404 0.5251694 0.3409391 ## [8] 0.3206761 0.5697642 0.2644058 0.1898708 0.4080338 0.6083222 0.4635421 ## [15] 0.5036622 0.3922462 0.5230088 0.3590511 0.4263829 0.3458455 Task: Calculate the standard deviation of each of the X columns of df. apply(df[ , 2:6], 2, sd, na.rm = T) ## X1 X2 X3 X4 X5 ## 0.9639913 0.8348326 0.9351637 1.2096203 1.2327756 There are quite a few functions in the apply() family (e.g., vapply(), tapply(), mapply(), sapply()), each doing something slightly different and each appropriate for a different data structure (vector, matrix, list, data.frame) and different task. You can read about all of them, if you want but we’d like to tell you about one in particular – lapply() The lapply() function takes a list (list apply) and applies a function to each of its elements. The function returns a list. This function is useful because data frames can also be treated as lists. This is what you do every time you use the $ operator. So, to do Task 1 using lapply(), you can simply do: lapply(df[ , 2:6], sd, na.rm = T) ## $X1 ## [1] 0.9639913 ## ## $X2 ## [1] 0.8348326 ## ## $X3 ## [1] 0.9351637 ## ## $X4 ## [1] 1.20962 ## ## $X5 ## [1] 1.232776 If you don’t want the output to be a list, just use unlist(): unlist(lapply(df[ , 2:6], sd, na.rm = T)) ## X1 X2 X3 X4 X5 ## 0.9639913 0.8348326 0.9351637 1.2096203 1.2327756 Task: Round all the X columns to 2 decimal places. Hint: Don’t forget you want to be modifying the columns in question. df[ , 2:6] &lt;- lapply(df[ , 2:6], round, 2) head(df) ## id X1 X2 X3 X4 X5 ## 1 1 1.64 -0.84 0.37 0.01 -0.61 ## 2 2 -0.88 0.37 -0.14 2.05 0.53 ## 3 3 1.80 -0.01 -1.06 1.91 2.91 ## 4 4 -0.55 0.24 1.12 1.66 0.23 ## 5 5 0.30 0.48 -0.17 -0.85 0.36 ## 6 6 1.04 0.42 0.17 -0.18 -2.06 So these are the two most useful functions from the apply() family you should know. As a general tip, you can use apply() for by-row operations and lapply() for by-column operations on data frames. The nice thing about these function is that they help you avoid repetitive code. For instance, you can now turn several columns of your data set into factors using something like df[ , c(2, 5, 7, 8)] &lt;- lapply(df[ , c(2, 5, 7, 8)], factor). Cool, innit?   10.2 Lab 10 Load the data for this lab using the following code: load(url(&quot;https://edin.ac/2KNwiMU&quot;)) The data concerns a study investigating attitudes about fox-hunting in the UK. 412 participants were asked to rate their attitude towards hunting by marking a point along a line. The endpoints of the line were labelled strongly opposed and strongly in favour; the distance of each mark along its line was later measured, and scaled to a variable ranging from 1 (opposed) to 7 (in favour). The resulting variable is called prohunt in the dataset. Also measured were where participants lived (urban, suburban, country) as well as their politics, using the Stone-Corley Wingedness Inventory, which returns a score along the political spectrum ranging from -100 (extremely left-wing, socialist) to 100 (extremely right-wing, conservative). This score can be found in the spectrum column. Finally, participants were asked to indicate whether they were prepared to participate in a follow-up interview; the followup column shows their response. 10.2.1 The Task Your job this week is to clean, describe and analyse the data to answer the questions below. You should also create some graphics to accompany your findings. Questions: How do participants’ political view and the rurality of where they live predict their attitudes towards foxhunting? Is the relationship between political views and foxhunting attitudes different between different ruralities? If so, how? Some things you might want to think about first: Does all of the data look sensible, given the descriptions above? Did participants decide not to participate in the follow-up at random? What are the predicted probability of completing follow-up for each value of spectrum? The types of output you might produce include regression statistics, scatterplots, and graphs showing regression effects. Don’t forget to document your R code to show what you did.   Good luck with the exam! 10.2.1 An Example Analysis (N.B., this is not a ‘correct’ answer, just a sensible one!) Start by loading the data and examining it… fox &lt;- read.csv(&#39;data/fox.csv&#39;) summary(fox) ## id home spectrum followup prohunt ## ABI : 1 country :138 Min. :-999.00 N: 25 Min. :0.000 ## ABK : 1 suburban:137 1st Qu.: -25.00 Y:387 1st Qu.:3.095 ## ABM : 1 urban :137 Median : -3.00 Median :3.770 ## ABR : 1 Mean : -11.69 Mean :3.732 ## ABY : 1 3rd Qu.: 19.25 3rd Qu.:4.322 ## ABZ : 1 Max. : 86.00 Max. :7.000 ## (Other):406 Most aspects look quite sensible; there are 412 rows, as promised, for example. However, there is at least one value of \\(-999\\) in spectrum (likely indicates missing data) and also some 0s in prohunt (coding error perhaps as we know prohunt should be on the scale 1-7). Let’s fix this, and record how many datapoints have been removed (this might go into a writeup) fox$spectrum[fox$spectrum == -999] &lt;- NA fox$prohunt[fox$prohunt == 0] &lt;- NA # complete.cases() might be new. Obviously there are other ways of doing it. # The &quot;!&quot; means &quot;not&quot; sum(!complete.cases(fox)) ## [1] 6 # alternative sum(is.na(fox$prohunt)) ## [1] 2 # etc. Probably the next thing I would do is do a scatterplot of the data, to get the ‘lay of the land’. spectrum makes a good \\(x\\)-axis variable. Two plot commands are below: The first, I did just to see what was going on. In the second, I used colour and shape to try and work out what was happening (note the trick of using col = as.numeric(home); the values will be c(1, 2, 3), and as long as those map onto different colours, I’m good to go). You can see the output in Fig. 1 # Produces figure 1 par(mfrow = c(1, 2)) # set two columns, 1 row with(fox, plot(spectrum, prohunt)) # second plot, this time with colour (for levels of &quot;home&quot;) # and shape (for &quot;non-followup people&quot;) with(fox, plot(spectrum, prohunt, col = as.numeric(home), pch = ifelse(followup==&#39;Y&#39;, 2, 4))) Figure 10.1: Figure 1. Two scatterplots of fox; the second is coloured by home (black = country, red = suburban, green = urban) and shaped by whether participants want to participate in the followup (cross = no). # A ggplot alternative for exploring this (produces figure 2): library(ggplot2) spec_pro &lt;- ggplot(data = fox, aes(x = spectrum, y = prohunt, colour = followup)) spec_pro + geom_point() + theme_minimal() + facet_grid(. ~ home) ## Warning: Removed 6 rows containing missing values (geom_point). Figure 10.2: Figure 2. A ggplot alternative to exploring the role of home on the relationship between spectrum and prohunt (also considering followup) OK, there’s something weird about the no-followup guys. They all seem to be left-wing. Can I show that spectrum predicts whether you want to follow up? Remember this is a logit model. model &lt;- glm(followup ~ spectrum, data = fox, family = binomial) # the test below shows that knowing about spectrum improves model fit anova(model, test = &#39;Chisq&#39;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: followup ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 407 188.055 ## spectrum 1 111.69 406 76.368 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # and the summary here shows that political spectrum is important summary(model) ## ## Call: ## glm(formula = followup ~ spectrum, family = binomial, data = fox) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.81851 0.00962 0.04340 0.13826 2.29444 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.84657 1.01355 6.755 1.43e-11 *** ## spectrum 0.12057 0.02079 5.799 6.68e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 188.055 on 407 degrees of freedom ## Residual deviance: 76.368 on 406 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 80.368 ## ## Number of Fisher Scoring iterations: 8 We can summarise the model by saying that for each point to the right on spectrum, you’re \\(e^{0.12} = 1.13\\) times more likely to agree to a followup study. Left-wingers don’t seem keen at all! Here’s how to draw a graph showing the model of the probability of accepting a followup depending on spectrum (output in Fig. 3). Note, it requires a bit of web-searching to find out about the na.action = na.exclude argument to glm() (and lm()). The search term I used was ``R include NAs in fitted()’’. # Produces figure 3 model &lt;- glm(followup ~ spectrum, data = fox, family = binomial, na.action = na.exclude) with(fox, plot(fitted(model)[order(spectrum)] ~ spectrum[order(spectrum)], type = &#39;l&#39;, xlab = &#39;spectrum&#39;, ylab = &#39;p(followup)&#39;)) Figure 10.3: Figure 3. Probability of allowing a followup as a function of spectrum: model fit. 10.2.1 The main analysis Regardless of whether we decided to run the binomial model or not, it’s clear that these non-followup people are odd – they seem to be behaving differently to the rest of the population. I’d be tempted to take them out of the data and build a model without them. Note that the scatterplot makes it pretty obvious that there’ll be some kind of interaction, so: # note the &quot;subset&quot; setting within &quot;lm()&quot; -- the lazy person&#39;s way! model &lt;- lm(prohunt ~ spectrum*home, data = fox, subset = followup == &#39;Y&#39;) # this is equivalent: fox &lt;- subset(fox, followup == &#39;Y&#39;) model &lt;- lm(prohunt ~ spectrum * home, data = fox) anova(model) ## Analysis of Variance Table ## ## Response: prohunt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## spectrum 1 4.049 4.049 6.0843 0.0140853 * ## home 2 83.813 41.907 62.9758 &lt; 2.2e-16 *** ## spectrum:home 2 9.908 4.954 7.4446 0.0006752 *** ## Residuals 375 249.540 0.665 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # shows that each term improves the model summary(model) ## ## Call: ## lm(formula = prohunt ~ spectrum * home, data = fox) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.63294 -0.48986 -0.02376 0.51816 2.18220 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.461984 0.074276 60.073 &lt; 2e-16 *** ## spectrum 0.012554 0.002743 4.577 6.43e-06 *** ## homesuburban -0.931977 0.103005 -9.048 &lt; 2e-16 *** ## homeurban -1.033300 0.103487 -9.985 &lt; 2e-16 *** ## spectrum:homesuburban -0.010994 0.003562 -3.087 0.002174 ** ## spectrum:homeurban -0.013265 0.003598 -3.687 0.000261 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8157 on 375 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.2815, Adjusted R-squared: 0.2719 ## F-statistic: 29.39 on 5 and 375 DF, p-value: &lt; 2.2e-16 All the coefficients are significant in this model (woop woop!), and can be read top-to-bottom as follows: People of middling political persuasion who live in the country, (spectrum = 0 and home = country at (Intercept)) have an attitude of 4.46. For each additional spectrum point, that attitude goes up by 0.01, for people in the country. People in suburban homes are less approving of foxhunting by -0.93. People in urban homes are less approving of foxhunting by -1.03. Points 3 and 4 hold for people at zero (intercept) on the political spectrum, but, For suburban people the rise in 2 is reduced: actual rise \\(=0.01+-0.01=0\\). Similarly, for urban people, the actual rise per point on the spectrum \\(=0.01+-0.01=0\\), or practically zero. So we can conclude that you need to live in the country and to be increasingly right-wing to have a positive attitude towards fox hunting. 10.2.1 Other stuff you could do You could re-run the analysis above using orthogonal coding, to compare townies (urban and suburban) to country-dwellers, and then different types of townie to each other. That would look something like this: contrasts(fox$home) &lt;- cbind(CvSU = c(-2, 1, 1) / 3, SvU = c(0, -1, 1) / 2) # we&#39;ve already removed the no-followup guys model &lt;- lm(prohunt ~ spectrum*home, data = fox) # anova(model) will be the same summary(model) ## ## Call: ## lm(formula = prohunt ~ spectrum * home, data = fox) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.63294 -0.48986 -0.02376 0.51816 2.18220 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.806892 0.041903 90.850 &lt; 2e-16 *** ## spectrum 0.004467 0.001418 3.150 0.001766 ** ## homeCvSU -0.982639 0.089935 -10.926 &lt; 2e-16 *** ## homeSvU -0.101323 0.101419 -0.999 0.318413 ## spectrum:homeCvSU -0.012129 0.003189 -3.803 0.000167 *** ## spectrum:homeSvU -0.002270 0.003253 -0.698 0.485690 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8157 on 375 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.2815, Adjusted R-squared: 0.2719 ## F-statistic: 29.39 on 5 and 375 DF, p-value: &lt; 2.2e-16 This model shows that townie types (urban and suburban) are less likely to have positive attitudes to fox-hunting than country-dwellers (line 3); and that it’s only the townies’ attitudes are less affected by position on the political spectrum, although there’s no difference between urbanites and suburbanites (lines 5 and 6). This last regression model may make the most sense of the data, such as it is, but, as long as you’ve explained why you’ve done what you’ve done, and as long as what you’ve done is reasonable, you’ve done a good job. "]
]
