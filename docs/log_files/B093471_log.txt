

	####### B093471 R script #######

library(psych, quietly = T)
[1] 1
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(car, quietly = T)
[1] 2
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[11] "base"     
library(stats, quietly = T)
[1] 3
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[11] "base"     
library(readr, quietly = T)
[1] 4
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[11] "base"     
library(interactions, quietly = T)
[1] 5
 [1] "interactions" "car"          "carData"      "psych"        "readr"        "stats"        "graphics"     "grDevices"   
 [9] "utils"        "datasets"     "methods"      "base"        
library(effects, quietly = T)
[1] 6
 [1] "effects"      "interactions" "car"          "carData"      "psych"        "readr"        "stats"        "graphics"    
 [9] "grDevices"    "utils"        "datasets"     "methods"      "base"        
setwd("C:/Users/user/Desktop/University work/3rd year/Semester 1/RMS 2/Report/Coursework information")
[1] 7
data <- read.csv("~/Desktop/rms2/code_check/RMS2_report_1920.csv", 
    header = TRUE)
[1] 8
View(data)
[1] 9
describe(data)[2:6, c(2:4, 8, 9, 11)]
[1] 10
            n  mean   sd  min   max  skew
incidents 200  0.48 0.19 0.08  0.92  0.32
training  200  1.50 0.50 1.00  2.00  0.00
feedback  200  1.50 0.50 1.00  2.00  0.00
empathy   200 10.18 2.54 3.00 16.00 -0.37
years     200  8.13 3.08 1.00 19.00  0.52
str(data)
[1] 11
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : int  2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : int  2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
data$training <- factor(data$training, labels = c("no training", 
    "training"))
[1] 12
data$feedback <- factor(data$feedback, labels = c("feedback", 
    "no feedback"))
[1] 13
data$incidents <- (data$incidents * 100)
[1] 14
str(data)
[1] 15
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  66.7 33.3 33.3 41.7 50 ...
 $ training : Factor w/ 2 levels "no training",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : Factor w/ 2 levels "feedback","no feedback": 2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
which(is.na(data))
[1] 16
integer(0)
summary(data)
[1] 17
    subject         incidents             training          feedback      empathy          years      
 Min.   :  1.00   Min.   : 8.333   no training:100   feedback   :100   Min.   : 3.00   Min.   : 1.00  
 1st Qu.: 50.75   1st Qu.:33.333   training   :100   no feedback:100   1st Qu.: 9.00   1st Qu.: 6.00  
 Median :100.50   Median :41.667                                       Median :10.00   Median : 8.00  
 Mean   :100.50   Mean   :47.500                                       Mean   :10.18   Mean   : 8.13  
 3rd Qu.:150.25   3rd Qu.:58.333                                       3rd Qu.:12.00   3rd Qu.:10.00  
 Max.   :200.00   Max.   :91.667                                       Max.   :16.00   Max.   :19.00  
describe(data)[c(2, 5), c(2:4, 8, 9, 11)]
[1] 18
            n  mean    sd  min   max  skew
incidents 200 47.50 18.66 8.33 91.67  0.32
empathy   200 10.18  2.54 3.00 16.00 -0.37
summary(data[, 3:4])
[1] 19
        training          feedback  
 no training:100   feedback   :100  
 training   :100   no feedback:100  
hist(data$incidents, main = "Histogram of incidents", col = "deeppink")
[1] 20
abline(v = mean(data$incidents), col = "black", lwd = 3)
[1] 21
dev.off()
[1] 22
hist(data$years, main = "Histogram of years", col = "plum2")
[1] 23
abline(v = mean(data$years), col = "black", lwd = 3)
[1] 24
dev.off()
[1] 25
hist(data$empathy, main = "Histogram of empathy", col = "seagreen4")
[1] 26
abline(v = mean(data$empathy), col = "black", lwd = 3)
[1] 27
dev.off()
[1] 28
plot1 <- boxplot(data$incidents ~ data$training, main = "Boxplot for training/notraining & incidents reported", 
    xlab = "Training", ylab = "Incidents", col = c("darkorchid", 
        "darkorchid4"))
[1] 29
dev.off()
[1] 30
plot2 <- boxplot(data$incidents ~ data$feedback, main = "Boxplot for feedback/no feedback & incidents reported", 
    xlab = "Feedback", ylab = "Incidents", col = c("deepskyblue", 
        "deepskyblue4"))
[1] 31
dev.off()
[1] 32
m1a <- (lm(empathy ~ years, data = data))
[1] 33
summary(m1a)
[1] 34

Call:
lm(formula = empathy ~ years, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.5399 -1.5160  0.0708  1.6357  5.8533 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  8.40625    0.49085  17.126  < 2e-16 ***
years        0.21756    0.05648   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.453 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

x11()
[1] 35
NULL
dev.off()
[1] 36
plot(data$empathy ~ data$years, main = "Scatterplot of years predicting empathy", 
    ylab = "Empathy measure", xlab = "Years of experience", col = "blueviolet")
[1] 37
abline(lm(data$empathy ~ data$years), col = "black", lwd = 2)
[1] 38
dev.off()
[1] 39
crPlots(m1a)
[1] 40
dev.off()
[1] 41
hist(m1a$residuals, main = "Histogram of empathy and years", 
    xlab = "Residuals", col = "gold")
[1] 42
dev.off()
[1] 43
shapiro.test(m1a$residuals)
[1] 44

	Shapiro-Wilk normality test

data:  m1a$residuals
W = 0.98911, p-value = 0.1321

par(mfrow = c(1, 2))
[1] 45
$mfrow
[1] 1 1

residualPlots(m1a, fitted = FALSE)
[1] 46
dev.off()
[1] 47
par(mfrow = c(1, 1))
[1] 48
$mfrow
[1] 1 2

ncvTest(m1a)
[1] 49
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.9815033, Df = 1, p = 0.32183
durbinWatsonTest(m1a)
[1] 50
 lag Autocorrelation D-W Statistic p-value
   1       0.1017162      1.769879   0.094
 Alternative hypothesis: rho != 0
data$yearscentre <- data$years - mean(data$years)
[1] 51
data$empathycentre <- data$empathy - mean(data$empathy)
[1] 52
centred_results <- lm(data$empathycentre ~ data$yearscentre, 
    data = data)
[1] 53
summary(centred_results)
[1] 54

Call:
lm(formula = data$empathycentre ~ data$yearscentre, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.5399 -1.5160  0.0708  1.6357  5.8533 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)      -6.013e-16  1.735e-01   0.000 1.000000    
data$yearscentre  2.176e-01  5.648e-02   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.453 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

m1b <- lm(data$empathycentre ~ data$yearscentre + I(yearscentre^2), 
    data = data)
[1] 55
summary(m1b)
[1] 56

Call:
lm(formula = data$empathycentre ~ data$yearscentre + I(yearscentre^2), 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.5069 -1.4011 -0.0835  1.1707  6.0622 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.977696   0.167423   5.840 2.13e-08 ***
data$yearscentre  0.383236   0.047986   7.986 1.13e-13 ***
I(yearscentre^2) -0.103645   0.009861 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.969 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

crPlots(m1b)
[1] 57
dev.off()
[1] 58
hist(m1b$residuals, main = "Histogram of empathy and years", 
    xlab = "Residuals", col = "tomato")
[1] 59
dev.off()
[1] 60
shapiro.test(m1b$residuals)
[1] 61

	Shapiro-Wilk normality test

data:  m1b$residuals
W = 0.9941, p-value = 0.6155

residualPlots(m1b)
[1] 62
ncvTest(m1b)
[1] 63
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
durbinWatsonTest(m1b)
[1] 64
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855    0.83
 Alternative hypothesis: rho != 0
vif(m1b)
[1] 65
data$yearscentre I(yearscentre^2) 
        1.120963         1.120963 
m2 <- lm(incidents ~ training + years + empathy, data = data)
[1] 66
summary(m2)
[1] 67

Call:
lm(formula = incidents ~ training + years + empathy, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-35.648 -12.270  -1.474  10.105  53.180 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       40.7320     5.1814   7.861 2.48e-13 ***
trainingtraining -15.2427     2.3166  -6.580 4.20e-10 ***
years             -1.4085     0.3889  -3.622 0.000372 ***
empathy            2.5396     0.4735   5.363 2.29e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.21 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

plot(m2, which = 4, sub.caption = "Cook's distance model 2 plot", 
    col = "magenta")
[1] 68
abline(a = 4/(200 - 3 - 1), b = 0, col = "red")
[1] 69
dev.off()
[1] 70
par(mfrow = c(1, 1))
[1] 71
$mfrow
[1] 1 1

which(cooks.distance(m2) > (4/(200 - 3 - 1)))
[1] 72
 13  35  74 137 165 
 13  35  74 137 165 
stdz <- MASS::studres(m2)
[1] 73
head(stdz, 20)
[1] 74
         1          2          3          4          5          6          7          8          9         10         11 
 1.7039666 -0.4759818 -0.6519659  0.1968240  0.6801599 -0.8893909  2.2672645 -0.3290103  0.2412298 -1.6591914  1.8757093 
        12         13         14         15         16         17         18         19         20 
 0.0298675  3.3861502  0.2412298  1.2329081 -1.0885527 -0.8362148 -0.3189418  1.9495174  0.4777973 
which(abs(stdz) > 2)
[1] 75
  7  13  35  74 159 165 176 181 
  7  13  35  74 159 165 176 181 
hats <- hatvalues(m2)
[1] 76
which(hats > 2 * mean(hats))
[1] 77
 61  88  89  92  94 137 141 154 163 175 200 
 61  88  89  92  94 137 141 154 163 175 200 
crPlots(m2)
[1] 78
hist(m2$residuals, main = "Histogram of model 2 residuals", xlab = "Residuals for model 2", 
    col = "turquoise")
[1] 79
dev.off()
[1] 80
shapiro.test(m2$residuals)
[1] 81

	Shapiro-Wilk normality test

data:  m2$residuals
W = 0.97994, p-value = 0.005838

ncvTest(m2)
[1] 82
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
residualPlots(m2)
[1] 83
durbinWatsonTest(m2)
[1] 84
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.164
 Alternative hypothesis: rho != 0
vif(m2)
[1] 85
training    years  empathy 
1.020633 1.085089 1.092467 
m3 <- lm(incidents ~ feedback + years + empathy, data = data)
[1] 86
summary(m3)
[1] 87

Call:
lm(formula = incidents ~ feedback + years + empathy, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-35.580 -11.148  -1.097  11.092  53.552 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)          27.6161     5.4108   5.104 7.84e-07 ***
feedbackno feedback  13.2132     2.3585   5.602 7.09e-08 ***
years                -1.2689     0.3975  -3.192  0.00165 ** 
empathy               2.3187     0.4829   4.802 3.11e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.63 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

plot(m3, which = 4, sub.caption = "Cook's distance model 3 plot", 
    col = "orange")
[1] 88
abline(a = 4/(200 - 3 - 1), b = 0, col = "blue")
[1] 89
dev.off()
[1] 90
par(mfrow = c(1, 1))
[1] 91
$mfrow
[1] 1 1

which(cooks.distance(m3) > (4/(200 - 3 - 1)))
[1] 92
 73  74  89 137 156 160 165 176 
 73  74  89 137 156 160 165 176 
hist(m3$residuals, main = "Histogram of model 3 residuals", xlab = "Residuals for model 3", 
    col = "firebrick")
[1] 93
dev.off()
[1] 94
ncvTest(m3)
[1] 95
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.01731234, Df = 1, p = 0.89532
residualPlots(m3)
[1] 96
dev.off()
[1] 97
crPlots(m3)
[1] 98
dev.off()
[1] 99
durbinWatsonTest(m3)
[1] 100
 lag Autocorrelation D-W Statistic p-value
   1       0.2738264      1.444081       0
 Alternative hypothesis: rho != 0
vif(m3)
[1] 101
feedback    years  empathy 
1.005211 1.077467 1.079397 
summary(m2)$r.squared
[1] 102
[1] 0.2564555
summary(m3)$r.squared
[1] 103
[1] 0.2175265
m4 <- lm(incidents ~ training + feedback + training:feedback, 
    data = data)
[1] 104
summary(m4)
[1] 105

Call:
lm(formula = incidents ~ training + feedback + training:feedback, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-40.000 -10.500   0.000   8.333  43.333 

Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                            49.333      2.314  21.323  < 2e-16 ***
trainingtraining                      -16.000      3.272  -4.890 2.09e-06 ***
feedbackno feedback                     9.667      3.272   2.954  0.00352 ** 
trainingtraining:feedbackno feedback    5.333      4.627   1.153  0.25047    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.36 on 196 degrees of freedom
Multiple R-squared:  0.2431,	Adjusted R-squared:  0.2315 
F-statistic: 20.99 on 3 and 196 DF,  p-value: 7.827e-12

shapiro.test(m4$residuals)
[1] 106

	Shapiro-Wilk normality test

data:  m4$residuals
W = 0.9767, p-value = 0.002072

hist(m4$residuals, main = "Histogram of model 4", xlab = "Model 4", 
    col = "steelblue")
[1] 107
ncvTest(m4)
[1] 108
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 1.881673, Df = 1, p = 0.17014
durbinWatsonTest(m4)
[1] 109
 lag Autocorrelation D-W Statistic p-value
   1       0.1114255      1.770733   0.124
 Alternative hypothesis: rho != 0
vif(m4)
[1] 110
         training          feedback training:feedback 
                2                 2                 3 
plot(effect("training:feedback", m4), main = "Training*feedback interaction plot", 
    multiline = TRUE)
[1] 111
dev.off()
[1] 112
