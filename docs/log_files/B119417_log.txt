

	####### B119417 R script #######

setwd("~/Downloads")
[1] 1
library(psych, quietly = T)
[1] 2
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(car, quietly = T)
[1] 3
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[10] "methods"   "base"     
library(DescTools, quietly = T)
[1] 4
 [1] "DescTools" "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"    
[10] "datasets"  "methods"   "base"     
library(interactions, quietly = T)
[1] 5
 [1] "interactions" "DescTools"    "car"          "carData"      "psych"        "readr"        "stats"       
 [8] "graphics"     "grDevices"    "utils"        "datasets"     "methods"      "base"        
data <- read.csv("../RMS2_report_1920.csv")
[1] 6
summary(data)
[1] 7
    subject         incidents          training      feedback      empathy          years      
 Min.   :  1.00   Min.   :0.08333   Min.   :1.0   Min.   :1.0   Min.   : 3.00   Min.   : 1.00  
 1st Qu.: 50.75   1st Qu.:0.33333   1st Qu.:1.0   1st Qu.:1.0   1st Qu.: 9.00   1st Qu.: 6.00  
 Median :100.50   Median :0.41667   Median :1.5   Median :1.5   Median :10.00   Median : 8.00  
 Mean   :100.50   Mean   :0.47500   Mean   :1.5   Mean   :1.5   Mean   :10.18   Mean   : 8.13  
 3rd Qu.:150.25   3rd Qu.:0.58333   3rd Qu.:2.0   3rd Qu.:2.0   3rd Qu.:12.00   3rd Qu.:10.00  
 Max.   :200.00   Max.   :0.91667   Max.   :2.0   Max.   :2.0   Max.   :16.00   Max.   :19.00  
data$training <- factor(data$training, labels = c("No training", 
    "Training"))
[1] 8
data$feedback <- factor(data$feedback, labels = c("No feedback", 
    "Feedback"))
[1] 9
data$subject <- factor(data$subject)
[1] 10
describe(data)
[1] 11
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject*     1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training*    3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
hist(data$incidents, main = "Intervention in Bullying Incidents", 
    xlab = "Percentage of Incidents")
[1] 12
hist(data$empathy, main = "Teacher Empathy", xlab = "Empathy Score")
[1] 13
hist(data$years, main = "Teaching Experience", xlab = "Number of Years")
[1] 14
years_center <- data$years - mean(data$years)
[1] 15
empathy_center <- data$empathy - mean(data$empathy)
[1] 16
contrasts(data$training)
[1] 17
            Training
No training        0
Training           1
contrasts(data$feedback)
[1] 18
            Feedback
No feedback        0
Feedback           1
model_empathy_years <- lm(empathy ~ years, data = data)
[1] 19
summary(model_empathy_years)
[1] 20

Call:
lm(formula = empathy ~ years, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.5399 -1.5160  0.0708  1.6357  5.8533 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  8.40625    0.49085  17.126  < 2e-16 ***
years        0.21756    0.05648   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.453 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

stdz <- MASS::studres(model_empathy_years)
[1] 21
which(abs(stdz) > 2)
[1] 22
 61  68  70  88  89 137 141 142 154 175 200 
 61  68  70  88  89 137 141 142 154 175 200 
hats <- hatvalues(model_empathy_years)
[1] 23
which(hats > 2 * mean(hats))
[1] 24
 61  88  89  92  94 108 109 116 131 137 140 141 154 155 163 190 200 
 61  88  89  92  94 108 109 116 131 137 140 141 154 155 163 190 200 
plot(cooks.distance(model_empathy_years), ylab = "Cook's Distance", 
    main = "Cook's Distance Plot")
[1] 25
abline(h = cooks.distance(model_empathy_years) > (4/(200 - 1 - 
    1)), col = "yellow")
[1] 26
which(cooks.distance(model_empathy_years) > (4/(200 - 1 - 1)))
[1] 27
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
CVRs <- covratio(model_empathy_years)
[1] 28
which(CVRs > 1.03 | CVRs < 0.97)
[1] 29
 68 109 142 175 190 
 68 109 142 175 190 
hist(model_empathy_years$residuals, xlab = "Residuals of the Model", 
    main = "Histogram of Model Residuals")
[1] 30
shapiro.test(model_empathy_years$residuals)
[1] 31

	Shapiro-Wilk normality test

data:  model_empathy_years$residuals
W = 0.98911, p-value = 0.1321

residualPlots(model_empathy_years, xlab = "Years of Esperience", 
    main = "Residual Plot")
[1] 32
ncvTest(model_empathy_years)
[1] 33
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.9815033, Df = 1, p = 0.32183
durbinWatsonTest(model_empathy_years)
[1] 34
 lag Autocorrelation D-W Statistic p-value
   1       0.1017162      1.769879   0.108
 Alternative hypothesis: rho != 0
plot(data$empathy ~ data$years, xlab = "Years of Experience", 
    ylab = "Empathy", main = "Relationship between Empathy and Experience")
[1] 35
abline(lm(data$empathy ~ data$years))
[1] 36
lines(lowess(data$years, data$empathy))
[1] 37
model_higher <- lm(empathy ~ years_center + I(years_center^2), 
    data = data)
[1] 38
summary(model_higher)
[1] 39

Call:
lm(formula = empathy ~ years_center + I(years_center^2), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.5069 -1.4011 -0.0835  1.1707  6.0622 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)       11.152696   0.167423  66.614  < 2e-16 ***
years_center       0.383236   0.047986   7.986 1.13e-13 ***
I(years_center^2) -0.103645   0.009861 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.969 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

confint(model_higher)
[1] 40
                       2.5 %      97.5 %
(Intercept)       10.8225254 11.48286648
years_center       0.2886040  0.47786841
I(years_center^2) -0.1230923 -0.08419814
stdz_higher <- MASS::studres(model_higher)
[1] 41
which(abs(stdz_higher) > 2)
[1] 42
 30  68  69  70  74  76 122 142 162 175 
 30  68  69  70  74  76 122 142 162 175 
hats_higher <- hatvalues(model_higher)
[1] 43
which(hats_higher > 2 * mean(hats_higher))
[1] 44
 16  61  88  89  92 116 137 141 154 155 165 171 190 200 
 16  61  88  89  92 116 137 141 154 155 165 171 190 200 
plot(cooks.distance(model_higher), ylab = "Cook's Distance", 
    main = "Cook's Distance Plot")
[1] 45
abline(h = cooks.distance(model_higher) > (4/(200 - 2 - 1)), 
    col = "yellow")
[1] 46
which(cooks.distance(model_higher) > (4/(200 - 2 - 1)))
[1] 47
 61  89 137 141 154 163 165 175 
 61  89 137 141 154 163 165 175 
model_higher2 <- lm(empathy ~ years_center + I(years_center^2), 
    subset = c(-61, -89, -137, -141, -154, -163, -165, -175), 
    data = data)
[1] 48
summary(model_higher2)
[1] 49

Call:
lm(formula = empathy ~ years_center + I(years_center^2), data = data, 
    subset = c(-61, -89, -137, -141, -154, -163, -165, -175))

Residuals:
    Min      1Q  Median      3Q     Max 
-4.5344 -1.1205  0.1144  1.2235  4.8795 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       11.17359    0.17329  64.480  < 2e-16 ***
years_center       0.39440    0.05054   7.804 3.97e-13 ***
I(years_center^2) -0.10772    0.01353  -7.962 1.54e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.912 on 189 degrees of freedom
Multiple R-squared:  0.3587,	Adjusted R-squared:  0.3519 
F-statistic: 52.86 on 2 and 189 DF,  p-value: < 2.2e-16

hist(model_higher$residuals, xlab = "Residuals of the Model", 
    main = "Histogram of Model Residuals")
[1] 50
shapiro.test(model_higher$residuals)
[1] 51

	Shapiro-Wilk normality test

data:  model_higher$residuals
W = 0.9941, p-value = 0.6155

residualPlots(model_higher, main = "Residual Plots")
[1] 52
ncvTest(model_higher)
[1] 53
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
durbinWatsonTest(model_higher)
[1] 54
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855    0.86
 Alternative hypothesis: rho != 0
crPlots(model_higher)
[1] 55
vif(model_higher)
[1] 56
     years_center I(years_center^2) 
         1.120963          1.120963 
cor.test(data$years, data$empathy, alternative = "two.sided", 
    method = "spearman", exact = F)
[1] 57

	Spearman's rank correlation rho

data:  data$years and data$empathy
S = 923838, p-value = 9.723e-06
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.3071045 

model_training <- lm(incidents ~ training + empathy_center + 
    years_center, data = data)
[1] 58
summary(model_training)
[1] 59

Call:
lm(formula = incidents ~ training + empathy_center + years_center, 
    data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.551213   0.016298  33.821  < 2e-16 ***
trainingTraining -0.152427   0.023166  -6.580 4.20e-10 ***
empathy_center    0.025396   0.004735   5.363 2.29e-07 ***
years_center     -0.014085   0.003889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

confint(model_training)
[1] 60
                       2.5 %       97.5 %
(Intercept)       0.51907118  0.583355664
trainingTraining -0.19811433 -0.106739356
empathy_center    0.01605669  0.034734598
years_center     -0.02175353 -0.006415556
stdz_training <- MASS::studres(model_training)
[1] 61
which(abs(stdz_training) > 2)
[1] 62
  7  13  35  74 159 165 176 181 
  7  13  35  74 159 165 176 181 
hats_training <- hatvalues(model_training)
[1] 63
which(hats_training > 2 * mean(hats_training))
[1] 64
 61  88  89  92  94 137 141 154 163 175 200 
 61  88  89  92  94 137 141 154 163 175 200 
plot(cooks.distance(model_training), ylab = "Cook's Distance", 
    main = "Cook's Distance Plot")
[1] 65
abline(h = cooks.distance(model_training) > (4/(200 - 3 - 1)), 
    col = "yellow")
[1] 66
which(cooks.distance(model_training) > (4/(200 - 3 - 1)))
[1] 67
 13  35  74 137 165 
 13  35  74 137 165 
CVRs_training <- covratio(model_training)
[1] 68
which(CVRs_training > 1.06 | CVRs_training < 0.94)
[1] 69
  7  13  61  88  89 141 154 155 159 163 175 176 181 200 
  7  13  61  88  89 141 154 155 159 163 175 176 181 200 
model_training2 <- lm(incidents ~ training + empathy_center + 
    years_center, subset = c(-13, -35, -74, -137, -165), data = data)
[1] 70
summary(model_training2)
[1] 71

Call:
lm(formula = incidents ~ training + empathy_center + years_center, 
    data = data, subset = c(-13, -35, -74, -137, -165))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.28818 -0.11426 -0.00621  0.09990  0.40582 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.546086   0.015423  35.408  < 2e-16 ***
trainingTraining -0.153846   0.021969  -7.003 4.16e-11 ***
empathy_center    0.028505   0.004568   6.240 2.76e-09 ***
years_center     -0.016177   0.003838  -4.215 3.84e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1521 on 191 degrees of freedom
Multiple R-squared:  0.3042,	Adjusted R-squared:  0.2932 
F-statistic: 27.83 on 3 and 191 DF,  p-value: 5.621e-15

hist(model_training$residuals, xlab = "Residuals of the Model", 
    main = "Histogram of Model Residuals")
[1] 72
shapiro.test(model_training$residuals)
[1] 73

	Shapiro-Wilk normality test

data:  model_training$residuals
W = 0.97994, p-value = 0.005838

summary(data$incidents)
[1] 74
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08333 0.33333 0.41667 0.47500 0.58333 0.91667 
incidents_log <- log(data$incidents + (-1 * min(data$incidents) + 
    1))
[1] 75
model_training_log <- lm(incidents_log ~ training + empathy_center + 
    years_center, data = data)
[1] 76
summary(model_training_log)
[1] 77

Call:
lm(formula = incidents_log ~ training + empathy_center + years_center, 
    data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.27676 -0.08678 -0.00271  0.07438  0.35035 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.377249   0.011606  32.504  < 2e-16 ***
trainingTraining -0.111288   0.016497  -6.746 1.67e-10 ***
empathy_center    0.018550   0.003372   5.501 1.17e-07 ***
years_center     -0.010283   0.002769  -3.713 0.000266 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1155 on 196 degrees of freedom
Multiple R-squared:  0.2661,	Adjusted R-squared:  0.2549 
F-statistic: 23.69 on 3 and 196 DF,  p-value: 3.963e-13

confint(model_training_log)
[1] 78
                       2.5 %       97.5 %
(Intercept)       0.35436052  0.400138207
trainingTraining -0.14382282 -0.078753698
empathy_center    0.01189931  0.025200058
years_center     -0.01574440 -0.004822059
stdz_training_log <- MASS::studres(model_training_log)
[1] 79
which(abs(stdz_training_log) > 2)
[1] 80
  7  13  35  60  74 159 176 
  7  13  35  60  74 159 176 
hats_training_log <- hatvalues(model_training_log)
[1] 81
which(hats_training_log > 2 * mean(hats_training_log))
[1] 82
 61  88  89  92  94 137 141 154 163 175 200 
 61  88  89  92  94 137 141 154 163 175 200 
plot(cooks.distance(model_training_log), ylab = "Cook's Distance", 
    main = "Cook's Distance Plot")
[1] 83
abline(h = cooks.distance(model_training_log) > (4/(200 - 3 - 
    1)), col = "yellow")
[1] 84
which(cooks.distance(model_training_log) > (4/(200 - 3 - 1)))
[1] 85
 13  30  35  60  74  88  94 137 
 13  30  35  60  74  88  94 137 
CVRs_training_log <- covratio(model_training_log)
[1] 86
which(CVRs_training_log > 1.06 | CVRs_training_log < 0.94)
[1] 87
 13  61  74  88  89 141 154 155 163 175 176 200 
 13  61  74  88  89 141 154 155 163 175 176 200 
model_training_log2 <- lm(incidents_log ~ training + empathy_center + 
    years_center, subset = c(-13, -30, -35, -60, -74, -88, -94, 
    -137), data = data)
[1] 88
summary(model_training_log2)
[1] 89

Call:
lm(formula = incidents_log ~ training + empathy_center + years_center, 
    data = data, subset = c(-13, -30, -35, -60, -74, -88, -94, 
        -137))

Residuals:
      Min        1Q    Median        3Q       Max 
-0.214502 -0.081578 -0.001217  0.071777  0.256612 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.375543   0.010831  34.673  < 2e-16 ***
trainingTraining -0.105954   0.015681  -6.757 1.72e-10 ***
empathy_center    0.019441   0.003347   5.809 2.64e-08 ***
years_center     -0.012095   0.002751  -4.397 1.84e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1074 on 188 degrees of freedom
Multiple R-squared:  0.2893,	Adjusted R-squared:  0.2779 
F-statistic:  25.5 on 3 and 188 DF,  p-value: 6.89e-14

hist(model_training_log$residuals, xlab = "Residuals of the Model", 
    main = "Histogram of Model Residuals")
[1] 90
shapiro.test(model_training_log$residuals)
[1] 91

	Shapiro-Wilk normality test

data:  model_training_log$residuals
W = 0.9905, p-value = 0.2109

residualPlots(model_training_log, main = "Residual Plot")
[1] 92
ncvTest(model_training_log)
[1] 93
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 1.036059, Df = 1, p = 0.30874
durbinWatsonTest(model_training_log)
[1] 94
 lag Autocorrelation D-W Statistic p-value
   1       -0.116516      2.217742   0.124
 Alternative hypothesis: rho != 0
crPlots(model_training_log)
[1] 95
vif(model_training_log)
[1] 96
      training empathy_center   years_center 
      1.020633       1.092467       1.085089 
model_feedback <- lm(incidents ~ feedback + training + empathy_center + 
    years_center, data = data)
[1] 97
summary(model_feedback)
[1] 98

Call:
lm(formula = incidents ~ feedback + training + empathy_center + 
    years_center, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.31626 -0.10096 -0.00949  0.09165  0.46650 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.485055   0.018126  26.760  < 2e-16 ***
feedbackFeedback  0.133715   0.020968   6.377 1.28e-09 ***
trainingTraining -0.153824   0.021128  -7.281 7.98e-12 ***
empathy_center    0.027191   0.004328   6.283 2.12e-09 ***
years_center     -0.015199   0.003551  -4.281 2.92e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1479 on 195 degrees of freedom
Multiple R-squared:  0.3848,	Adjusted R-squared:  0.3721 
F-statistic: 30.49 on 4 and 195 DF,  p-value: < 2.2e-16

confint(model_feedback)
[1] 99
                       2.5 %       97.5 %
(Intercept)       0.44930678  0.520802583
feedbackFeedback  0.09236165  0.175067553
trainingTraining -0.19549291 -0.112155017
empathy_center    0.01865573  0.035726020
years_center     -0.02220122 -0.008196122
stdz_feedback <- MASS::studres(model_feedback)
[1] 100
which(abs(stdz_feedback) > 2)
[1] 101
 13  74 101 117 150 156 159 160 176 181 
 13  74 101 117 150 156 159 160 176 181 
hats_feedback <- hatvalues(model_feedback)
[1] 102
which(hats_feedback > 2 * mean(hats_feedback))
[1] 103
 61  88  89  92 137 141 154 175 200 
 61  88  89  92 137 141 154 175 200 
plot(cooks.distance(model_feedback), ylab = "Cook's Distance", 
    main = "Cook's Distance Plot")
[1] 104
abline(h = cooks.distance(model_feedback) > (4/(200 - 4 - 1)), 
    col = "yellow")
[1] 105
which(cooks.distance(model_feedback) > (4/(200 - 4 - 1)))
[1] 106
 13  35  74 117 131 137 141 156 160 176 
 13  35  74 117 131 137 141 156 160 176 
CVRs_feedback <- covratio(model_feedback)
[1] 107
1 + 3 * (4 + 1)/200
[1] 108
[1] 1.075
1 - 3 * (4 + 1)/200
[1] 109
[1] 0.925
which(CVRs_feedback > 1.08 | CVRs_feedback < 0.93)
[1] 110
 13  61  88  92 150 154 156 160 176 200 
 13  61  88  92 150 154 156 160 176 200 
model_feedback2 <- lm(incidents ~ feedback + training + empathy_center + 
    years_center, c(-13, -35, -74, -117, -131, -137, -141, -156, 
    -160, -176), data = data)
[1] 111
summary(model_feedback2)
[1] 112

Call:
lm(formula = incidents ~ feedback + training + empathy_center + 
    years_center, data = data, subset = c(-13, -35, -74, -117, 
    -131, -137, -141, -156, -160, -176))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30197 -0.09301 -0.00430  0.09141  0.36025 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.475794   0.016526  28.790  < 2e-16 ***
feedbackFeedback  0.140371   0.019014   7.383 5.12e-12 ***
trainingTraining -0.153419   0.019134  -8.018 1.19e-13 ***
empathy_center    0.028642   0.004022   7.121 2.30e-11 ***
years_center     -0.017718   0.003365  -5.266 3.85e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.131 on 185 degrees of freedom
Multiple R-squared:  0.4702,	Adjusted R-squared:  0.4588 
F-statistic: 41.05 on 4 and 185 DF,  p-value: < 2.2e-16

hist(model_feedback$residuals, xlab = "Residuals of the Model", 
    main = "Histogram of Model Residuals")
[1] 113
shapiro.test(model_feedback$residuals)
[1] 114

	Shapiro-Wilk normality test

data:  model_feedback$residuals
W = 0.98734, p-value = 0.07172

residualPlots(model_feedback, main = "Residual Plot")
[1] 115
ncvTest(model_feedback)
[1] 116
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 3.684027, Df = 1, p = 0.054936
durbinWatsonTest(model_feedback)
[1] 117
 lag Autocorrelation D-W Statistic p-value
   1      0.07303546      1.841386   0.274
 Alternative hypothesis: rho != 0
crPlots(model_feedback)
[1] 118
vif(model_feedback)
[1] 119
      feedback       training empathy_center   years_center 
      1.005320       1.020743       1.097109       1.087722 
model_interaction <- lm(incidents ~ feedback + training + empathy_center + 
    years_center + training * feedback, data = data)
[1] 120
summary(model_interaction)
[1] 121

Call:
lm(formula = incidents ~ feedback + training + empathy_center + 
    years_center + training * feedback, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30367 -0.10365 -0.00795  0.08806  0.44359 

Coefficients:
                                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.509636   0.020807  24.494  < 2e-16 ***
feedbackFeedback                   0.085558   0.029299   2.920  0.00391 ** 
trainingTraining                  -0.203496   0.029874  -6.812 1.18e-10 ***
empathy_center                     0.028695   0.004328   6.630 3.25e-10 ***
years_center                      -0.015720   0.003518  -4.468 1.34e-05 ***
feedbackFeedback:trainingTraining  0.097334   0.041840   2.326  0.02103 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1462 on 194 degrees of freedom
Multiple R-squared:  0.4015,	Adjusted R-squared:  0.386 
F-statistic: 26.02 on 5 and 194 DF,  p-value: < 2.2e-16

confint(model_interaction)
[1] 122
                                        2.5 %       97.5 %
(Intercept)                        0.46859906  0.550672482
feedbackFeedback                   0.02777246  0.143343544
trainingTraining                  -0.26241518 -0.144577748
empathy_center                     0.02015864  0.037231125
years_center                      -0.02265877 -0.008780863
feedbackFeedback:trainingTraining  0.01481491  0.179852742
stdz_interaction <- MASS::studres(model_interaction)
[1] 123
which(abs(stdz_interaction) > 2)
[1] 124
 13  73 117 150 156 159 160 176 181 
 13  73 117 150 156 159 160 176 181 
hats_interaction <- hatvalues(model_interaction)
[1] 125
which(hats_interaction > 2 * mean(hats_interaction))
[1] 126
 61  88  89 137 141 154 200 
 61  88  89 137 141 154 200 
plot(cooks.distance(model_interaction), ylab = "Cook's Distance", 
    main = "Cook's Distance Plot")
[1] 127
abline(h = cooks.distance(model_interaction) > (4/(200 - 5 - 
    1)), col = "yellow")
[1] 128
which(cooks.distance(model_interaction) > (4/(200 - 5 - 1)))
[1] 129
 13  73  74  89 117 137 160 165 176 
 13  73  74  89 117 137 160 165 176 
1 + 3 * (5 + 1)/200
[1] 130
[1] 1.09
1 - 3 * (5 + 1)/200
[1] 131
[1] 0.91
CVRs_interaction <- covratio(model_interaction)
[1] 132
which(CVRs_interaction > 1.1 | CVRs_interaction < 0.9)
[1] 133
 13  61  88 150 154 176 
 13  61  88 150 154 176 
model_interaction2 <- lm(incidents ~ feedback + training + empathy_center + 
    years_center + training * feedback, subset = c(-13, -73, 
    -74, -89, -117, -137, -160, -165, -176), data = data)
[1] 134
summary(model_interaction2)
[1] 135

Call:
lm(formula = incidents ~ feedback + training + empathy_center + 
    years_center + training * feedback, data = data, subset = c(-13, 
    -73, -74, -89, -117, -137, -160, -165, -176))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.28269 -0.09632  0.00081  0.09067  0.34167 

Coefficients:
                                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.494958   0.019144  25.855  < 2e-16 ***
feedbackFeedback                   0.094956   0.027103   3.504 0.000576 ***
trainingTraining                  -0.184679   0.027340  -6.755 1.80e-10 ***
empathy_center                     0.030839   0.004103   7.516 2.35e-12 ***
years_center                      -0.016135   0.003427  -4.708 4.89e-06 ***
feedbackFeedback:trainingTraining  0.084936   0.038572   2.202 0.028899 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1315 on 185 degrees of freedom
Multiple R-squared:  0.4496,	Adjusted R-squared:  0.4348 
F-statistic: 30.23 on 5 and 185 DF,  p-value: < 2.2e-16

hist(model_interaction$residuals, xlab = "Residuals of the Model", 
    main = "Histogram of Model Residuals")
[1] 136
shapiro.test(model_interaction$residuals)
[1] 137

	Shapiro-Wilk normality test

data:  model_interaction$residuals
W = 0.98794, p-value = 0.08817

residualPlots(model_interaction, main = "Residual Plot")
[1] 138
ncvTest(model_interaction)
[1] 139
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 4.398557, Df = 1, p = 0.035969
durbinWatsonTest(model_interaction)
[1] 140
 lag Autocorrelation D-W Statistic p-value
   1      0.09456969      1.801048   0.204
 Alternative hypothesis: rho != 0
vif(model_interaction)
[1] 141
         feedback          training    empathy_center      years_center feedback:training 
         2.007327          2.086826          1.122147          1.092150          3.070068 
cat_plot(model_interaction, pred = training, modx = feedback, 
    plot.points = TRUE, x.label = "Training", y.label = "Teachers' Intervention", 
    legend.main = "Feedback", main.title = "Interaction between Training and Feedback")
[1] 142
citation("psych")
[1] 143

To cite the psych package in publications use:

  Revelle, W. (2018) psych: Procedures for Personality and Psychological Research, Northwestern
  University, Evanston, Illinois, USA, https://CRAN.R-project.org/package=psych Version = 1.8.12.

A BibTeX entry for LaTeX users is

  @Manual{,
    title = {psych: Procedures for Psychological, Psychometric, and Personality Research},
    author = {William Revelle},
    organization = { Northwestern University},
    address = { Evanston, Illinois},
    year = {2018},
    note = {R package version 1.8.12},
    url = {https://CRAN.R-project.org/package=psych},
  }

citation("car")
[1] 144

To cite the car package in publications use:

  John Fox and Sanford Weisberg (2019). An {R} Companion to Applied Regression, Third Edition. Thousand
  Oaks CA: Sage. URL: https://socialsciences.mcmaster.ca/jfox/Books/Companion/

A BibTeX entry for LaTeX users is

  @Book{,
    title = {An {R} Companion to Applied Regression},
    edition = {Third},
    author = {John Fox and Sanford Weisberg},
    year = {2019},
    publisher = {Sage},
    address = {Thousand Oaks {CA}},
    url = {https://socialsciences.mcmaster.ca/jfox/Books/Companion/},
  }

citation("MASS")
[1] 145

To cite the MASS package in publications use:

  Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New
  York. ISBN 0-387-95457-0

A BibTeX entry for LaTeX users is

  @Book{,
    title = {Modern Applied Statistics with S},
    author = {W. N. Venables and B. D. Ripley},
    publisher = {Springer},
    edition = {Fourth},
    address = {New York},
    year = {2002},
    note = {ISBN 0-387-95457-0},
    url = {http://www.stats.ox.ac.uk/pub/MASS4},
  }

citation("DescTools")
[1] 146

To cite package 'DescTools' in publications use:

  Andri Signorell et mult. al. (2019). DescTools: Tools for descriptive statistics. R package version
  0.99.30.

A BibTeX entry for LaTeX users is

  @Manual{,
    title = {DescTools: Tools for Descriptive Statistics},
    author = {Andri Signorell et mult. al.},
    year = {2019},
    note = {R package version 0.99.30},
    url = {https://cran.r-project.org/package=DescTools},
  }

citation("interactions")
[1] 147

Long JA (2019). _interactions: Comprehensive, User-Friendly Toolkit for Probing Interactions_. R package
version 1.1.0, <URL: https://cran.r-project.org/package=interactions>.

A BibTeX entry for LaTeX users is

  @Manual{interactions,
    title = {interactions: Comprehensive, User-Friendly Toolkit for Probing Interactions},
    author = {Jacob A. Long},
    year = {2019},
    note = {R package version 1.1.0},
    url = {https://cran.r-project.org/package=interactions},
  }

