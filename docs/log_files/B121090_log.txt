

	####### B121090 R script #######

data1 <- read.csv("~/Desktop/rms2/code_check/RMS2_report_1920.csv")
[1] 1
library(psych, quietly = T)
[1] 2
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(car, quietly = T)
[1] 3
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[11] "base"     
library(lm.beta, quietly = T)
[1] 4
 [1] "lm.beta"   "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[11] "methods"   "base"     
describeBy(data1$incidents, group = list(data1$training, data1$feedback), 
    mat = T)
[1] 5
    item group1 group2 vars  n      mean        sd    median   trimmed     mad        min       max     range       skew
X11    1      1      1    1 50 0.4933333 0.1673794 0.4166667 0.4770833 0.12355 0.25000000 0.9166667 0.6666667 0.81813067
X12    2      2      1    1 50 0.3333333 0.1398493 0.3333333 0.3270833 0.12355 0.08333333 0.5833333 0.5000000 0.15233791
X13    3      1      2    1 50 0.5900000 0.1707327 0.5833333 0.5875000 0.12355 0.25000000 0.9166667 0.6666667 0.04793352
      kurtosis         se
X11 -0.0698501 0.02367103
X12 -0.9575614 0.01977768
X13 -0.8075298 0.02414525
 [ reached 'max' / getOption("max.print") -- omitted 1 rows ]
describe(data1)
[1] 6
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training     3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
str(data1$training)
[1] 7
 int [1:200] 2 2 2 2 2 2 2 2 2 2 ...
NULL
str(data1$feedback)
[1] 8
 int [1:200] 2 1 2 1 2 1 2 1 2 1 ...
NULL
data1$training <- as.factor(data1$training)
[1] 9
class(data1$training)
[1] 10
[1] "factor"
data1$feedback <- as.factor(data1$feedback)
[1] 11
class(data1$feedback)
[1] 12
[1] "factor"
scatterplot(data1$empathy, data1$years, main = "Relationship between empathy and years of experience", 
    ylab = "empathy ratio", xlab = "years")
[1] 13
plot(data1$empathy, data1$years, main = "Relationship between empathy and years of experience", 
    ylab = "empathy ratio", xlab = "years")
[1] 14
pairs.panels(data1[, 5:6])
[1] 15
NULL
hist(data1$empathy, main = "Histogram of empathy")
[1] 16
qqPlot(data1$empathy, main = "QQ-plot of empathy")
[1] 17
shapiro.test(data1$empathy)
[1] 18

	Shapiro-Wilk normality test

data:  data1$empathy
W = 0.97073, p-value = 0.0003477

hist(data1$years, main = "Histogram of years of experience")
[1] 19
qqPlot(data1$years, main = "QQ-plot of years of experience")
[1] 20
shapiro.test(data1$years)
[1] 21

	Shapiro-Wilk normality test

data:  data1$years
W = 0.97276, p-value = 0.0006279

var.test(data1$empathy, data1$years)
[1] 22

	F test to compare two variances

data:  data1$empathy and data1$years
F = 0.67893, num df = 199, denom df = 199, p-value = 0.006538
alternative hypothesis: true ratio of variances is not equal to 1
95 percent confidence interval:
 0.5138019 0.8971166
sample estimates:
ratio of variances 
         0.6789258 

cor(data1$empathy, data1$years, method = "spearm")
[1] 23
[1] 0.3071045
cor.test(scale(data1$empathy), data1$years, method = "spearm", 
    exact = FALSE)
[1] 24

	Spearman's rank correlation rho

data:  scale(data1$empathy) and data1$years
S = 923838, p-value = 9.723e-06
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.3071045 

m3 <- lm(data = data1, incidents ~ feedback + scale(empathy) + 
    years)
[1] 25
summary(m3)
[1] 26

Call:
lm(formula = incidents ~ feedback + scale(empathy) + years, data = data1)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35580 -0.11148 -0.01097  0.11092  0.53552 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.512093   0.035846  14.286  < 2e-16 ***
feedback2       0.132132   0.023585   5.602 7.09e-08 ***
scale(empathy)  0.058827   0.012251   4.802 3.11e-06 ***
years          -0.012689   0.003975  -3.192  0.00165 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1663 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

hist(m3$residuals, main = "Histogram of residuals")
[1] 27
shapiro.test(m3$residuals)
[1] 28

	Shapiro-Wilk normality test

data:  m3$residuals
W = 0.98772, p-value = 0.08174

qqPlot(m3$residuals, main = "QQ-plot of residuals")
[1] 29
residualPlots(m3, main = "residual-vs-predicted values plot")
[1] 30
ncvTest(m3)
[1] 31
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.01731234, Df = 1, p = 0.89532
crPlots(m3)
[1] 32
durbinWatsonTest(m3)
[1] 33
 lag Autocorrelation D-W Statistic p-value
   1       0.2738264      1.444081       0
 Alternative hypothesis: rho != 0
vif(m3)
[1] 34
      feedback scale(empathy)          years 
      1.005211       1.079397       1.077467 
plot(cooks.distance(m3))
[1] 35
abline(h = cooks.distance(m3) > (4/(200 - 3 - 1)), col = "red")
[1] 36
names(which(cooks.distance(m3) > (4/(200 - 3 - 1))))
[1] 37
[1] "73"  "74"  "89"  "137" "156" "160" "165" "176"
plot(m3, which = 4)
[1] 38
m3a <- lm(data = data1[-c(74, 89, 137), ], incidents ~ feedback + 
    scale(empathy) + years)
[1] 39
summary(m3a)
[1] 40

Call:
lm(formula = incidents ~ feedback + scale(empathy) + years, data = data1[-c(74, 
    89, 137), ])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.34740 -0.11326 -0.01073  0.11335  0.53413 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.523166   0.037024  14.130  < 2e-16 ***
feedback2       0.128900   0.023282   5.537 9.99e-08 ***
scale(empathy)  0.062160   0.012362   5.028 1.13e-06 ***
years          -0.013583   0.004147  -3.276  0.00125 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1633 on 193 degrees of freedom
Multiple R-squared:  0.2282,	Adjusted R-squared:  0.2162 
F-statistic: 19.02 on 3 and 193 DF,  p-value: 7.541e-11

m3_coef <- summary(m3)
[1] 41
m3a_coef <- summary(m3a)
[1] 42
compare3 <- data.frame(round(m3_coef$coefficients[, 1], 3), round(m3_coef$coefficients[, 
    4], 4), round(m3a_coef$coefficients[, 1], 3), round(m3a_coef$coefficients[, 
    4], 4))
[1] 43
colnames(compare3) <- c("M3 Coef", "M3 p-value", "M3a Coef", 
    "M3 p-value")
[1] 44
compare3

               M3 Coef M3 p-value M3a Coef M3 p-value
(Intercept)      0.512     0.0000    0.523     0.0000
feedback2        0.132     0.0000    0.129     0.0000
scale(empathy)   0.059     0.0000    0.062     0.0000
years           -0.013     0.0016   -0.014     0.0013


m2 <- lm(data = data1, incidents ~ training + scale(empathy) + 
    years)
[1] 46
summary(m2)
[1] 47

Call:
lm(formula = incidents ~ training + scale(empathy) + years, data = data1)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.665721   0.036551  18.214  < 2e-16 ***
training2      -0.152427   0.023166  -6.580 4.20e-10 ***
scale(empathy)  0.064430   0.012014   5.363 2.29e-07 ***
years          -0.014085   0.003889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

hist(m2$residuals, main = "Histogram of residuals")
[1] 48
shapiro.test(m2$residuals)
[1] 49

	Shapiro-Wilk normality test

data:  m2$residuals
W = 0.97994, p-value = 0.005838

qqPlot(m2$residuals, main = "QQ-plot of residuals")
[1] 50
residualPlots(m2, main = "residual-vs-predicted values plot")
[1] 51
ncvTest(m2)
[1] 52
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
crPlots(m2)
[1] 53
durbinWatsonTest(m2)
[1] 54
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.146
 Alternative hypothesis: rho != 0
vif(m2)
[1] 55
      training scale(empathy)          years 
      1.020633       1.092467       1.085089 
data1$incidents <- 1 + data1$incidents
[1] 56
data1$incidents <- log(data1$incidents)
[1] 57
m2 <- lm(data = data1, incidents ~ training + scale(empathy) + 
    years)
[1] 58
summary(m2)
[1] 59

Call:
lm(formula = incidents ~ training + scale(empathy) + years, data = data1)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25974 -0.08206 -0.00301  0.07026  0.33183 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.511876   0.024544  20.856  < 2e-16 ***
training2      -0.104810   0.015556  -6.737 1.75e-10 ***
scale(empathy)  0.044322   0.008067   5.494 1.21e-07 ***
years          -0.009683   0.002611  -3.708 0.000272 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1089 on 196 degrees of freedom
Multiple R-squared:  0.2657,	Adjusted R-squared:  0.2544 
F-statistic: 23.63 on 3 and 196 DF,  p-value: 4.229e-13

hist(m2$residuals, main = "Histogram of residuals")
[1] 60
shapiro.test(m2$residuals)
[1] 61

	Shapiro-Wilk normality test

data:  m2$residuals
W = 0.9901, p-value = 0.1845

qqPlot(m2$residuals, main = "QQ-plot of residuals")
[1] 62
residualPlots(m2, main = "residual-vs-predicted values plot")
[1] 63
ncvTest(m2)
[1] 64
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.8932792, Df = 1, p = 0.34459
crPlots(m2)
[1] 65
durbinWatsonTest(m2)
[1] 66
 lag Autocorrelation D-W Statistic p-value
   1      -0.1160146      2.216759   0.116
 Alternative hypothesis: rho != 0
vif(m2)
[1] 67
      training scale(empathy)          years 
      1.020633       1.092467       1.085089 
plot(cooks.distance(m2))
[1] 68
abline(h = cooks.distance(m2) > (4/(200 - 3 - 1)), col = "red")
[1] 69
names(which(cooks.distance(m2) > (4/(200 - 3 - 1))))
[1] 70
[1] "13"  "30"  "35"  "60"  "74"  "88"  "94"  "137"
plot(m2, which = 4)
[1] 71
m2a <- lm(data = data1[-c(35, 74, 137), ], incidents ~ training + 
    scale(empathy) + years)
[1] 72
summary(m2a)
[1] 73

Call:
lm(formula = incidents ~ training + scale(empathy) + years, data = data1[-c(35, 
    74, 137), ])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.21361 -0.07971 -0.00072  0.07005  0.33221 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.528282   0.024420  21.633  < 2e-16 ***
training2      -0.104122   0.015173  -6.862 8.98e-11 ***
scale(empathy)  0.048637   0.007952   6.117 5.21e-09 ***
years          -0.011904   0.002646  -4.500 1.18e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1057 on 193 degrees of freedom
Multiple R-squared:  0.2988,	Adjusted R-squared:  0.2879 
F-statistic: 27.42 on 3 and 193 DF,  p-value: 8.169e-15

m2_coef <- summary(m2)
[1] 74
m2a_coef <- summary(m2a)
[1] 75
compare2 <- data.frame(round(m2_coef$coefficients[, 1], 3), round(m2_coef$coefficients[, 
    4], 4), round(m2a_coef$coefficients[, 1], 3), round(m2a_coef$coefficients[, 
    4], 4))
[1] 76
colnames(compare2) <- c("M2 Coef", "M2 p-value", "M2a Coef", 
    "M2 p-value")
[1] 77
compare2

               M2 Coef M2 p-value M2a Coef M2 p-value
(Intercept)      0.512      0e+00    0.528          0
training2       -0.105      0e+00   -0.104          0
scale(empathy)   0.044      0e+00    0.049          0
years           -0.010      3e-04   -0.012          0


AIC(m2, m3)
[1] 79
   df       AIC
m2  5 -313.4627
m3  5 -143.9591
BIC(m2, m3)
[1] 80
   df       BIC
m2  5 -296.9711
m3  5 -127.4675
contrasts(data1$training) <- contr.sum(2)
[1] 81
contrasts(data1$training)
[1] 82
  [,1]
1    1
2   -1
contrasts(data1$feedback) <- contr.sum(2)
[1] 83
contrasts(data1$feedback)
[1] 84
  [,1]
1    1
2   -1
m4 <- lm(data = data1, incidents ~ training + feedback + training:feedback)
[1] 85
summary(m4)
[1] 86

Call:
lm(formula = incidents ~ training + feedback + training:feedback, 
    data = data1)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.307482 -0.069320  0.005393  0.066018  0.263063 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)          0.380746   0.007773  48.986  < 2e-16 ***
training1            0.045839   0.007773   5.897 1.59e-08 ***
feedback1           -0.042032   0.007773  -5.408 1.85e-07 ***
training1:feedback1  0.010586   0.007773   1.362    0.175    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1099 on 196 degrees of freedom
Multiple R-squared:  0.2516,	Adjusted R-squared:  0.2401 
F-statistic: 21.96 on 3 and 196 DF,  p-value: 2.653e-12

hist(m4$residuals, main = "Histogram of residuals")
[1] 87
shapiro.test(m4$residuals)
[1] 88

	Shapiro-Wilk normality test

data:  m4$residuals
W = 0.98615, p-value = 0.04751

qqPlot(m4$residuals, main = "QQ-plot of residuals")
[1] 89
residualPlots(m4, main = "residual-vs-predicted values plot")
[1] 90
ncvTest(m4)
[1] 91
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.07388189, Df = 1, p = 0.78577
durbinWatsonTest(m4)
[1] 92
 lag Autocorrelation D-W Statistic p-value
   1       0.1133076       1.76692   0.128
 Alternative hypothesis: rho != 0
plot(cooks.distance(m4))
[1] 93
abline(h = cooks.distance(m4) > (4/(200 - 2 - 1)), col = "red")
[1] 94
names(which(cooks.distance(m4) > (4/(200 - 2 - 1))))
[1] 95
[1] "7"   "13"  "17"  "21"  "89"  "131" "160" "176"
plot(m4, which = 4)
[1] 96
m4a <- lm(data = data1[-c(13, 89, 160), ], incidents ~ training + 
    feedback + training:feedback)
[1] 97
summary(m4a)
[1] 98

Call:
lm(formula = incidents ~ training + feedback + training:feedback, 
    data = data1[-c(13, 89, 160), ])

Residuals:
      Min        1Q    Median        3Q       Max 
-0.234887 -0.059146  0.005393  0.066018  0.260663 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)          0.379674   0.007500  50.626  < 2e-16 ***
training1            0.044304   0.007500   5.908 1.54e-08 ***
feedback1           -0.043567   0.007500  -5.809 2.55e-08 ***
training1:feedback1  0.009514   0.007500   1.269    0.206    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1052 on 193 degrees of freedom
Multiple R-squared:  0.2698,	Adjusted R-squared:  0.2585 
F-statistic: 23.77 on 3 and 193 DF,  p-value: 3.875e-13

m4_coef <- summary(m4)
[1] 99
m4a_coef <- summary(m4a)
[1] 100
compare4 <- data.frame(round(m4_coef$coefficients[, 1], 3), round(m4_coef$coefficients[, 
    4], 4), round(m4a_coef$coefficients[, 1], 3), round(m4a_coef$coefficients[, 
    4], 4))
[1] 101
colnames(compare4) <- c("M4 Coef", "M4 p-value", "M4a Coef", 
    "M4 p-value")
[1] 102
compare4

                    M4 Coef M4 p-value M4a Coef M4 p-value
(Intercept)           0.381     0.0000    0.380     0.0000
training1             0.046     0.0000    0.044     0.0000
feedback1            -0.042     0.0000   -0.044     0.0000
training1:feedback1   0.011     0.1748    0.010     0.2061


a1 <- aov(data1$incidents ~ data1$training * data1$feedback)
[1] 104
summary(a1)
[1] 105
                               Df Sum Sq Mean Sq F value   Pr(>F)    
data1$training                  1 0.4202  0.4202  34.780 1.59e-08 ***
data1$feedback                  1 0.3533  0.3533  29.244 1.85e-07 ***
data1$training:data1$feedback   1 0.0224  0.0224   1.855    0.175    
Residuals                     196 2.3682  0.0121                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
interaction.plot(data1$training, data1$feedback, data1$incidents, 
    main = "The interaction effect between training and feedback")
[1] 106
