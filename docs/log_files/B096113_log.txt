

	####### B096113 R script #######

library(psych, quietly = T)
[1] 1
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(car, quietly = T)
[1] 2
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[10] "methods"   "base"     
library(carData, quietly = T)
[1] 3
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[10] "methods"   "base"     
library(lm.beta, quietly = T)
[1] 4
 [1] "lm.beta"   "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"    
[10] "datasets"  "methods"   "base"     
library(interactions, quietly = T)
[1] 5
 [1] "interactions" "lm.beta"      "car"          "carData"      "psych"        "readr"        "stats"       
 [8] "graphics"     "grDevices"    "utils"        "datasets"     "methods"      "base"        
RMS2_report_1920 <- read.csv("../RMS2_report_1920.csv")
[1] 6
dframe <- RMS2_report_1920
[1] 7
str(dframe)
[1] 8
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : int  2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : int  2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
dframe$feedback <- as.factor(dframe$feedback)
[1] 9
dframe$training <- as.factor(dframe$training)
[1] 10
dframe$subject <- as.factor(dframe$subject)
[1] 11
dframe$feedback <- factor(dframe$feedback, labels = c("no feedback", 
    "feedback"))
[1] 12
dframe$training <- factor(dframe$training, labels = c("no training", 
    "training"))
[1] 13
contrasts(dframe$training)
[1] 14
            training
no training        0
training           1
contrasts(dframe$feedback)
[1] 15
            feedback
no feedback        0
feedback           1
str(dframe)
[1] 16
'data.frame':	200 obs. of  6 variables:
 $ subject  : Factor w/ 200 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : Factor w/ 2 levels "no training",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : Factor w/ 2 levels "no feedback",..: 2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
describe(dframe)
[1] 17
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject*     1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training*    3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
summary(dframe)
[1] 18
    subject      incidents              training          feedback      empathy          years      
 1      :  1   Min.   :0.08333   no training:100   no feedback:100   Min.   : 3.00   Min.   : 1.00  
 2      :  1   1st Qu.:0.33333   training   :100   feedback   :100   1st Qu.: 9.00   1st Qu.: 6.00  
 3      :  1   Median :0.41667                                       Median :10.00   Median : 8.00  
 4      :  1   Mean   :0.47500                                       Mean   :10.18   Mean   : 8.13  
 5      :  1   3rd Qu.:0.58333                                       3rd Qu.:12.00   3rd Qu.:10.00  
 6      :  1   Max.   :0.91667                                       Max.   :16.00   Max.   :19.00  
 (Other):194                                                                                        
hist_I <- hist(dframe$incidents, breaks = 11, main = "Histogram of the variable Incidents", 
    xlab = "Percentage of Reported Incidents", col = "lightblue")
[1] 19
hist_E <- hist(dframe$empathy, breaks = 16, main = "Histogram of the variable Empathy", 
    xlab = "Empathy Measure", col = "lightgreen")
[1] 20
hist_Y <- hist(dframe$years, breaks = 10, main = "Histogram of the variable Years (Experience)", 
    xlab = "Teaching Experience in Years", col = "pink")
[1] 21
table(dframe$incidents)
[1] 22

0.083333333 0.166666667        0.25 0.333333333 0.416666667         0.5 0.583333333 0.666666667        0.75 
          3          12          14          31          45          28          24          19          12 
0.833333333 0.916666667 
          6           6 
table(dframe$empathy)
[1] 23

 3  5  6  7  8  9 10 11 12 13 14 15 16 
 4  5  5 20  9 26 38 31 33 11 10  6  2 
table(dframe$years)
[1] 24

 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 18 19 
 2  3  3  9 21 24 34 21 21 23 11 11  5  6  4  1  1 
lm_1 <- lm(empathy ~ years, data = dframe)
[1] 25
summary(lm_1)
[1] 26

Call:
lm(formula = empathy ~ years, data = dframe)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.5399 -1.5160  0.0708  1.6357  5.8533 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  8.40625    0.49085  17.126  < 2e-16 ***
years        0.21756    0.05648   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.453 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

summary(lm_1)$r.squared
[1] 27
[1] 0.0697155
cor(dframe$empathy, dframe$years)^2
[1] 28
[1] 0.0697155
round(summary(lm_1)$r.squared, 3) == round(cor(dframe$empathy, 
    dframe$years)^2, 3)
[1] 29
[1] TRUE
plot(dframe$years, dframe$empathy, main = "Relationship between Empathy and Experience", 
    ylab = "Empathy", xlab = "Experience (years)", pch = 19)
[1] 30
abline(lm_1, col = "blue")
[1] 31
qqPlot(lm_1, main = "Normality of residuals (lm_1)")
[1] 32
hist(lm_1$residuals, main = "Normality of residuals (lm_1)", 
    col = "blue")
[1] 33
shapiro.test(lm_1$residuals)
[1] 34

	Shapiro-Wilk normality test

data:  lm_1$residuals
W = 0.98911, p-value = 0.1321

residualPlots(lm_1, main = "Plot of fitted values against residuals for lm_1")
[1] 35
ncvTest(lm_1)
[1] 36
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.9815033, Df = 1, p = 0.32183
plot((dframe$empathy ~ dframe$years), main = "Relation between empathy and experience", 
    xlab = "Experience (years)", ylab = "Empathy")
[1] 37
abline(lm(empathy ~ years, data = dframe), col = "blue")
[1] 38
lines(lowess(dframe$years, dframe$empathy))
[1] 39
durbinWatsonTest(lm_1)
[1] 40
 lag Autocorrelation D-W Statistic p-value
   1       0.1017162      1.769879   0.064
 Alternative hypothesis: rho != 0
lm_2 <- lm(incidents ~ training + empathy + years, data = dframe)
[1] 41
summary(lm_2)
[1] 42

Call:
lm(formula = incidents ~ training + empathy + years, data = dframe)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.407320   0.051814   7.861 2.48e-13 ***
trainingtraining -0.152427   0.023166  -6.580 4.20e-10 ***
empathy           0.025396   0.004735   5.363 2.29e-07 ***
years            -0.014085   0.003889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

par(mfrow = c(1, 3))
[1] 43
$mfrow
[1] 1 1

plot(dframe$training, dframe$incidents, main = "", xlab = "Training", 
    ylab = "Incidents")
[1] 44
abline(lm(dframe$incidents ~ dframe$training), col = "darkblue")
[1] 45
plot(dframe$empathy, dframe$incidents, main = "", xlab = "Empathy", 
    ylab = "Incidents")
[1] 46
abline(lm(dframe$incidents ~ dframe$empathy), col = "darkgreen")
[1] 47
title("Relationship of each predictor variable with the variable Incidents", 
    line = -2, outer = TRUE)
[1] 48
NULL
plot(dframe$years, dframe$incidents, main = "", xlab = "Experience", 
    ylab = "Incidents")
[1] 49
abline(lm(dframe$incidents ~ dframe$years), col = "darkred")
[1] 50
summary(lm.beta(lm_2))
[1] 51

Call:
lm(formula = incidents ~ training + empathy + years, data = dframe)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                  Estimate Standardized Std. Error t value Pr(>|t|)    
(Intercept)       0.407320     0.000000   0.051814   7.861 2.48e-13 ***
trainingtraining -0.152427    -0.409414   0.023166  -6.580 4.20e-10 ***
empathy           0.025396     0.345246   0.004735   5.363 2.29e-07 ***
years            -0.014085    -0.232381   0.003889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

confint(lm_2)
[1] 52
                       2.5 %       97.5 %
(Intercept)       0.30513599  0.509504162
trainingtraining -0.19811433 -0.106739356
empathy           0.01605669  0.034734598
years            -0.02175353 -0.006415556
qqPlot(lm_2, main = "Normality of residuals (lm_2)", col = "orange")
[1] 53
hist(lm_2$residuals, main = "Normality of residuals (lm_2)", 
    col = "orange")
[1] 54
shapiro.test(lm_2$residuals)
[1] 55

	Shapiro-Wilk normality test

data:  lm_2$residuals
W = 0.97994, p-value = 0.005838

residualPlots(lm_2, main = "Residuals vs. predicted values")
[1] 56
ncvTest(lm_2)
[1] 57
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
crPlots(lm_2)
[1] 58
durbinWatsonTest(lm_2)
[1] 59
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.186
 Alternative hypothesis: rho != 0
vif(lm_2)
[1] 60
training  empathy    years 
1.020633 1.092467 1.085089 
lm_3 <- lm(incidents ~ feedback + empathy + years, data = dframe)
[1] 61
summary(lm_3)
[1] 62

Call:
lm(formula = incidents ~ feedback + empathy + years, data = dframe)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35580 -0.11148 -0.01097  0.11092  0.53552 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.276161   0.054108   5.104 7.84e-07 ***
feedbackfeedback  0.132132   0.023585   5.602 7.09e-08 ***
empathy           0.023187   0.004829   4.802 3.11e-06 ***
years            -0.012689   0.003975  -3.192  0.00165 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1663 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

par(mfrow = c(1, 3))
[1] 63
$mfrow
[1] 1 3

plot(dframe$feedback, dframe$incidents, main = "", xlab = "Feedback", 
    ylab = "Incidents")
[1] 64
abline(lm(dframe$incidents ~ dframe$feedback), col = "darkblue")
[1] 65
plot(dframe$empathy, dframe$incidents, main = "", xlab = "Empathy", 
    ylab = "Incidents")
[1] 66
abline(lm(dframe$incidents ~ dframe$empathy), col = "darkgreen")
[1] 67
title("Relationship of each predictor variable with the variable Incidents", 
    line = -2, outer = TRUE)
[1] 68
NULL
plot(dframe$years, dframe$incidents, main = "", xlab = "Experience", 
    ylab = "Incidents")
[1] 69
abline(lm(dframe$incidents ~ dframe$years), col = "darkred")
[1] 70
summary(lm.beta(lm_3))
[1] 71

Call:
lm(formula = incidents ~ feedback + empathy + years, data = dframe)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35580 -0.11148 -0.01097  0.11092  0.53552 

Coefficients:
                  Estimate Standardized Std. Error t value Pr(>|t|)    
(Intercept)       0.276161     0.000000   0.054108   5.104 7.84e-07 ***
feedbackfeedback  0.132132     0.354902   0.023585   5.602 7.09e-08 ***
empathy           0.023187     0.315225   0.004829   4.802 3.11e-06 ***
years            -0.012689    -0.209350   0.003975  -3.192  0.00165 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1663 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

confint(lm_3)
[1] 72
                       2.5 %       97.5 %
(Intercept)       0.16945259  0.382869109
feedbackfeedback  0.08561885  0.178644455
empathy           0.01366456  0.032710219
years            -0.02052811 -0.004849104
qqPlot(lm_3, main = "Normality of residuals (lm_3)", col = "green")
[1] 73
hist(lm_3$residuals, main = "Normality of residuals (lm_3)", 
    col = "green")
[1] 74
shapiro.test(lm_3$residuals)
[1] 75

	Shapiro-Wilk normality test

data:  lm_3$residuals
W = 0.98772, p-value = 0.08174

residualPlots(lm_3, main = "Residuals vs. predicted values")
[1] 76
ncvTest(lm_3)
[1] 77
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.01731234, Df = 1, p = 0.89532
crPlots(lm_3)
[1] 78
durbinWatsonTest(lm_3)
[1] 79
 lag Autocorrelation D-W Statistic p-value
   1       0.2738264      1.444081       0
 Alternative hypothesis: rho != 0
vif(lm_3)
[1] 80
feedback  empathy    years 
1.005211 1.079397 1.077467 
AIC(lm_2, lm_3)
[1] 81
     df       AIC
lm_2  5 -154.1654
lm_3  5 -143.9591
BIC(lm_2, lm_3)
[1] 82
     df       BIC
lm_2  5 -137.6738
lm_3  5 -127.4675
int <- lm(incidents ~ training + feedback + training * feedback, 
    data = dframe)
[1] 83
summary(int)
[1] 84

Call:
lm(formula = incidents ~ training + feedback + training * feedback, 
    data = dframe)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40000 -0.10500  0.00000  0.08333  0.43333 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.49333    0.02314  21.323  < 2e-16 ***
trainingtraining                  -0.16000    0.03272  -4.890 2.09e-06 ***
feedbackfeedback                   0.09667    0.03272   2.954  0.00352 ** 
trainingtraining:feedbackfeedback  0.05333    0.04627   1.153  0.25047    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1636 on 196 degrees of freedom
Multiple R-squared:  0.2431,	Adjusted R-squared:  0.2315 
F-statistic: 20.99 on 3 and 196 DF,  p-value: 7.827e-12

cat_plot(int, pred = training, modx = feedback, plot.points = TRUE, 
    x.label = "Training", y.label = "Feedback", legend.main = "Interaction between Training and Feedback")
[1] 85
