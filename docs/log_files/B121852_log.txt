

	####### B121852 R script #######

library(psych, quietly = T)
[1] 1
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(car, quietly = T)
[1] 2
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[10] "methods"   "base"     
library(interactions, quietly = T)
[1] 3
 [1] "interactions" "car"          "carData"      "psych"        "readr"        "stats"        "graphics"    
 [8] "grDevices"    "utils"        "datasets"     "methods"      "base"        
df <- read.csv("../RMS2_report_1920.csv")
[1] 4
str(df)
[1] 5
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : int  2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : int  2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
df$subject <- as.factor(df$subject)
[1] 6
df$training <- factor(df$training, labels = c("No training", 
    "Training"))
[1] 7
df$feedback <- factor(df$feedback, labels = c("No feedback", 
    "Feedback"))
[1] 8
qqplot(df$empathy, df$years)
[1] 9
scatter.smooth(x = df$empathy, y = df$years, main = "Empathy ~ Years")
[1] 10
NULL
plot(df$empathy, df$years)
[1] 11
cor.test(df$years, df$empathy)
[1] 12

	Pearson's product-moment correlation

data:  df$years and df$empathy
t = 3.852, df = 198, p-value = 0.0001582
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.1300604 0.3885445
sample estimates:
      cor 
0.2640369 

cor.test(df$empathy, df$years, method = "spearman")
[1] 13

	Spearman's rank correlation rho

data:  df$empathy and df$years
S = 923838, p-value = 9.723e-06
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.3071045 

model_Q2 <- lm(incidents ~ training + empathy + years, data = df)
[1] 14
summary(model_Q2)
[1] 15

Call:
lm(formula = incidents ~ training + empathy + years, data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.407320   0.051814   7.861 2.48e-13 ***
trainingTraining -0.152427   0.023166  -6.580 4.20e-10 ***
empathy           0.025396   0.004735   5.363 2.29e-07 ***
years            -0.014085   0.003889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

plot(cooks.distance(model_Q2))
[1] 16
abline(h = cooks.distance(model_Q2) > (4/(200 - 3 - 1)), col = "red")
[1] 17
which(cooks.distance(model_Q2) > (4/(200 - 3 - 1)))
[1] 18
 13  35  74 137 165 
 13  35  74 137 165 
plot(model_Q2, which = 5)
[1] 19
stdz_Q2 <- MASS::studres(model_Q2)
[1] 20
which(abs(stdz_Q2) > 2)
[1] 21
  7  13  35  74 159 165 176 181 
  7  13  35  74 159 165 176 181 
hats_Q2 <- hatvalues(model_Q2)
[1] 22
which(hats_Q2 > 2 * mean(hats_Q2))
[1] 23
 61  88  89  92  94 137 141 154 163 175 200 
 61  88  89  92  94 137 141 154 163 175 200 
cleared_Q2 <- lm(df$incidents ~ df$training + df$empathy + df$years, 
    subset = -c(13, 35, 74, 137, 165, 7, 159, 165, 176, 181, 
        61, 88, 89, 92, 94, 141, 154, 163, 175, 200))
[1] 24
summary(cleared_Q2)
[1] 25

Call:
lm(formula = df$incidents ~ df$training + df$empathy + df$years, 
    subset = -c(13, 35, 74, 137, 165, 7, 159, 165, 176, 181, 
        61, 88, 89, 92, 94, 141, 154, 163, 175, 200))

Residuals:
      Min        1Q    Median        3Q       Max 
-0.281862 -0.114010  0.000087  0.099588  0.312674 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)          0.393626   0.054150   7.269 1.12e-11 ***
df$trainingTraining -0.140869   0.021949  -6.418 1.22e-09 ***
df$empathy           0.026285   0.005178   5.077 9.67e-07 ***
df$years            -0.015656   0.004432  -3.532 0.000525 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1455 on 177 degrees of freedom
Multiple R-squared:  0.2546,	Adjusted R-squared:  0.2419 
F-statistic: 20.15 on 3 and 177 DF,  p-value: 2.786e-11

qqPlot(model_Q2$residuals)
[1] 26
qqPlot(cleared_Q2$residuals)
[1] 27
hist(model_Q2$residuals)
[1] 28
hist(cleared_Q2$residuals)
[1] 29
shapiro.test(model_Q2$residuals)
[1] 30

	Shapiro-Wilk normality test

data:  model_Q2$residuals
W = 0.97994, p-value = 0.005838

shapiro.test(cleared_Q2$residuals)
[1] 31

	Shapiro-Wilk normality test

data:  cleared_Q2$residuals
W = 0.97978, p-value = 0.009933

log_trans_Q2 <- log(model_Q2$residuals + (-1 * min(model_Q2$residuals)) + 
    1)
[1] 32
shapiro.test(log_trans_Q2)
[1] 33

	Shapiro-Wilk normality test

data:  log_trans_Q2
W = 0.99159, p-value = 0.3006

log_trans_cleared_Q2 <- log(cleared_Q2$residuals + (-1 * min(cleared_Q2$residuals)) + 
    1)
[1] 34
shapiro.test(log_trans_cleared_Q2)
[1] 35

	Shapiro-Wilk normality test

data:  log_trans_cleared_Q2
W = 0.9837, p-value = 0.03324

residualPlots(model_Q2)
[1] 36
residualPlots(cleared_Q2)
[1] 37
ncvTest(model_Q2)
[1] 38
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
ncvTest(cleared_Q2)
[1] 39
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.02245673, Df = 1, p = 0.88088
crPlots(model_Q2)
[1] 40
crPlots(cleared_Q2)
[1] 41
durbinWatsonTest(model_Q2)
[1] 42
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.168
 Alternative hypothesis: rho != 0
durbinWatsonTest(cleared_Q2)
[1] 43
 lag Autocorrelation D-W Statistic p-value
   1     -0.07044206      2.116284   0.496
 Alternative hypothesis: rho != 0
vif(model_Q2)
[1] 44
training  empathy    years 
1.020633 1.092467 1.085089 
vif(cleared_Q2)
[1] 45
df$training  df$empathy    df$years 
   1.029576    1.165141    1.161485 
model_Q3 <- lm(incidents ~ feedback + empathy + years, data = df)
[1] 46
summary(model_Q3)
[1] 47

Call:
lm(formula = incidents ~ feedback + empathy + years, data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35580 -0.11148 -0.01097  0.11092  0.53552 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.276161   0.054108   5.104 7.84e-07 ***
feedbackFeedback  0.132132   0.023585   5.602 7.09e-08 ***
empathy           0.023187   0.004829   4.802 3.11e-06 ***
years            -0.012689   0.003975  -3.192  0.00165 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1663 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

plot(cooks.distance(model_Q3))
[1] 48
abline(h = cooks.distance(model_Q3) > (4/(200 - 3 - 1)), col = "red")
[1] 49
which(cooks.distance(model_Q3) > (4/(200 - 3 - 1)))
[1] 50
 73  74  89 137 156 160 165 176 
 73  74  89 137 156 160 165 176 
stdz_Q3 <- MASS::studres(model_Q3)
[1] 51
which(abs(stdz_Q3) > 2)
[1] 52
 13  21  73  74 150 156 159 160 165 176 181 
 13  21  73  74 150 156 159 160 165 176 181 
hats_Q3 <- hatvalues(model_Q3)
[1] 53
which(hats_Q3 > 2 * mean(hats_Q3))
[1] 54
 61  88  89  92 137 141 154 163 175 200 
 61  88  89  92 137 141 154 163 175 200 
cleared_Q3 <- lm(df$incidents ~ df$feedback + df$empathy + df$years, 
    subset = -c(73, 74, 89, 137, 156, 160, 165, 176, 13, 21, 
        159, 181, 61, 88, 92, 141, 154, 163, 175, 200))
[1] 55
summary(cleared_Q3)
[1] 56

Call:
lm(formula = df$incidents ~ df$feedback + df$empathy + df$years, 
    subset = -c(73, 74, 89, 137, 156, 160, 165, 176, 13, 21, 
        159, 181, 61, 88, 92, 141, 154, 163, 175, 200))

Residuals:
    Min      1Q  Median      3Q     Max 
-0.3076 -0.1023 -0.0047  0.1098  0.4395 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)          0.217718   0.054624   3.986 9.84e-05 ***
df$feedbackFeedback  0.136606   0.021450   6.369 1.61e-09 ***
df$empathy           0.027192   0.005052   5.382 2.32e-07 ***
df$years            -0.011974   0.004249  -2.818  0.00539 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1434 on 176 degrees of freedom
Multiple R-squared:  0.2716,	Adjusted R-squared:  0.2592 
F-statistic: 21.87 on 3 and 176 DF,  p-value: 4.354e-12

qqPlot(model_Q3$residuals)
[1] 57
qqPlot(cleared_Q3$residuals)
[1] 58
hist(model_Q3$residuals)
[1] 59
hist(cleared_Q3$residuals)
[1] 60
shapiro.test(model_Q3$residuals)
[1] 61

	Shapiro-Wilk normality test

data:  model_Q3$residuals
W = 0.98772, p-value = 0.08174

shapiro.test(cleared_Q3$residuals)
[1] 62

	Shapiro-Wilk normality test

data:  cleared_Q3$residuals
W = 0.99182, p-value = 0.4032

residualPlots(model_Q3)
[1] 63
residualPlots(cleared_Q3)
[1] 64
ncvTest(model_Q3)
[1] 65
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.01731234, Df = 1, p = 0.89532
ncvTest(cleared_Q3)
[1] 66
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.09523248, Df = 1, p = 0.75763
crPlots(model_Q3)
[1] 67
crPlots(cleared_Q3)
[1] 68
durbinWatsonTest(model_Q3)
[1] 69
 lag Autocorrelation D-W Statistic p-value
   1       0.2738264      1.444081       0
 Alternative hypothesis: rho != 0
durbinWatsonTest(cleared_Q3)
[1] 70
 lag Autocorrelation D-W Statistic p-value
   1       0.2580103      1.476003       0
 Alternative hypothesis: rho != 0
vif(model_Q3)
[1] 71
feedback  empathy    years 
1.005211 1.079397 1.077467 
vif(cleared_Q3)
[1] 72
df$feedback  df$empathy    df$years 
   1.006281    1.132230    1.128961 
model_Q4 <- lm(incidents ~ empathy + years, data = df)
[1] 73
summary(model_Q4)
[1] 74

Call:
lm(formula = incidents ~ empathy + years, data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.41333 -0.11682 -0.02057  0.10135  0.46713 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.351145   0.056325   6.234 2.71e-09 ***
empathy      0.021449   0.005177   4.143 5.08e-05 ***
years       -0.011610   0.004266  -2.722  0.00708 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1787 on 197 degrees of freedom
Multiple R-squared:  0.09222,	Adjusted R-squared:  0.08301 
F-statistic: 10.01 on 2 and 197 DF,  p-value: 7.259e-05

plot(cooks.distance(model_Q4))
[1] 75
abline(h = cooks.distance(model_Q4) > (4/(200 - 2 - 1)), col = "red")
[1] 76
which(cooks.distance(model_Q4) > (4/(200 - 2 - 1)))
[1] 77
 16  30  60  74  88  89  94 137 165 181 
 16  30  60  74  88  89  94 137 165 181 
stdz_Q4 <- MASS::studres(model_Q4)
[1] 78
which(abs(stdz_Q4) > 2)
[1] 79
 13  60  74 150 159 160 165 176 181 
 13  60  74 150 159 160 165 176 181 
hats_Q4 <- hatvalues(model_Q4)
[1] 80
which(hats_Q4 > 2 * mean(hats_Q4))
[1] 81
 16  61  68  88  89  92  94 108 116 137 141 142 154 155 163 171 175 190 200 
 16  61  68  88  89  92  94 108 116 137 141 142 154 155 163 171 175 190 200 
cleared_Q4 <- lm(df$incidents ~ df$empathy + df$years, subset = -c(16, 
    30, 60, 74, 88, 89, 94, 137, 165, 181, 13, 150, 159, 160, 
    176, 61, 68, 92, 108, 116, 141, 142, 154, 155, 163, 171, 
    175, 190, 200))
[1] 82
summary(cleared_Q4)
[1] 83

Call:
lm(formula = df$incidents ~ df$empathy + df$years, subset = -c(16, 
    30, 60, 74, 88, 89, 94, 137, 165, 181, 13, 150, 159, 160, 
    176, 61, 68, 92, 108, 116, 141, 142, 154, 155, 163, 171, 
    175, 190, 200))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.32618 -0.10160 -0.01827  0.10505  0.35505 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.372912   0.064795   5.755 4.01e-08 ***
df$empathy   0.018145   0.006166   2.943  0.00372 ** 
df$years    -0.010868   0.005226  -2.080  0.03909 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1575 on 168 degrees of freedom
Multiple R-squared:  0.05591,	Adjusted R-squared:  0.04467 
F-statistic: 4.974 on 2 and 168 DF,  p-value: 0.007967

qqPlot(model_Q4$residuals)
[1] 84
qqPlot(cleared_Q4$residuals)
[1] 85
hist(model_Q4$residuals)
[1] 86
hist(cleared_Q4$residuals)
[1] 87
shapiro.test(model_Q4$residuals)
[1] 88

	Shapiro-Wilk normality test

data:  model_Q4$residuals
W = 0.98801, p-value = 0.09043

shapiro.test(cleared_Q4$residuals)
[1] 89

	Shapiro-Wilk normality test

data:  cleared_Q4$residuals
W = 0.98491, p-value = 0.06151

residualPlots(model_Q4)
[1] 90
residualPlots(cleared_Q4)
[1] 91
ncvTest(model_Q4)
[1] 92
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.6254715, Df = 1, p = 0.42902
ncvTest(cleared_Q4)
[1] 93
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 1.290609, Df = 1, p = 0.25594
crPlots(model_Q4)
[1] 94
crPlots(cleared_Q4)
[1] 95
durbinWatsonTest(model_Q4)
[1] 96
 lag Autocorrelation D-W Statistic p-value
   1      0.09754768       1.79735    0.18
 Alternative hypothesis: rho != 0
durbinWatsonTest(cleared_Q4)
[1] 97
 lag Autocorrelation D-W Statistic p-value
   1      0.01559474      1.950757   0.752
 Alternative hypothesis: rho != 0
vif(model_Q4)
[1] 98
empathy   years 
1.07494 1.07494 
vif(cleared_Q4)
[1] 99
df$empathy   df$years 
  1.134575   1.134575 
anova(model_Q4, model_Q2)
[1] 100
Analysis of Variance Table

Model 1: incidents ~ empathy + years
Model 2: incidents ~ training + empathy + years
  Res.Df    RSS Df Sum of Sq      F  Pr(>F)    
1    197 6.2914                                
2    196 5.1532  1    1.1382 43.292 4.2e-10 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
anova(model_Q4, model_Q3)
[1] 101
Analysis of Variance Table

Model 1: incidents ~ empathy + years
Model 2: incidents ~ feedback + empathy + years
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)    
1    197 6.2914                                 
2    196 5.4230  1   0.86841 31.387 7.09e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
describeBy(df$incidents, group = list(df$training, df$feedback), 
    mat = TRUE, digits = 2)
[1] 102
    item      group1      group2 vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X11    1 No training No feedback    1 50 0.49 0.17   0.42    0.48 0.12 0.25 0.92  0.67 0.82    -0.07 0.02
X12    2    Training No feedback    1 50 0.33 0.14   0.33    0.33 0.12 0.08 0.58  0.50 0.15    -0.96 0.02
X13    3 No training    Feedback    1 50 0.59 0.17   0.58    0.59 0.12 0.25 0.92  0.67 0.05    -0.81 0.02
 [ reached 'max' / getOption("max.print") -- omitted 1 rows ]
model_Q5 <- lm(incidents ~ training + feedback + training * feedback, 
    data = df)
[1] 103
summary(model_Q5)
[1] 104

Call:
lm(formula = incidents ~ training + feedback + training * feedback, 
    data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40000 -0.10500  0.00000  0.08333  0.43333 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.49333    0.02314  21.323  < 2e-16 ***
trainingTraining                  -0.16000    0.03272  -4.890 2.09e-06 ***
feedbackFeedback                   0.09667    0.03272   2.954  0.00352 ** 
trainingTraining:feedbackFeedback  0.05333    0.04627   1.153  0.25047    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1636 on 196 degrees of freedom
Multiple R-squared:  0.2431,	Adjusted R-squared:  0.2315 
F-statistic: 20.99 on 3 and 196 DF,  p-value: 7.827e-12

interaction.plot(df$training, df$feedback, df$incidents, fun = mean, 
    xlab = "Training", ylab = "Incidents", col = c("blue4", "red4"), 
    trace.label = "Feedback")
[1] 105
plot(cooks.distance(model_Q5))
[1] 106
abline(h = cooks.distance(model_Q5) > (4/(200 - 3 - 1)), col = "red")
[1] 107
which(cooks.distance(model_Q5) > (4/(200 - 3 - 1)))
[1] 108
  7  13  89 131 150 159 160 165 176 181 
  7  13  89 131 150 159 160 165 176 181 
stdz_Q5 <- MASS::studres(model_Q5)
[1] 109
which(abs(stdz_Q5) > 2)
[1] 110
  7  13  89 131 150 159 160 165 176 181 
  7  13  89 131 150 159 160 165 176 181 
hats_Q5 <- hatvalues(model_Q5)
[1] 111
which(hats_Q5 > 2 * mean(hats_Q5))
[1] 112
named integer(0)
cleared_Q5 <- lm(df$incidents ~ df$training + df$feedback + df$training * 
    df$feedback, subset = -c(7, 13, 89, 131, 150, 159, 160, 165, 
    176, 181))
[1] 113
summary(cleared_Q5)
[1] 114

Call:
lm(formula = df$incidents ~ df$training + df$feedback + df$training * 
    df$feedback, subset = -c(7, 13, 89, 131, 150, 159, 160, 165, 
    176, 181))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30851 -0.12190  0.00000  0.09058  0.28192 

Coefficients:
                                        Estimate Std. Error t value Pr(>|t|)    
(Intercept)                              0.46809    0.02091  22.382  < 2e-16 ***
df$trainingTraining                     -0.13475    0.02913  -4.626 6.97e-06 ***
df$feedbackFeedback                      0.10800    0.02974   3.632 0.000364 ***
df$trainingTraining:df$feedbackFeedback  0.03384    0.04163   0.813 0.417260    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1434 on 186 degrees of freedom
Multiple R-squared:  0.273,	Adjusted R-squared:  0.2613 
F-statistic: 23.28 on 3 and 186 DF,  p-value: 7.654e-13

qqPlot(model_Q5$residuals)
[1] 115
qqPlot(cleared_Q5$residuals)
[1] 116
hist(model_Q5$residuals)
[1] 117
hist(cleared_Q5$residuals)
[1] 118
shapiro.test(model_Q5$residuals)
[1] 119

	Shapiro-Wilk normality test

data:  model_Q5$residuals
W = 0.9767, p-value = 0.002072

shapiro.test(cleared_Q5$residuals)
[1] 120

	Shapiro-Wilk normality test

data:  cleared_Q5$residuals
W = 0.97256, p-value = 0.0008688

log_trans_Q5 <- log(model_Q5$residuals + (-1 * min(model_Q5$residuals)) + 
    1)
[1] 121
shapiro.test(log_trans_Q5)
[1] 122

	Shapiro-Wilk normality test

data:  log_trans_Q5
W = 0.98403, p-value = 0.02292

log_trans_cleared_Q5 <- log(cleared_Q5$residuals + (-1 * min(cleared_Q5$residuals)) + 
    1)
[1] 123
shapiro.test(log_trans_cleared_Q5)
[1] 124

	Shapiro-Wilk normality test

data:  log_trans_cleared_Q5
W = 0.97673, p-value = 0.002946

training_Q5 <- as.integer(df$training) - 1
[1] 125
feedback_Q5 <- as.integer(df$feedback) - 1
[1] 126
trainingfeedback_Q5 <- training_Q5 * feedback_Q5
[1] 127
residualPlots(lm(df$incidents ~ df$training + df$feedback + trainingfeedback_Q5))
[1] 128
ncvTest(model_Q5)
[1] 129
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 1.881673, Df = 1, p = 0.17014
ncvTest(cleared_Q5)
[1] 130
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.1495276, Df = 1, p = 0.69899
crPlots(lm(df$incidents ~ df$training + df$feedback + trainingfeedback_Q5))
[1] 131
durbinWatsonTest(model_Q5)
[1] 132
 lag Autocorrelation D-W Statistic p-value
   1       0.1114255      1.770733   0.142
 Alternative hypothesis: rho != 0
durbinWatsonTest(cleared_Q5)
[1] 133
 lag Autocorrelation D-W Statistic p-value
   1       0.0576509      1.874842   0.436
 Alternative hypothesis: rho != 0
vif(model_Q5)
[1] 134
         training          feedback training:feedback 
                2                 2                 3 
vif(cleared_Q5)
[1] 135
            df$training             df$feedback df$training:df$feedback 
               1.959769                2.042341                2.981730 
