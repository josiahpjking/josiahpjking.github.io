

	####### B112389 R script #######

install.packages("car")
[1] 1
install.packages("psych")
[1] 2
install.packages("ez")
[1] 3
install.packages("interactions")
[1] 4
install.packages("sandwich")
[1] 5
install.packages("stR")
[1] 6
library(car, quietly = T)
[1] 7
 [1] "car"       "carData"   "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[10] "base"     
library(psych, quietly = T)
[1] 8
 [1] "psych"     "car"       "carData"   "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[10] "methods"   "base"     
library(ez, quietly = T)
[1] 9
 [1] "ez"        "psych"     "car"       "carData"   "readr"     "stats"     "graphics"  "grDevices" "utils"    
[10] "datasets"  "methods"   "base"     
library(interactions, quietly = T)
[1] 10
 [1] "interactions" "ez"           "psych"        "car"          "carData"      "readr"        "stats"       
 [8] "graphics"     "grDevices"    "utils"        "datasets"     "methods"      "base"        
library(sandwich, quietly = T)
[1] 11
 [1] "sandwich"     "interactions" "ez"           "psych"        "car"          "carData"      "readr"       
 [8] "stats"        "graphics"     "grDevices"    "utils"        "datasets"     "methods"      "base"        
library(stR, quietly = T)
[1] 12
[1] "Error in library(stR, quietly = T) : there is no package called ‘stR’\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<packageNotFoundError in library(stR, quietly = T): there is no package called ‘stR’>


####################################################################################################

[1] "Error in library(stR, quietly = T) : there is no package called ‘stR’\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<packageNotFoundError in library(stR, quietly = T): there is no package called ‘stR’>
####################################################################################################


datax <- read.csv("../RMS2_report_1920.csv")
[1] 13
M1 <- lm(datax$empathy ~ datax$years)
[1] 14
summary(M1)
[1] 15

Call:
lm(formula = datax$empathy ~ datax$years)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.5399 -1.5160  0.0708  1.6357  5.8533 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  8.40625    0.49085  17.126  < 2e-16 ***
datax$years  0.21756    0.05648   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.453 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

line(datax$empathy ~ datax$years)
[1] 16
cor(datax$years, datax$empathy, method = "spearman")
[1] 17
[1] 0.3071045
plot(datax$empathy, datax$years, main = "Relationship Between Empathy and Experience", 
    xlab = "Years of Experience", ylab = "Emapathy")
[1] 18
abline(M1)
[1] 19
lines(lowess(datax$empathy ~ datax$years))
[1] 20
crPlots(M1, main = "Distribution of Data", xlab = "Experience in Years", 
    ylab = "component+Residual Empathy")
[1] 21
M1a <- lm(empathy ~ log(datax$years, base = 10), data = datax)
[1] 22
crPlots(model = M1a, main = "Plot to Demonstrate Relationship Between Empathy and Experience", 
    xlab = "Years", ylab = "Empathy")
[1] 23
plot(M1a, main = "Relationship between empathy and eperience - Post log transformation")
[1] 24
lines(lowess(datax$empathy ~ datax$years))
[1] 25
hist(log(datax$years, base = 10))
[1] 26
boxplot(datax$empathy_min, datax$years, main = "Check for Outliers for Emapthy and Experience", 
    xlab = "Years of Experience", ylab = "Empathy")
[1] 27
ncvTest(M1)
[1] 28
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.9815033, Df = 1, p = 0.32183
qqPlot(M1, main = "Distribution of Residuals")
[1] 29
residualPlot(M1, main = "Distribution of Residuals")
[1] 30
shapiro.test(datax$empathy)
[1] 31

	Shapiro-Wilk normality test

data:  datax$empathy
W = 0.97073, p-value = 0.0003477

shapiro.test(datax$years)
[1] 32

	Shapiro-Wilk normality test

data:  datax$years
W = 0.97276, p-value = 0.0006279

residualPlot(M1, main = "Distribution of Residuals Before Log Transformation")
[1] 33
residualPlot(M1a, main = "Distribution of Residuals After Log Transformation")
[1] 34
qqPlot(M1a, main = "Distribution of Residuals After Log Transformation")
[1] 35
hist(M1$residuals, main = "Histogram to Show Distribution of Residuals Before log Transformation")
[1] 36
hist(M1a$residuals, main = "Histogram to Show Distribution of Residuals After log Transformation")
[1] 37
M2 <- lm(incidents ~ training + empathy + years, data = datax)
[1] 38
summary(M2)
[1] 39

Call:
lm(formula = incidents ~ training + empathy + years, data = datax)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.559747   0.060141   9.307  < 2e-16 ***
training    -0.152427   0.023166  -6.580 4.20e-10 ***
empathy      0.025396   0.004735   5.363 2.29e-07 ***
years       -0.014085   0.003889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

plot(M2)
[1] 40
hist(M2$residuals, main = "Distribution of Residuals")
[1] 41
qqPlot(M2, main = "Distribution of Residuals")
[1] 42
shapiro.test(M2$residuals)
[1] 43

	Shapiro-Wilk normality test

data:  M2$residuals
W = 0.97994, p-value = 0.005838

residualPlot(M2, main = "Residual Plot")
[1] 44
ncvTest(M2)
[1] 45
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
vif(M2)
[1] 46
training  empathy    years 
1.020633 1.092467 1.085089 
durbinWatsonTest(M2)
[1] 47
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.196
 Alternative hypothesis: rho != 0
crPlots(model = M2, main = "Component and Residual Plots Between Preictor Variables")
[1] 48
plot(cooks.distance(M2), main = "Cook's Distance Plot")
[1] 49
abline(h = cooks.distance(M2) > (4/(200 - 3 - 1)), col = "red")
[1] 50
which(cooks.distance(M2) > (4/(200 - 3 - 1)))
[1] 51
 13  35  74 137 165 
 13  35  74 137 165 
plot(M2, which = 4, main = "Outliers")
[1] 52
M2a <- lm(incidents ~ training + empathy + years, data = datax[-c(13, 
    74, 137)])
[1] 53
summary(M2a)
[1] 54

Call:
lm(formula = incidents ~ training + empathy + years, data = datax[-c(13, 
    74, 137)])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.559747   0.060141   9.307  < 2e-16 ***
training    -0.152427   0.023166  -6.580 4.20e-10 ***
empathy      0.025396   0.004735   5.363 2.29e-07 ***
years       -0.014085   0.003889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

datax$training <- factor(x = datax$training, label = c("No training", 
    "Training"))
[1] 55
datax$feedback <- factor(x = datax$feedback, label = c("No feedback", 
    "feedback"))
[1] 56
summary(datax)
[1] 57
    subject         incidents              training          feedback      empathy          years      
 Min.   :  1.00   Min.   :0.08333   No training:100   No feedback:100   Min.   : 3.00   Min.   : 1.00  
 1st Qu.: 50.75   1st Qu.:0.33333   Training   :100   feedback   :100   1st Qu.: 9.00   1st Qu.: 6.00  
 Median :100.50   Median :0.41667                                       Median :10.00   Median : 8.00  
 Mean   :100.50   Mean   :0.47500                                       Mean   :10.18   Mean   : 8.13  
 3rd Qu.:150.25   3rd Qu.:0.58333                                       3rd Qu.:12.00   3rd Qu.:10.00  
 Max.   :200.00   Max.   :0.91667                                       Max.   :16.00   Max.   :19.00  
contrasts(datax$feedback)
[1] 58
            feedback
No feedback        0
feedback           1
summary(lm(incidents ~ feedback + years + empathy, data = datax))
[1] 59

Call:
lm(formula = incidents ~ feedback + years + empathy, data = datax)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35580 -0.11148 -0.01097  0.11092  0.53552 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.276161   0.054108   5.104 7.84e-07 ***
feedbackfeedback  0.132132   0.023585   5.602 7.09e-08 ***
years            -0.012689   0.003975  -3.192  0.00165 ** 
empathy           0.023187   0.004829   4.802 3.11e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1663 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

M3 <- lm(incidents ~ training + empathy + years + feedback, data = datax)
[1] 60
summary(M3)
[1] 61

Call:
lm(formula = incidents ~ training + empathy + years + feedback, 
    data = datax)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.31626 -0.10096 -0.00949  0.09165  0.46650 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.331953   0.048708   6.815 1.15e-10 ***
trainingTraining -0.153824   0.021128  -7.281 7.98e-12 ***
empathy           0.027191   0.004328   6.283 2.12e-09 ***
years            -0.015199   0.003551  -4.281 2.92e-05 ***
feedbackfeedback  0.133715   0.020968   6.377 1.28e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1479 on 195 degrees of freedom
Multiple R-squared:  0.3848,	Adjusted R-squared:  0.3721 
F-statistic: 30.49 on 4 and 195 DF,  p-value: < 2.2e-16

as.factor(datax$feedback)
[1] 62
 [1] feedback    No feedback feedback    No feedback feedback    No feedback feedback    No feedback feedback   
[10] No feedback feedback    No feedback feedback    No feedback feedback    No feedback feedback    No feedback
[19] feedback    No feedback feedback    No feedback feedback    No feedback feedback    No feedback feedback   
[28] No feedback feedback    No feedback feedback    No feedback feedback    No feedback feedback    No feedback
[37] feedback    No feedback feedback    No feedback feedback    No feedback feedback    No feedback feedback   
[46] No feedback feedback    No feedback feedback    No feedback
 [ reached getOption("max.print") -- omitted 150 entries ]
Levels: No feedback feedback
str(datax)
[1] 63
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : Factor w/ 2 levels "No training",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : Factor w/ 2 levels "No feedback",..: 2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
durbinWatsonTest(M3)
[1] 64
 lag Autocorrelation D-W Statistic p-value
   1      0.07303546      1.841386   0.248
 Alternative hypothesis: rho != 0
vif(M3)
[1] 65
training  empathy    years feedback 
1.020743 1.097109 1.087722 1.005320 
hist(M3$residuals, main = "Distribution of Residuals")
[1] 66
qqPlot(M3, main = "Distribution of Residuals")
[1] 67
shapiro.test(M3$residuals)
[1] 68

	Shapiro-Wilk normality test

data:  M3$residuals
W = 0.98734, p-value = 0.07172

ncvTest(M3)
[1] 69
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 3.684027, Df = 1, p = 0.054936
residualPlot(M3, main = "Residual Plot")
[1] 70
crPlots(model = M3, main = "Compontant and Residual Plots")
[1] 71
plot(cooks.distance(M3), main = "Cook's Distance")
[1] 72
abline(h = cooks.distance(M3) > (4/(200 - 4 - 1)), col = "red")
[1] 73
which(cooks.distance(M3) > (4/(200 - 4 - 1)))
[1] 74
 13  35  74 117 131 137 141 156 160 176 
 13  35  74 117 131 137 141 156 160 176 
plot(M3, which = 4, main = "Outliers")
[1] 75
M3a <- lm(incidents ~ training + empathy + years + feedback, 
    data = datax[-c(13, 137, 176)])
[1] 76
summary(M3a)
[1] 77

Call:
lm(formula = incidents ~ training + empathy + years + feedback, 
    data = datax[-c(13, 137, 176)])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.31626 -0.10096 -0.00949  0.09165  0.46650 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.331953   0.048708   6.815 1.15e-10 ***
trainingTraining -0.153824   0.021128  -7.281 7.98e-12 ***
empathy           0.027191   0.004328   6.283 2.12e-09 ***
years            -0.015199   0.003551  -4.281 2.92e-05 ***
feedbackfeedback  0.133715   0.020968   6.377 1.28e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1479 on 195 degrees of freedom
Multiple R-squared:  0.3848,	Adjusted R-squared:  0.3721 
F-statistic: 30.49 on 4 and 195 DF,  p-value: < 2.2e-16

AIC(M2, M3)
[1] 78
   df       AIC
M2  5 -154.1654
M3  6 -190.0499
BIC(M2, M3)
[1] 79
   df       BIC
M2  5 -137.6738
M3  6 -170.2600
anova(M2, M3)
[1] 80
Analysis of Variance Table

Model 1: incidents ~ training + empathy + years
Model 2: incidents ~ training + empathy + years + feedback
  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
1    196 5.1532                                  
2    195 4.2639  1   0.88925 40.668 1.279e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
install.packages("interactions")
[1] 81
library(interactions, quietly = T)
[1] 82
 [1] "sandwich"     "interactions" "ez"           "psych"        "car"          "carData"      "readr"       
 [8] "stats"        "graphics"     "grDevices"    "utils"        "datasets"     "methods"      "base"        
contrasts(datax$training) <- contr.treatment(2, base = 1)
[1] 83
contrasts(datax$feedback) <- contr.treatment(2, base = 1)
[1] 84
M5 <- lm(formula = incidents ~ training + feedback + training * 
    feedback, data = datax)
[1] 85
summary(M5)
[1] 86

Call:
lm(formula = incidents ~ training + feedback + training * feedback, 
    data = datax)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40000 -0.10500  0.00000  0.08333  0.43333 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)          0.49333    0.02314  21.323  < 2e-16 ***
training2           -0.16000    0.03272  -4.890 2.09e-06 ***
feedback2            0.09667    0.03272   2.954  0.00352 ** 
training2:feedback2  0.05333    0.04627   1.153  0.25047    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1636 on 196 degrees of freedom
Multiple R-squared:  0.2431,	Adjusted R-squared:  0.2315 
F-statistic: 20.99 on 3 and 196 DF,  p-value: 7.827e-12

plot(M5)
[1] 87
residualPlot(M5)
[1] 88
cat_plot(M5, pred = training, modx = feedback, plot.points = TRUE, 
    main = "Interaction Between Training and Feedback")
[1] 89
M5a <- lm(incidents ~ training + feedback, data = datax)
[1] 90
summary(M5a)
[1] 91

Call:
lm(formula = incidents ~ training + feedback, data = datax)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.38667 -0.11167 -0.01333  0.07000  0.44667 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.48000    0.02005  23.937  < 2e-16 ***
training2   -0.13333    0.02315  -5.758 3.22e-08 ***
feedback2    0.12333    0.02315   5.326 2.72e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1637 on 197 degrees of freedom
Multiple R-squared:  0.238,	Adjusted R-squared:  0.2303 
F-statistic: 30.76 on 2 and 197 DF,  p-value: 2.359e-12

M5b <- aov(incidents ~ training + feedback, data = datax)
[1] 92
summary(M5b)
[1] 93
             Df Sum Sq Mean Sq F value   Pr(>F)    
training      1  0.889  0.8889   33.16 3.22e-08 ***
feedback      1  0.761  0.7606   28.37 2.72e-07 ***
Residuals   197  5.281  0.0268                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
anova(M5)
[1] 94
Analysis of Variance Table

Response: incidents
                   Df Sum Sq Mean Sq F value    Pr(>F)    
training            1 0.8889 0.88889 33.2133 3.164e-08 ***
feedback            1 0.7606 0.76056 28.4181 2.675e-07 ***
training:feedback   1 0.0356 0.03556  1.3285    0.2505    
Residuals         196 5.2456 0.02676                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
