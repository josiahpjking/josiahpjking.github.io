

	####### B107954 R script #######

setwd("~/Desktop/Stats Labs/RMS2")
[1] 1
library(jtools, quietly = T)
[1] 2
[1] "jtools"    "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(sandwich, quietly = T)
[1] 3
 [1] "sandwich"  "jtools"    "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[10] "base"     
library(car, quietly = T)
[1] 4
 [1] "car"       "carData"   "sandwich"  "jtools"    "readr"     "stats"     "graphics"  "grDevices" "utils"    
[10] "datasets"  "methods"   "base"     
library(psych, quietly = T)
[1] 5
 [1] "psych"     "car"       "carData"   "sandwich"  "jtools"    "readr"     "stats"     "graphics"  "grDevices"
[10] "utils"     "datasets"  "methods"   "base"     
library(interactions, quietly = T)
[1] 6
 [1] "interactions" "psych"        "car"          "carData"      "sandwich"     "jtools"       "readr"       
 [8] "stats"        "graphics"     "grDevices"    "utils"        "datasets"     "methods"      "base"        
library(ggplot2, quietly = T)
[1] 7
 [1] "ggplot2"      "interactions" "psych"        "car"          "carData"      "sandwich"     "jtools"      
 [8] "readr"        "stats"        "graphics"     "grDevices"    "utils"        "datasets"     "methods"     
[15] "base"        
library(lattice, quietly = T)
[1] 8
 [1] "lattice"      "ggplot2"      "interactions" "psych"        "car"          "carData"      "sandwich"    
 [8] "jtools"       "readr"        "stats"        "graphics"     "grDevices"    "utils"        "datasets"    
[15] "methods"      "base"        
CW <- read.csv("../RMS2_report_1920.csv")
[1] 9
View(CW)
[1] 10
CW$training <- as.factor(CW$training)
[1] 11
levels(CW$training)
[1] 12
[1] "1" "2"
CW$training <- factor(CW$training, labels = c("No Training", 
    "Training"))
[1] 13
summary(CW$training)
[1] 14
No Training    Training 
        100         100 
CW$feedback <- as.factor(CW$feedback)
[1] 15
levels(CW$feedback)
[1] 16
[1] "1" "2"
CW$feedback <- factor(CW$feedback, labels = c("No Feedback", 
    "Feedback"))
[1] 17
summary(CW$feedback)
[1] 18
No Feedback    Feedback 
        100         100 
which((CW$incidents) > 1)
[1] 19
integer(0)
which((CW$empathy) > 20)
[1] 20
integer(0)
which((CW$years) > 30)
[1] 21
integer(0)
describe(CW)
[1] 22
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training*    3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
CW$empathy_mc <- CW$empathy - mean(CW$empathy)
[1] 23
CW$years_mc <- CW$years - mean(CW$years)
[1] 24
describe(CW)[c(5, 6), c(c(2:4, 8, 9, 11, 12))]
[1] 25
          n  mean   sd min max  skew kurtosis
empathy 200 10.18 2.54   3  16 -0.37     0.23
years   200  8.13 3.08   1  19  0.52     0.48
summary(CW[, 5:6])
[1] 26
    empathy          years      
 Min.   : 3.00   Min.   : 1.00  
 1st Qu.: 9.00   1st Qu.: 6.00  
 Median :10.00   Median : 8.00  
 Mean   :10.18   Mean   : 8.13  
 3rd Qu.:12.00   3rd Qu.:10.00  
 Max.   :16.00   Max.   :19.00  
hist(CW$years, main = "Distribution for the variable Experience in Years", 
    xlab = "Experience in years")
[1] 27
hist(CW$empathy, main = "Distrubition for the variable Empathy", 
    xlab = "Empathy")
[1] 28
pairs.panels(CW[, 5:6])
[1] 29
NULL
M1 <- lm(empathy_mc ~ years_mc, data = CW)
[1] 30
summary(M1)
[1] 31

Call:
lm(formula = empathy_mc ~ years_mc, data = CW)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.5399 -1.5160  0.0708  1.6357  5.8533 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -6.013e-16  1.735e-01   0.000 1.000000    
years_mc     2.176e-01  5.648e-02   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.453 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

plot(CW$empathy_mc ~ CW$years_mc, xlab = "Experience in Years", 
    ylab = "Empathy", main = "Relationship between Empathy and Experience")
[1] 32
abline(M1, col = "blue")
[1] 33
lines(lowess(CW$years_mc, CW$empathy_mc), col = "red")
[1] 34
plot(M1, which = 3)
[1] 35
qqPlot(M1, main = "QQ-plot for M1")
[1] 36
hist(M1$residuals)
[1] 37
shapiro.test(M1$residuals)
[1] 38

	Shapiro-Wilk normality test

data:  M1$residuals
W = 0.98911, p-value = 0.1321

M1_pol <- lm(CW$empathy_mc ~ poly(CW$years_mc, 2))
[1] 39
summary(M1_pol)
[1] 40

Call:
lm(formula = CW$empathy_mc ~ poly(CW$years_mc, 2))

Residuals:
    Min      1Q  Median      3Q     Max 
-4.5069 -1.4011 -0.0835  1.1707  6.0622 

Coefficients:
                        Estimate Std. Error t value Pr(>|t|)    
(Intercept)           -8.375e-16  1.392e-01    0.00        1    
poly(CW$years_mc, 2)1  9.450e+00  1.969e+00    4.80 3.13e-06 ***
poly(CW$years_mc, 2)2 -2.069e+01  1.969e+00  -10.51  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.969 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

crPlots(M1_pol, xlab = "Experience in Years", ylab = "Empathy", 
    main = "Relationship between Empathy and Experience after Polynomial \n        Transformation")
[1] 41
plot(M1_pol, which = 3)
[1] 42
residualPlots(M1_pol)
[1] 43
ncvTest(M1_pol)
[1] 44
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
hist(M1_pol$residuals)
[1] 45
durbinWatsonTest(M1_pol)
[1] 46
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855   0.876
 Alternative hypothesis: rho != 0
describe(CW)[c(2:3, 5:6), c(c(2:4, 8, 9, 11, 12))]
[1] 47
            n  mean   sd  min   max  skew kurtosis
incidents 200  0.48 0.19 0.08  0.92  0.32    -0.31
training* 200  1.50 0.50 1.00  2.00  0.00    -2.01
empathy   200 10.18 2.54 3.00 16.00 -0.37     0.23
years     200  8.13 3.08 1.00 19.00  0.52     0.48
hist(CW$incidents)
[1] 48
hist(CW$years_mc)
[1] 49
boxplot(CW$incidents ~ CW$training, main = "Distribution of the variable 'Training' ")
[1] 50
M2 <- lm(incidents ~ training + years_mc + empathy_mc, data = CW)
[1] 51
summary(M2)
[1] 52

Call:
lm(formula = incidents ~ training + years_mc + empathy_mc, data = CW)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.551213   0.016298  33.821  < 2e-16 ***
trainingTraining -0.152427   0.023166  -6.580 4.20e-10 ***
years_mc         -0.014085   0.003889  -3.622 0.000372 ***
empathy_mc        0.025396   0.004735   5.363 2.29e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

crPlots(M2)
[1] 53
qqPlot(M2, main = "QQ-plot for M2")
[1] 54
hist(M2$residuals)
[1] 55
shapiro.test(M2$residuals)
[1] 56

	Shapiro-Wilk normality test

data:  M2$residuals
W = 0.97994, p-value = 0.005838

log(CW$incidents)
[1] 57
 [1] -0.40546511 -1.09861229 -1.09861229 -0.87546874 -0.69314718 -1.38629436 -0.18232156 -0.87546874 -0.69314718
[10] -1.79175947 -0.28768207 -1.09861229 -0.08701138 -0.69314718 -0.53899650 -1.79175947 -1.79175947 -1.09861229
[19] -0.28768207 -0.87546874 -1.79175947 -0.87546874 -0.87546874 -0.53899650 -0.87546874 -1.38629436 -0.69314718
[28] -1.09861229 -0.87546874 -2.48490665 -1.09861229 -0.69314718 -1.09861229 -0.87546874 -0.28768207 -0.87546874
[37] -0.69314718 -0.53899650 -0.53899650 -1.79175947 -0.40546511 -0.87546874 -1.09861229 -1.38629436 -0.87546874
[46] -1.38629436 -0.53899650 -0.53899650 -0.69314718 -1.79175947
 [ reached getOption("max.print") -- omitted 150 entries ]
logM2 <- lm(log(incidents + 1) ~ training + years_mc + empathy_mc, 
    data = CW)
[1] 58
summary(logM2)
[1] 59

Call:
lm(formula = log(incidents + 1) ~ training + years_mc + empathy_mc, 
    data = CW)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25974 -0.08206 -0.00301  0.07026  0.33183 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.433150   0.010944  39.578  < 2e-16 ***
trainingTraining -0.104810   0.015556  -6.737 1.75e-10 ***
years_mc         -0.009683   0.002611  -3.708 0.000272 ***
empathy_mc        0.017470   0.003180   5.494 1.21e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1089 on 196 degrees of freedom
Multiple R-squared:  0.2657,	Adjusted R-squared:  0.2544 
F-statistic: 23.63 on 3 and 196 DF,  p-value: 4.229e-13

crPlots(logM2)
[1] 60
hist(logM2$residuals)
[1] 61
shapiro.test(logM2$residuals)
[1] 62

	Shapiro-Wilk normality test

data:  logM2$residuals
W = 0.9901, p-value = 0.1845

durbinWatsonTest(logM2)
[1] 63
 lag Autocorrelation D-W Statistic p-value
   1      -0.1160146      2.216759   0.134
 Alternative hypothesis: rho != 0
residualPlots(logM2, main = "Residual Plots for logM2")
[1] 64
ncvTest(logM2)
[1] 65
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.8932792, Df = 1, p = 0.34459
vif(logM2)
[1] 66
  training   years_mc empathy_mc 
  1.020633   1.085089   1.092467 
Cooks2 <- cooks.distance(logM2)
[1] 67
which(Cooks2 > 4/(200 - 3 - 1))
[1] 68
 13  30  35  60  74  88  94 137 
 13  30  35  60  74  88  94 137 
plot(logM2, which = 4)
[1] 69
plot(logM2, which = 5)
[1] 70
M2_check <- lm(log(incidents + 1) ~ training + years + empathy_mc, 
    data = CW[-c(35, 74, 137), ])
[1] 71
summary(M2_check)
[1] 72

Call:
lm(formula = log(incidents + 1) ~ training + years + empathy_mc, 
    data = CW[-c(35, 74, 137), ])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.21361 -0.07971 -0.00072  0.07005  0.33221 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.528624   0.024437  21.632  < 2e-16 ***
trainingTraining -0.104122   0.015173  -6.862 8.98e-11 ***
years            -0.011904   0.002646  -4.500 1.18e-05 ***
empathy_mc        0.019398   0.003171   6.117 5.21e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1057 on 193 degrees of freedom
Multiple R-squared:  0.2988,	Adjusted R-squared:  0.2879 
F-statistic: 27.42 on 3 and 193 DF,  p-value: 8.169e-15

check1 <- summary(M2_check)
[1] 73
check2 <- summary(logM2)
[1] 74
comparison <- data.frame(round(check2$coefficients[, 2], 3), 
    round(check2$coefficients[, 4], 4), round(check1$coefficients[, 
        2], 3), round(check1$coefficients[, 4], 4))
[1] 75
colnames(comparison) <- c("Check2", "logM2 p-value", "Check1", 
    "M2 check p-value")
[1] 76
comparison

                 Check2 logM2 p-value Check1 M2 check p-value
(Intercept)       0.011         0e+00  0.024                0
trainingTraining  0.016         0e+00  0.015                0
years_mc          0.003         3e-04  0.003                0
empathy_mc        0.003         0e+00  0.003                0


logM3 <- lm(log(incidents + 1) ~ feedback + empathy_mc + years_mc, 
    data = CW)
[1] 78
summary(logM3)
[1] 79

Call:
lm(formula = log(incidents + 1) ~ feedback + empathy_mc + years_mc, 
    data = CW)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25959 -0.07169 -0.00250  0.07834  0.33399 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.335689   0.011213  29.938  < 2e-16 ***
feedbackFeedback  0.090112   0.015878   5.675 4.93e-08 ***
empathy_mc        0.015942   0.003251   4.904 1.97e-06 ***
years_mc         -0.008717   0.002676  -3.257  0.00132 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.112 on 196 degrees of freedom
Multiple R-squared:  0.2232,	Adjusted R-squared:  0.2113 
F-statistic: 18.78 on 3 and 196 DF,  p-value: 9.56e-11

qqPlot(logM3, main = "QQ-plot for m3")
[1] 80
hist(logM3$residuals)
[1] 81
shapiro.test(logM3$residuals)
[1] 82

	Shapiro-Wilk normality test

data:  logM3$residuals
W = 0.99557, p-value = 0.8301

residualPlots(logM3, main = "Residual Plots for m3")
[1] 83
ncvTest(logM3)
[1] 84
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5811934, Df = 1, p = 0.44584
vif(logM3)
[1] 85
  feedback empathy_mc   years_mc 
  1.005211   1.079397   1.077467 
crPlots(logM3)
[1] 86
durbinWatsonTest(logM3)
[1] 87
 lag Autocorrelation D-W Statistic p-value
   1       0.2792812      1.432218       0
 Alternative hypothesis: rho != 0
plot(logM3, which = 4)
[1] 88
plot(logM3, which = 5)
[1] 89
AIC(logM3, logM2)
[1] 90
      df       AIC
logM3  5 -302.2294
logM2  5 -313.4627
describe(CW)[c(2:4), c(c(2:4, 8, 9, 11, 12))]
[1] 91
            n mean   sd  min  max skew kurtosis
incidents 200 0.48 0.19 0.08 0.92 0.32    -0.31
training* 200 1.50 0.50 1.00 2.00 0.00    -2.01
feedback* 200 1.50 0.50 1.00 2.00 0.00    -2.01
describeBy(CW$incidents, group = list(CW$feedback, CW$training), 
    mat = T)
[1] 92
    item      group1      group2 vars  n      mean        sd    median   trimmed     mad        min       max
X11    1 No Feedback No Training    1 50 0.4933333 0.1673794 0.4166667 0.4770833 0.12355 0.25000000 0.9166667
X12    2    Feedback No Training    1 50 0.5900000 0.1707327 0.5833333 0.5875000 0.12355 0.25000000 0.9166667
X13    3 No Feedback    Training    1 50 0.3333333 0.1398493 0.3333333 0.3270833 0.12355 0.08333333 0.5833333
        range       skew   kurtosis         se
X11 0.6666667 0.81813067 -0.0698501 0.02367103
X12 0.6666667 0.04793352 -0.8075298 0.02414525
X13 0.5000000 0.15233791 -0.9575614 0.01977768
 [ reached 'max' / getOption("max.print") -- omitted 1 rows ]
M5a <- lm(incidents ~ training + feedback + training:feedback, 
    data = CW)
[1] 93
summary(M5a)
[1] 94

Call:
lm(formula = incidents ~ training + feedback + training:feedback, 
    data = CW)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40000 -0.10500  0.00000  0.08333  0.43333 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.49333    0.02314  21.323  < 2e-16 ***
trainingTraining                  -0.16000    0.03272  -4.890 2.09e-06 ***
feedbackFeedback                   0.09667    0.03272   2.954  0.00352 ** 
trainingTraining:feedbackFeedback  0.05333    0.04627   1.153  0.25047    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1636 on 196 degrees of freedom
Multiple R-squared:  0.2431,	Adjusted R-squared:  0.2315 
F-statistic: 20.99 on 3 and 196 DF,  p-value: 7.827e-12

cat_plot(M5a, pred = training, modx = feedback, data = CW, geom = "line", 
    main = "Interaction between Training and Feedback")
[1] 95
