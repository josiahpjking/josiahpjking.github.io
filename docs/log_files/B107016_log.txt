

	####### B107016 R script #######

library(psych, quietly = T)
[1] 1
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(car, quietly = T)
[1] 2
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[11] "base"     
library(lm.beta, quietly = T)
[1] 3
 [1] "lm.beta"   "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[11] "methods"   "base"     
library(interactions, quietly = T)
[1] 4
 [1] "interactions" "lm.beta"      "car"          "carData"      "psych"        "readr"        "stats"        "graphics"    
 [9] "grDevices"    "utils"        "datasets"     "methods"      "base"        
data = read.csv("~/Desktop/rms2/code_check/RMS2_report_1920.csv")
[1] 5
View(data)
[1] 6
str(data)
[1] 7
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : int  2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : int  2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
data$training = factor(data$training, labels = c("No training", 
    "Training"))
[1] 8
data$feedback = factor(data$feedback, labels = c("No feedback", 
    "Feedback"))
[1] 9
data$incidents = data$incidents * 100
[1] 10
str(data)
[1] 11
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  66.7 33.3 33.3 41.7 50 ...
 $ training : Factor w/ 2 levels "No training",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : Factor w/ 2 levels "No feedback",..: 2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
describe(data)[, c(2:4, 8, 9)]
[1] 12
            n   mean    sd  min    max
subject   200 100.50 57.88 1.00 200.00
incidents 200  47.50 18.66 8.33  91.67
training* 200   1.50  0.50 1.00   2.00
feedback* 200   1.50  0.50 1.00   2.00
empathy   200  10.18  2.54 3.00  16.00
years     200   8.13  3.08 1.00  19.00
which(is.na(data))
[1] 13
integer(0)
summary(data)[, 3:4]
[1] 14
        training          feedback  
 No training:100   No feedback:100  
 Training   :100   Feedback   :100  
                                    
                                    
                                    
                                    
describe(data)[c(2, 5:6), c(2:4, 8, 9, 11)]
[1] 15
            n  mean    sd  min   max  skew
incidents 200 47.50 18.66 8.33 91.67  0.32
empathy   200 10.18  2.54 3.00 16.00 -0.37
years     200  8.13  3.08 1.00 19.00  0.52
hist(data$incidents)
[1] 16
hist(data$empathy)
[1] 17
hist(data$years)
[1] 18
years_log = log10(data$years)
[1] 19
hist(years_log)
[1] 20
empathy_log = log(data$empathy)
[1] 21
hist(empathy_log)
[1] 22
plot(data$training, data$incidents)
[1] 23
plot(data$feedback, data$incidents)
[1] 24
scatterplot(data$empathy, data$incidents)
[1] 25
scatterplot(data$years, data$incidents)
[1] 26
m1 = lm(data$empathy ~ data$years)
[1] 27
cooks = cooks.distance(m1)
[1] 28
names(which(cooks > (4/(200 - 1 - 1))))
[1] 29
 [1] "16"  "61"  "88"  "89"  "94"  "108" "137" "141" "154" "163" "171" "175" "192" "200"
plot(m1, which = 4)
[1] 30
data[c(88, 137, 154), ]
[1] 31
    subject incidents    training    feedback empathy years
88       88  16.66667    Training No feedback       3     1
137     137  58.33333 No training    Feedback       6    18
154     154  25.00000 No training No feedback       5    19
qqPlot(m1)
[1] 32
hist(m1$residuals)
[1] 33
shapiro.test(m1$residuals)
[1] 34

	Shapiro-Wilk normality test

data:  m1$residuals
W = 0.98911, p-value = 0.1321

residualPlots(m1)
[1] 35
ncvTest(m1)
[1] 36
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.9815033, Df = 1, p = 0.32183
crPlots(m1)
[1] 37
scatterplot(data$years, data$empathy, main = "Relationship between the Empathy score and Years of experience", 
    ylab = "Empathy", xlab = "Experience")
[1] 38
m1a = lm(data$empathy ~ years_log)
[1] 39
crPlots(m1a)
[1] 40
years_mc = data$years - mean(data$years)
[1] 41
empathy_mc = data$empathy - mean(data$empathy)
[1] 42
m1b = lm(empathy_mc ~ years_mc + I(years_mc^2))
[1] 43
crPlots(m1b)
[1] 44
cooks = cooks.distance(m1b)
[1] 45
names(which(cooks > (4/(200 - 2 - 1))))
[1] 46
[1] "61"  "89"  "137" "141" "154" "163" "165" "175"
plot(m1b, which = 4)
[1] 47
data[c(89, 154, 175), ]
[1] 48
    subject incidents    training    feedback empathy years
89       89  8.333333    Training    Feedback       6    15
154     154 25.000000 No training No feedback       5    19
175     175 66.666667 No training    Feedback      15     5
qqPlot(m1b)
[1] 49
hist(m1b$residuals)
[1] 50
shapiro.test(m1b$residuals)
[1] 51

	Shapiro-Wilk normality test

data:  m1b$residuals
W = 0.9941, p-value = 0.6155

residualPlots(m1b)
[1] 52
ncvTest(m1b)
[1] 53
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
durbinWatsonTest(m1b)
[1] 54
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855   0.838
 Alternative hypothesis: rho != 0
vif(m1b)
[1] 55
     years_mc I(years_mc^2) 
     1.120963      1.120963 
cov(data$empathy, data$years, method = "spearman")
[1] 56
[1] 1012.956
spearman = cor.test(data$empathy, data$years, method = "spearman", 
    exact = F)
[1] 57
spearman


	Spearman's rank correlation rho

data:  data$empathy and data$years
S = 923838, p-value = 9.723e-06
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.3071045 



t <- 0.3071045 * sqrt((200 - 2)/(1 - (0.3071045)^2))
[1] 59
t

[1] 4.540773


critical.t <- qt(p = 1 - (0.05/2), df = 200 - 2)
[1] 61
critical.t

[1] 1.972017


summary(m1b)
[1] 63

Call:
lm(formula = empathy_mc ~ years_mc + I(years_mc^2))

Residuals:
    Min      1Q  Median      3Q     Max 
-4.5069 -1.4011 -0.0835  1.1707  6.0622 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)    0.977696   0.167423   5.840 2.13e-08 ***
years_mc       0.383236   0.047986   7.986 1.13e-13 ***
I(years_mc^2) -0.103645   0.009861 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.969 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

R = sqrt(0.404)
[1] 64
R

[1] 0.6356099


m1bs = lm(scale(empathy_mc) ~ years_mc + I(years_mc^2))
[1] 66
summary(m1bs)
[1] 67

Call:
lm(formula = scale(empathy_mc) ~ years_mc + I(years_mc^2))

Residuals:
     Min       1Q   Median       3Q      Max 
-1.77645 -0.55225 -0.03293  0.46143  2.38949 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)    0.385369   0.065991   5.840 2.13e-08 ***
years_mc       0.151057   0.018914   7.986 1.13e-13 ***
I(years_mc^2) -0.040853   0.003887 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.776 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

m2 = lm(data$incidents ~ data$training + data$empathy + data$years)
[1] 68
cooks = cooks.distance(m2)
[1] 69
names(which(cooks > (4/(200 - 3 - 1))))
[1] 70
[1] "13"  "35"  "74"  "137" "165"
plot(m2, which = 4)
[1] 71
data[c(13, 74, 137), ]
[1] 72
    subject incidents    training    feedback empathy years
13       13  91.66667    Training    Feedback       9     7
74       74  16.66667    Training No feedback      15     8
137     137  58.33333 No training    Feedback       6    18
qqPlot(m2)
[1] 73
hist(m2$residuals)
[1] 74
shapiro.test(m2$residuals)
[1] 75

	Shapiro-Wilk normality test

data:  m2$residuals
W = 0.97994, p-value = 0.005838

m2a = lm(data$incidents ~ data$training + empathy_log + years_log)
[1] 76
m2b = lm(data$incidents ~ data$training + data$empathy + data$years + 
    I(data$years^2))
[1] 77
incidents_mc = data$incidents - mean(data$incidents)
[1] 78
m2c = lm(incidents_mc ~ data$training + empathy_mc + years_mc + 
    I(years_mc^2))
[1] 79
m2d = lm(incidents ~ training + empathy + years, data = data[-c(13, 
    74, 137), ])
[1] 80
shapiro.test(m2c$residuals)
[1] 81

	Shapiro-Wilk normality test

data:  m2c$residuals
W = 0.97991, p-value = 0.005777

residualPlots(m2)
[1] 82
ncvTest(m2)
[1] 83
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
crPlots(m2)
[1] 84
durbinWatsonTest(m2)
[1] 85
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.186
 Alternative hypothesis: rho != 0
vif(m2)
[1] 86
data$training  data$empathy    data$years 
     1.020633      1.092467      1.085089 
summary(m2)
[1] 87

Call:
lm(formula = data$incidents ~ data$training + data$empathy + 
    data$years)

Residuals:
    Min      1Q  Median      3Q     Max 
-35.648 -12.270  -1.474  10.105  53.180 

Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)            40.7320     5.1814   7.861 2.48e-13 ***
data$trainingTraining -15.2427     2.3166  -6.580 4.20e-10 ***
data$empathy            2.5396     0.4735   5.363 2.29e-07 ***
data$years             -1.4085     0.3889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.21 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

m3 = lm(data$incidents ~ data$feedback + data$empathy + data$years)
[1] 88
cooks = cooks.distance(m3)
[1] 89
names(which(cooks > (4/(200 - 3 - 1))))
[1] 90
[1] "73"  "74"  "89"  "137" "156" "160" "165" "176"
plot(m3, which = 4)
[1] 91
data[c(74, 89, 137), ]
[1] 92
    subject incidents    training    feedback empathy years
74       74 16.666667    Training No feedback      15     8
89       89  8.333333    Training    Feedback       6    15
137     137 58.333333 No training    Feedback       6    18
qqPlot(m3)
[1] 93
hist(m3$residuals)
[1] 94
shapiro.test(m3$residuals)
[1] 95

	Shapiro-Wilk normality test

data:  m3$residuals
W = 0.98772, p-value = 0.08174

residualPlots(m3)
[1] 96
ncvTest(m3)
[1] 97
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.01731234, Df = 1, p = 0.89532
crPlots(m3)
[1] 98
durbinWatsonTest(m3)
[1] 99
 lag Autocorrelation D-W Statistic p-value
   1       0.2738264      1.444081       0
 Alternative hypothesis: rho != 0
data_new = data[order(runif(nrow(data))), ]
[1] 100
m3a = lm(data_new$incidents ~ data_new$feedback + data_new$empathy + 
    data_new$years)
[1] 101
cooks = cooks.distance(m3a)
[1] 102
names(which(cooks > (4/(200 - 3 - 1))))
[1] 103
[1] "10"  "95"  "132" "153" "165" "166" "170" "173"
plot(m3a, which = 4)
[1] 104
qqPlot(m3a)
[1] 105
hist(m3a$residuals)
[1] 106
shapiro.test(m3a$residuals)
[1] 107

	Shapiro-Wilk normality test

data:  m3a$residuals
W = 0.98772, p-value = 0.08174

residualPlots(m3a)
[1] 108
ncvTest(m3a)
[1] 109
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.01731234, Df = 1, p = 0.89532
crPlots(m3a)
[1] 110
durbinWatsonTest(m3a)
[1] 111
 lag Autocorrelation D-W Statistic p-value
   1    -0.001030355       2.00116   0.964
 Alternative hypothesis: rho != 0
vif(m3a)
[1] 112
data_new$feedback  data_new$empathy    data_new$years 
         1.005211          1.079397          1.077467 
summary(m3a)
[1] 113

Call:
lm(formula = data_new$incidents ~ data_new$feedback + data_new$empathy + 
    data_new$years)

Residuals:
    Min      1Q  Median      3Q     Max 
-35.580 -11.148  -1.097  11.092  53.552 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)                27.6161     5.4108   5.104 7.84e-07 ***
data_new$feedbackFeedback  13.2132     2.3585   5.602 7.09e-08 ***
data_new$empathy            2.3187     0.4829   4.802 3.11e-06 ***
data_new$years             -1.2689     0.3975  -3.192  0.00165 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.63 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

m3b = lm(data$incidents ~ data$feedback + data$training + data$empathy + 
    data$years)
[1] 114
cooks = cooks.distance(m3b)
[1] 115
names(which(cooks > (4/(200 - 4 - 1))))
[1] 116
 [1] "13"  "35"  "74"  "117" "131" "137" "141" "156" "160" "176"
plot(m3b, which = 4)
[1] 117
data[c(13, 137, 176), ]
[1] 118
    subject incidents    training    feedback empathy years
13       13  91.66667    Training    Feedback       9     7
137     137  58.33333 No training    Feedback       6    18
176     176  91.66667 No training No feedback      10    10
qqPlot(m3b)
[1] 119
hist(m3b$residuals)
[1] 120
shapiro.test(m3b$residuals)
[1] 121

	Shapiro-Wilk normality test

data:  m3b$residuals
W = 0.98734, p-value = 0.07172

residualPlots(m3b)
[1] 122
ncvTest(m3b)
[1] 123
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 3.684027, Df = 1, p = 0.054936
crPlots(m3b)
[1] 124
durbinWatsonTest(m3b)
[1] 125
 lag Autocorrelation D-W Statistic p-value
   1      0.07303546      1.841386   0.256
 Alternative hypothesis: rho != 0
vif(m3b)
[1] 126
data$feedback data$training  data$empathy    data$years 
     1.005320      1.020743      1.097109      1.087722 
summary(m3b)
[1] 127

Call:
lm(formula = data$incidents ~ data$feedback + data$training + 
    data$empathy + data$years)

Residuals:
    Min      1Q  Median      3Q     Max 
-31.626 -10.096  -0.949   9.165  46.650 

Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)            33.1953     4.8708   6.815 1.15e-10 ***
data$feedbackFeedback  13.3715     2.0968   6.377 1.28e-09 ***
data$trainingTraining -15.3824     2.1128  -7.281 7.98e-12 ***
data$empathy            2.7191     0.4328   6.283 2.12e-09 ***
data$years             -1.5199     0.3551  -4.281 2.92e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 14.79 on 195 degrees of freedom
Multiple R-squared:  0.3848,	Adjusted R-squared:  0.3721 
F-statistic: 30.49 on 4 and 195 DF,  p-value: < 2.2e-16

AIC(m3, m3a, m3b)
[1] 128
    df      AIC
m3   5 1698.109
m3a  5 1698.109
m3b  6 1652.018
BIC(m3, m3a, m3b)
[1] 129
    df      BIC
m3   5 1714.601
m3a  5 1714.601
m3b  6 1671.808
anova(m2, m3, m3b)
[1] 130
Analysis of Variance Table

Model 1: data$incidents ~ data$training + data$empathy + data$years
Model 2: data$incidents ~ data$feedback + data$empathy + data$years
Model 3: data$incidents ~ data$feedback + data$training + data$empathy + 
    data$years
  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    
1    196 51532                                  
2    196 54230  0     -2698                     
3    195 42639  1     11590 53.006 7.979e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
AIC(m2, m3, m3b)
[1] 131
    df      AIC
m2   5 1687.903
m3   5 1698.109
m3b  6 1652.018
BIC(m2, m3, m3b)
[1] 132
    df      BIC
m2   5 1704.394
m3   5 1714.601
m3b  6 1671.808
contrasts(data$training) = contr.treatment(2, base = 1)
[1] 133
contrasts(data$feedback) = contr.treatment(2, base = 1)
[1] 134
m5 = lm(incidents ~ training + feedback + training * feedback, 
    data = data)
[1] 135
m5a = lm(incidents_mc ~ training * feedback, data = data)
[1] 136
cooks = cooks.distance(m5)
[1] 137
names(which(cooks > (4/(200 - 3 - 1))))
[1] 138
 [1] "7"   "13"  "89"  "131" "150" "159" "160" "165" "176" "181"
plot(m5, which = 4)
[1] 139
data[c(13, 160, 176), ]
[1] 140
    subject incidents    training    feedback empathy years
13       13  91.66667    Training    Feedback       9     7
160     160  91.66667 No training No feedback      12     6
176     176  91.66667 No training No feedback      10    10
summary(m5)
[1] 141

Call:
lm(formula = incidents ~ training + feedback + training * feedback, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-40.000 -10.500   0.000   8.333  43.333 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)           49.333      2.314  21.323  < 2e-16 ***
training2            -16.000      3.272  -4.890 2.09e-06 ***
feedback2              9.667      3.272   2.954  0.00352 ** 
training2:feedback2    5.333      4.627   1.153  0.25047    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.36 on 196 degrees of freedom
Multiple R-squared:  0.2431,	Adjusted R-squared:  0.2315 
F-statistic: 20.99 on 3 and 196 DF,  p-value: 7.827e-12

summary(m5a)
[1] 142

Call:
lm(formula = incidents_mc ~ training * feedback, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-40.000 -10.500   0.000   8.333  43.333 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)            1.833      2.314   0.792  0.42907    
training2            -16.000      3.272  -4.890 2.09e-06 ***
feedback2              9.667      3.272   2.954  0.00352 ** 
training2:feedback2    5.333      4.627   1.153  0.25047    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.36 on 196 degrees of freedom
Multiple R-squared:  0.2431,	Adjusted R-squared:  0.2315 
F-statistic: 20.99 on 3 and 196 DF,  p-value: 7.827e-12

describeBy(data$incidents, group = list(data$training, data$feedback), 
    mat = T)
[1] 143
    item      group1      group2 vars  n     mean       sd   median  trimmed    mad       min      max    range       skew
X11    1 No training No feedback    1 50 49.33333 16.73794 41.66667 47.70833 12.355 25.000000 91.66667 66.66667 0.81813067
X12    2    Training No feedback    1 50 33.33333 13.98493 33.33333 32.70833 12.355  8.333333 58.33333 50.00000 0.15233791
X13    3 No training    Feedback    1 50 59.00000 17.07327 58.33333 58.75000 12.355 25.000000 91.66667 66.66667 0.04793352
      kurtosis       se
X11 -0.0698501 2.367103
X12 -0.9575614 1.977768
X13 -0.8075298 2.414525
 [ reached 'max' / getOption("max.print") -- omitted 1 rows ]
cat_plot(m5, pred = training, modx = feedback, geom = "line", 
    plot.points = F, main.title = "Interactions plot between Incidents and Feedback")
[1] 144
