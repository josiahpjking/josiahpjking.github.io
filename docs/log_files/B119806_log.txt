

	####### B119806 R script #######

install.packages("psych")
[1] 1
install.packages("car")
[1] 2
install.packages("interactions")
[1] 3
library(psych, quietly = T)
[1] 4
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(car, quietly = T)
[1] 5
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[10] "methods"   "base"     
library(interactions, quietly = T)
[1] 6
 [1] "interactions" "car"          "carData"      "psych"        "readr"        "stats"        "graphics"    
 [8] "grDevices"    "utils"        "datasets"     "methods"      "base"        
library(readr, quietly = T)
[1] 7
 [1] "interactions" "car"          "carData"      "psych"        "readr"        "stats"        "graphics"    
 [8] "grDevices"    "utils"        "datasets"     "methods"      "base"        
data <- read_csv("../RMS2_report_1920.csv")
[1] 8
str(data)
[1] 9
Classes ‘spec_tbl_df’, ‘tbl_df’, ‘tbl’ and 'data.frame':	200 obs. of  6 variables:
 $ subject  : num  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : num  2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : num  2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : num  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : num  10 7 5 7 12 10 8 10 7 9 ...
 - attr(*, "spec")=
  .. cols(
  ..   subject = col_double(),
  ..   incidents = col_double(),
  ..   training = col_double(),
  ..   feedback = col_double(),
  ..   empathy = col_double(),
  ..   years = col_double()
  .. )
NULL
describe(data)[, c(2:4, 8, 9, 11, 12)]
[1] 10
            n   mean    sd  min    max  skew kurtosis
subject   200 100.50 57.88 1.00 200.00  0.00    -1.22
incidents 200   0.48  0.19 0.08   0.92  0.32    -0.31
training  200   1.50  0.50 1.00   2.00  0.00    -2.01
feedback  200   1.50  0.50 1.00   2.00  0.00    -2.01
empathy   200  10.18  2.54 3.00  16.00 -0.37     0.23
years     200   8.13  3.08 1.00  19.00  0.52     0.48
data$training <- factor(data$training, labels = c("no training", 
    "training"))
[1] 11
data$feedback <- factor(data$feedback, labels = c("no feedback", 
    "feedback"))
[1] 12
summary(data[, c(3:4)])
[1] 13
        training          feedback  
 no training:100   no feedback:100  
 training   :100   feedback   :100  
data$years_m <- data$years - mean(data$years)
[1] 14
M1 <- lm(empathy ~ years_m, data = data)
[1] 15
cooks <- cooks.distance(M1)
[1] 16
names(which(cooks > (4/(200 - 1 - 1))))
[1] 17
 [1] "16"  "61"  "88"  "89"  "94"  "108" "137" "141" "154" "163" "171" "175" "192" "200"
plot(M1, which = 4)
[1] 18
stdz <- MASS::studres(M1)
[1] 19
names(which(abs(stdz) > 2))
[1] 20
 [1] "61"  "68"  "70"  "88"  "89"  "137" "141" "142" "154" "175" "200"
hats <- hatvalues(M1)
[1] 21
names(which(hats > 2 * mean(hats)))
[1] 22
 [1] "61"  "88"  "89"  "92"  "94"  "108" "109" "116" "131" "137" "140" "141" "154" "155" "163" "190" "200"
plot(data$empathy ~ data$years_m)
[1] 23
abline(lm(empathy ~ years_m, data = data))
[1] 24
lines(lowess(data$years_m, data$empathy))
[1] 25
M1a <- lm(empathy ~ years_m + I(years_m^2), data = data)
[1] 26
plot(M1a, which = 1)
[1] 27
hist(M1a$residuals)
[1] 28
qqPlot(M1a)
[1] 29
shapiro.test(M1a$residuals)
[1] 30

	Shapiro-Wilk normality test

data:  M1a$residuals
W = 0.9941, p-value = 0.6155

residualPlots(M1a)
[1] 31
ncvTest(M1a)
[1] 32
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
durbinWatsonTest(M1a)
[1] 33
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855   0.838
 Alternative hypothesis: rho != 0
summary(M1a)
[1] 34

Call:
lm(formula = empathy ~ years_m + I(years_m^2), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.5069 -1.4011 -0.0835  1.1707  6.0622 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  11.152696   0.167423  66.614  < 2e-16 ***
years_m       0.383236   0.047986   7.986 1.13e-13 ***
I(years_m^2) -0.103645   0.009861 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.969 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

data$empathy_m <- data$empathy - mean(data$empathy)
[1] 35
M2 <- lm(incidents ~ empathy_m + years_m + training, data = data)
[1] 36
cooks2 <- cooks.distance(M2)
[1] 37
names(which(cooks2 > 4/(200 - 3 - 1)))
[1] 38
[1] "13"  "35"  "74"  "137" "165"
plot(M2, which = 4)
[1] 39
hist(M2$residuals)
[1] 40
qqPlot(M2)
[1] 41
shapiro.test(M2$residuals)
[1] 42

	Shapiro-Wilk normality test

data:  M2$residuals
W = 0.97994, p-value = 0.005838

data$incidents_log <- log(data$incidents + 0.91667)
[1] 43
M2a <- lm(incidents_log ~ empathy_m + years_m + training, data = data)
[1] 44
hist(M2a$residuals)
[1] 45
qqPlot(M2a)
[1] 46
shapiro.test(M2a$residuals)
[1] 47

	Shapiro-Wilk normality test

data:  M2a$residuals
W = 0.9905, p-value = 0.2109

residualPlots(M2a)
[1] 48
ncvTest(M2a)
[1] 49
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 1.036052, Df = 1, p = 0.30874
crPlots(M2a)
[1] 50
durbinWatsonTest(M2a)
[1] 51
 lag Autocorrelation D-W Statistic p-value
   1       -0.116516      2.217742   0.166
 Alternative hypothesis: rho != 0
vif(M2a)
[1] 52
empathy_m   years_m  training 
 1.092467  1.085089  1.020633 
summary(M2a)
[1] 53

Call:
lm(formula = incidents_log ~ empathy_m + years_m + training, 
    data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.27676 -0.08678 -0.00271  0.07438  0.35035 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.377252   0.011606  32.505  < 2e-16 ***
empathy_m         0.018550   0.003372   5.501 1.17e-07 ***
years_m          -0.010283   0.002769  -3.713 0.000266 ***
trainingtraining -0.111288   0.016497  -6.746 1.67e-10 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1155 on 196 degrees of freedom
Multiple R-squared:  0.2661,	Adjusted R-squared:  0.2549 
F-statistic: 23.69 on 3 and 196 DF,  p-value: 3.963e-13

M3 <- lm(incidents ~ empathy_m + years_m + training + feedback, 
    data = data)
[1] 54
cooks_3 <- cooks.distance(M3)
[1] 55
names(which(cooks_3 > (4/(200 - 4 - 1))))
[1] 56
 [1] "13"  "35"  "74"  "117" "131" "137" "141" "156" "160" "176"
plot(M3, which = 4)
[1] 57
hist(M3$residuals)
[1] 58
qqPlot(M3)
[1] 59
shapiro.test(M3$residuals)
[1] 60

	Shapiro-Wilk normality test

data:  M3$residuals
W = 0.98734, p-value = 0.07172

residualPlots(M3)
[1] 61
ncvTest(M3)
[1] 62
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 3.684027, Df = 1, p = 0.054936
crPlots(M3)
[1] 63
durbinWatsonTest(M3)
[1] 64
 lag Autocorrelation D-W Statistic p-value
   1      0.07303546      1.841386   0.282
 Alternative hypothesis: rho != 0
vif(M3)
[1] 65
empathy_m   years_m  training  feedback 
 1.097109  1.087722  1.020743  1.005320 
summary(M3)
[1] 66

Call:
lm(formula = incidents ~ empathy_m + years_m + training + feedback, 
    data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.31626 -0.10096 -0.00949  0.09165  0.46650 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.485055   0.018126  26.760  < 2e-16 ***
empathy_m         0.027191   0.004328   6.283 2.12e-09 ***
years_m          -0.015199   0.003551  -4.281 2.92e-05 ***
trainingtraining -0.153824   0.021128  -7.281 7.98e-12 ***
feedbackfeedback  0.133715   0.020968   6.377 1.28e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1479 on 195 degrees of freedom
Multiple R-squared:  0.3848,	Adjusted R-squared:  0.3721 
F-statistic: 30.49 on 4 and 195 DF,  p-value: < 2.2e-16

M4 <- lm(incidents ~ empathy_m + years_m + training + feedback + 
    training:feedback, data = data)
[1] 67
summary(M4)
[1] 68

Call:
lm(formula = incidents ~ empathy_m + years_m + training + feedback + 
    training:feedback, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30367 -0.10365 -0.00795  0.08806  0.44359 

Coefficients:
                                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.509636   0.020807  24.494  < 2e-16 ***
empathy_m                          0.028695   0.004328   6.630 3.25e-10 ***
years_m                           -0.015720   0.003518  -4.468 1.34e-05 ***
trainingtraining                  -0.203496   0.029874  -6.812 1.18e-10 ***
feedbackfeedback                   0.085558   0.029299   2.920  0.00391 ** 
trainingtraining:feedbackfeedback  0.097334   0.041840   2.326  0.02103 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1462 on 194 degrees of freedom
Multiple R-squared:  0.4015,	Adjusted R-squared:  0.386 
F-statistic: 26.02 on 5 and 194 DF,  p-value: < 2.2e-16

contrasts(data$training)
[1] 69
            training
no training        0
training           1
contrasts(data$feedback)
[1] 70
            feedback
no feedback        0
feedback           1
cooks_4 <- cooks.distance(M5)
[1] 71


####################################################################################################

[1] "Error in cooks.distance(M5) : object 'M5' not found\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in cooks.distance(M5): object 'M5' not found>
####################################################################################################


names(which(cooks_4 > (4/(200 - 5 - 1))))
[1] 72
[1] "Error in which(cooks_4 > (4/(200 - 5 - 1))) : object 'cooks_4' not found\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in which(cooks_4 > (4/(200 - 5 - 1))): object 'cooks_4' not found>


####################################################################################################

[1] "Error in which(cooks_4 > (4/(200 - 5 - 1))) : object 'cooks_4' not found\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in which(cooks_4 > (4/(200 - 5 - 1))): object 'cooks_4' not found>
####################################################################################################


plot(M4, which = 4)
[1] 73
residualPlots(M4)
[1] 74
ncvTest(M4)
[1] 75
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 4.398557, Df = 1, p = 0.035969
M4a <- lm(incidents_log ~ empathy_m + years_m + training + feedback + 
    training:feedback, data = data)
[1] 76
ncvTest(M4a)
[1] 77
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5773005, Df = 1, p = 0.44737
hist(M4a$residuals)
[1] 78
qqPlot(M4a)
[1] 79
shapiro.test(M4a$residuals)
[1] 80

	Shapiro-Wilk normality test

data:  M4a$residuals
W = 0.99434, p-value = 0.6515

durbinWatsonTest(M4a)
[1] 81
 lag Autocorrelation D-W Statistic p-value
   1      0.09726586      1.794988   0.144
 Alternative hypothesis: rho != 0
vif(M4a)
[1] 82
        empathy_m           years_m          training          feedback training:feedback 
         1.122147          1.092150          2.086826          2.007327          3.070068 
summary(M4a)
[1] 83

Call:
lm(formula = incidents_log ~ empathy_m + years_m + training + 
    feedback + training:feedback, data = data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.226383 -0.071808 -0.000811  0.069089  0.284821 

Coefficients:
                                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.348970   0.014712  23.720  < 2e-16 ***
empathy_m                          0.021048   0.003060   6.878 8.14e-11 ***
years_m                           -0.011505   0.002488  -4.625 6.84e-06 ***
trainingtraining                  -0.151902   0.021123  -7.191 1.36e-11 ***
feedbackfeedback                   0.058376   0.020716   2.818  0.00533 ** 
trainingtraining:feedbackfeedback  0.077603   0.029583   2.623  0.00940 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1034 on 194 degrees of freedom
Multiple R-squared:  0.4176,	Adjusted R-squared:  0.4026 
F-statistic: 27.82 on 5 and 194 DF,  p-value: < 2.2e-16

cat_plot(M4a, pred = training, modx = feedback, data = data, 
    geom = c("line"))
[1] 84
