

	####### B095141 R script #######

data <- read.csv("../RMS2_report_1920.csv")
[1] 1
summary(data)
[1] 2
    subject         incidents          training      feedback      empathy          years      
 Min.   :  1.00   Min.   :0.08333   Min.   :1.0   Min.   :1.0   Min.   : 3.00   Min.   : 1.00  
 1st Qu.: 50.75   1st Qu.:0.33333   1st Qu.:1.0   1st Qu.:1.0   1st Qu.: 9.00   1st Qu.: 6.00  
 Median :100.50   Median :0.41667   Median :1.5   Median :1.5   Median :10.00   Median : 8.00  
 Mean   :100.50   Mean   :0.47500   Mean   :1.5   Mean   :1.5   Mean   :10.18   Mean   : 8.13  
 3rd Qu.:150.25   3rd Qu.:0.58333   3rd Qu.:2.0   3rd Qu.:2.0   3rd Qu.:12.00   3rd Qu.:10.00  
 Max.   :200.00   Max.   :0.91667   Max.   :2.0   Max.   :2.0   Max.   :16.00   Max.   :19.00  
library(carData, quietly = T)
[1] 3
[1] "carData"   "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(psych, quietly = T)
[1] 4
 [1] "psych"     "carData"   "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[10] "base"     
library(car, quietly = T)
[1] 5
 [1] "car"       "psych"     "carData"   "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[10] "methods"   "base"     
View(data)
[1] 6
str(data)
[1] 7
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : int  2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : int  2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
data$training <- as.factor(data$training)
[1] 8
View(data$training)
[1] 9
str(data$training)
[1] 10
 Factor w/ 2 levels "1","2": 2 2 2 2 2 2 2 2 2 2 ...
NULL
data$training <- factor(data$training, labels = c("no training", 
    "training"))
[1] 11
summary(data$training)
[1] 12
no training    training 
        100         100 
data$feedback <- as.factor(data$feedback)
[1] 13
View(data$feedback)
[1] 14
str(data$feedback)
[1] 15
 Factor w/ 2 levels "1","2": 2 1 2 1 2 1 2 1 2 1 ...
NULL
data$feedback <- factor(data$feedback, labels = c("no feedback", 
    "feedback"))
[1] 16
summary(data$feedback)
[1] 17
no feedback    feedback 
        100         100 
describe(data)
[1] 18
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training*    3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
hist(data$incidents)
[1] 19
which(is.na(data))
[1] 20
integer(0)
describe(data)
[1] 21
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training*    3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
datamodel1 <- lm(empathy ~ scale(years), data = data)
[1] 22
summary(datamodel1)
[1] 23

Call:
lm(formula = empathy ~ scale(years), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.5399 -1.5160  0.0708  1.6357  5.8533 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)   10.1750     0.1735  58.657  < 2e-16 ***
scale(years)   0.6699     0.1739   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.453 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

plot(scale(data$years), data$empathy, main = "scatterplot of experience and empathy", 
    ylab = "empathy", xlab = "experience (years)", pch = 16)
[1] 24
abline(datamodel1, col = "green")
[1] 25
scatterplot(data$years, data$empathy, main = "scatterplot model 1")
[1] 26
durbinWatsonTest(datamodel1)
[1] 27
 lag Autocorrelation D-W Statistic p-value
   1       0.1017162      1.769879   0.102
 Alternative hypothesis: rho != 0
plot(cooks.distance(datamodel1))
[1] 28
abline(h = cooks.distance(datamodel1) > (4/(200 - 1 - 1)), col = "red")
[1] 29
which(cooks.distance(datamodel1) > (4/(200 - 1 - 1)))
[1] 30
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
stdz <- MASS::studres(datamodel1)
[1] 31
which(abs(stdz) > 2)
[1] 32
 61  68  70  88  89 137 141 142 154 175 200 
 61  68  70  88  89 137 141 142 154 175 200 
hats <- hatvalues(datamodel1)
[1] 33
head(hats, 3)
[1] 34
          1           2           3 
0.006853526 0.005676819 0.010192832 
which(hats > 2 * mean(hats))
[1] 35
 61  88  89  92  94 108 109 116 131 137 140 141 154 155 163 190 200 
 61  88  89  92  94 108 109 116 131 137 140 141 154 155 163 190 200 
hist(datamodel1$residuals, main = "histogram model 1")
[1] 36
qqPlot(datamodel1, main = "model 1 qq plot")
[1] 37
shapiro.test(datamodel1$residuals)
[1] 38

	Shapiro-Wilk normality test

data:  datamodel1$residuals
W = 0.98911, p-value = 0.1321

residualPlots(datamodel1)
[1] 39
ncvTest(datamodel1)
[1] 40
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.9815033, Df = 1, p = 0.32183
residualPlots(datamodel1)
[1] 41
crPlots(datamodel1)
[1] 42
durbinWatsonTest(datamodel1)
[1] 43
 lag Autocorrelation D-W Statistic p-value
   1       0.1017162      1.769879   0.112
 Alternative hypothesis: rho != 0
yearspoly <- poly(scale(data$years), 2, raw = TRUE)
[1] 44
datamodel1poly <- lm(empathy ~ yearspoly, data = data)
[1] 45
summary(datamodel1poly)
[1] 46

Call:
lm(formula = empathy ~ yearspoly, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.5069 -1.4011 -0.0835  1.1707  6.0622 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 11.15270    0.16742  66.614  < 2e-16 ***
yearspoly1   1.18000    0.14775   7.986 1.13e-13 ***
yearspoly2  -0.98261    0.09349 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.969 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

plot(cooks.distance(datamodel1))
[1] 47
abline(h = cooks.distance(datamodel1) > (4/(200 - 2 - 1)), col = "red")
[1] 48
which(cooks.distance(datamodel1) > (4/(200 - 2 - 1)))
[1] 49
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
hist(datamodel1poly$residuals, main = "histogram model 1 poly")
[1] 50
qqPlot(datamodel1poly)
[1] 51
shapiro.test(datamodel1poly$residuals)
[1] 52

	Shapiro-Wilk normality test

data:  datamodel1poly$residuals
W = 0.9941, p-value = 0.6155

residualPlots(datamodel1poly)
[1] 53
ncvTest(datamodel1poly)
[1] 54
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
crPlots(datamodel1poly)
[1] 55
durbinWatsonTest(datamodel1poly)
[1] 56
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855   0.884
 Alternative hypothesis: rho != 0
describe(data)
[1] 57
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training*    3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
summary(data)
[1] 58
    subject         incidents              training          feedback      empathy          years      
 Min.   :  1.00   Min.   :0.08333   no training:100   no feedback:100   Min.   : 3.00   Min.   : 1.00  
 1st Qu.: 50.75   1st Qu.:0.33333   training   :100   feedback   :100   1st Qu.: 9.00   1st Qu.: 6.00  
 Median :100.50   Median :0.41667                                       Median :10.00   Median : 8.00  
 Mean   :100.50   Mean   :0.47500                                       Mean   :10.18   Mean   : 8.13  
 3rd Qu.:150.25   3rd Qu.:0.58333                                       3rd Qu.:12.00   3rd Qu.:10.00  
 Max.   :200.00   Max.   :0.91667                                       Max.   :16.00   Max.   :19.00  
boxplot(incidents ~ training, data = data, main = "boxplot model 2")
[1] 59
datamodel2 <- lm(incidents ~ training + scale(empathy) + scale(years), 
    data = data)
[1] 60
summary(datamodel2)
[1] 61

Call:
lm(formula = incidents ~ training + scale(empathy) + scale(years), 
    data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.55121    0.01630  33.821  < 2e-16 ***
trainingtraining -0.15243    0.02317  -6.580 4.20e-10 ***
scale(empathy)    0.06443    0.01201   5.363 2.29e-07 ***
scale(years)     -0.04337    0.01197  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

plot(cooks.distance(datamodel2))
[1] 62
abline(h = cooks.distance(datamodel2) > (4/(200 - 3 - 1)), col = "red")
[1] 63
which(cooks.distance(datamodel1) > (4/(200 - 3 - 1)))
[1] 64
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
stdz2 <- MASS::studres(datamodel2)
[1] 65
which(abs(stdz2) > 2)
[1] 66
  7  13  35  74 159 165 176 181 
  7  13  35  74 159 165 176 181 
hats <- hatvalues(datamodel2)
[1] 67
head(hats, 3)
[1] 68
         1          2          3 
0.01228875 0.01050612 0.01460984 
which(hats > 2 * mean(hats))
[1] 69
 61  88  89  92  94 137 141 154 163 175 200 
 61  88  89  92  94 137 141 154 163 175 200 
hist(datamodel2$residuals, main = "histogram model 2")
[1] 70
qqPlot(datamodel2, main = "model 2 qq plot")
[1] 71
shapiro.test(datamodel2$residuals)
[1] 72

	Shapiro-Wilk normality test

data:  datamodel2$residuals
W = 0.97994, p-value = 0.005838

residualPlots(datamodel2)
[1] 73
ncvTest(datamodel2)
[1] 74
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
crPlots(datamodel2)
[1] 75
durbinWatsonTest(datamodel2)
[1] 76
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897    0.17
 Alternative hypothesis: rho != 0
vif(datamodel2)
[1] 77
      training scale(empathy)   scale(years) 
      1.020633       1.092467       1.085089 
describe(data)
[1] 78
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training*    3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
summary(data)
[1] 79
    subject         incidents              training          feedback      empathy          years      
 Min.   :  1.00   Min.   :0.08333   no training:100   no feedback:100   Min.   : 3.00   Min.   : 1.00  
 1st Qu.: 50.75   1st Qu.:0.33333   training   :100   feedback   :100   1st Qu.: 9.00   1st Qu.: 6.00  
 Median :100.50   Median :0.41667                                       Median :10.00   Median : 8.00  
 Mean   :100.50   Mean   :0.47500                                       Mean   :10.18   Mean   : 8.13  
 3rd Qu.:150.25   3rd Qu.:0.58333                                       3rd Qu.:12.00   3rd Qu.:10.00  
 Max.   :200.00   Max.   :0.91667                                       Max.   :16.00   Max.   :19.00  
boxplot(incidents ~ feedback, data = data)
[1] 80
datamodel3 <- lm(incidents ~ feedback + training + scale(empathy) + 
    scale(years), data = data)
[1] 81
summary(datamodel3)
[1] 82

Call:
lm(formula = incidents ~ feedback + training + scale(empathy) + 
    scale(years), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.31626 -0.10096 -0.00949  0.09165  0.46650 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.48505    0.01813  26.760  < 2e-16 ***
feedbackfeedback  0.13371    0.02097   6.377 1.28e-09 ***
trainingtraining -0.15382    0.02113  -7.281 7.98e-12 ***
scale(empathy)    0.06898    0.01098   6.283 2.12e-09 ***
scale(years)     -0.04680    0.01093  -4.281 2.92e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1479 on 195 degrees of freedom
Multiple R-squared:  0.3848,	Adjusted R-squared:  0.3721 
F-statistic: 30.49 on 4 and 195 DF,  p-value: < 2.2e-16

plot(cooks.distance(datamodel3))
[1] 83
abline(h = cooks.distance(datamodel2) > (4/(200 - 4 - 1)), col = "red")
[1] 84
which(cooks.distance(datamodel1) > (4/(200 - 3 - 1)))
[1] 85
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
stdz3 <- MASS::studres(datamodel3)
[1] 86
which(abs(stdz3) > 2)
[1] 87
 13  74 101 117 150 156 159 160 176 181 
 13  74 101 117 150 156 159 160 176 181 
hats3 <- hatvalues(datamodel3)
[1] 88
head(hats3, 3)
[1] 89
         1          2          3 
0.01712163 0.01549576 0.02001541 
which(hats3 > 2 * mean(hats3))
[1] 90
 61  88  89  92 137 141 154 175 200 
 61  88  89  92 137 141 154 175 200 
hist(datamodel3$residuals)
[1] 91
qqPlot(datamodel3, main = " model 3 qq plot")
[1] 92
shapiro.test(datamodel3$residuals)
[1] 93

	Shapiro-Wilk normality test

data:  datamodel3$residuals
W = 0.98734, p-value = 0.07172

residualPlots(datamodel3)
[1] 94
ncvTest(datamodel3)
[1] 95
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 3.684027, Df = 1, p = 0.054936
crPlots(datamodel3, main = "linearity model 3")
[1] 96
durbinWatsonTest(datamodel3)
[1] 97
 lag Autocorrelation D-W Statistic p-value
   1      0.07303546      1.841386    0.26
 Alternative hypothesis: rho != 0
vif(datamodel3)
[1] 98
      feedback       training scale(empathy)   scale(years) 
      1.005320       1.020743       1.097109       1.087722 
describeBy(data$incidents, group = list(data$training, data$feedback), 
    mat = T)
[1] 99
    item      group1      group2 vars  n      mean        sd    median   trimmed     mad        min       max
X11    1 no training no feedback    1 50 0.4933333 0.1673794 0.4166667 0.4770833 0.12355 0.25000000 0.9166667
X12    2    training no feedback    1 50 0.3333333 0.1398493 0.3333333 0.3270833 0.12355 0.08333333 0.5833333
X13    3 no training    feedback    1 50 0.5900000 0.1707327 0.5833333 0.5875000 0.12355 0.25000000 0.9166667
        range       skew   kurtosis         se
X11 0.6666667 0.81813067 -0.0698501 0.02367103
X12 0.5000000 0.15233791 -0.9575614 0.01977768
X13 0.6666667 0.04793352 -0.8075298 0.02414525
 [ reached 'max' / getOption("max.print") -- omitted 1 rows ]
datamodel5 <- lm(incidents ~ scale(years) + scale(empathy) + 
    training + feedback + training * feedback, data = data)
[1] 100
summary(datamodel5)
[1] 101

Call:
lm(formula = incidents ~ scale(years) + scale(empathy) + training + 
    feedback + training * feedback, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30367 -0.10365 -0.00795  0.08806  0.44359 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.50964    0.02081  24.494  < 2e-16 ***
scale(years)                      -0.04840    0.01083  -4.468 1.34e-05 ***
scale(empathy)                     0.07280    0.01098   6.630 3.25e-10 ***
trainingtraining                  -0.20350    0.02987  -6.812 1.18e-10 ***
feedbackfeedback                   0.08556    0.02930   2.920  0.00391 ** 
trainingtraining:feedbackfeedback  0.09733    0.04184   2.326  0.02103 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1462 on 194 degrees of freedom
Multiple R-squared:  0.4015,	Adjusted R-squared:  0.386 
F-statistic: 26.02 on 5 and 194 DF,  p-value: < 2.2e-16

plot(cooks.distance(datamodel5))
[1] 102
abline(h = cooks.distance(datamodel2) > (4/(200 - 5 - 1)), col = "red")
[1] 103
which(cooks.distance(datamodel1) > (4/(200 - 5 - 1)))
[1] 104
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
 16  61  88  89  94 108 137 141 154 163 171 175 192 200 
hist(datamodel5$residuals, main = "histogram model 5")
[1] 105
qqPlot(datamodel5, main = " model 5 qq plot")
[1] 106
shapiro.test(datamodel5$residuals)
[1] 107

	Shapiro-Wilk normality test

data:  datamodel5$residuals
W = 0.98794, p-value = 0.08817

residualPlots(datamodel5)
[1] 108
ncvTest(datamodel5)
[1] 109
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 4.398557, Df = 1, p = 0.035969
durbinWatsonTest(datamodel5)
[1] 110
 lag Autocorrelation D-W Statistic p-value
   1      0.09456969      1.801048   0.168
 Alternative hypothesis: rho != 0
vif(datamodel5)
[1] 111
     scale(years)    scale(empathy)          training          feedback training:feedback 
         1.092150          1.122147          2.086826          2.007327          3.070068 
install.packages("emmeans")
[1] 112
install.packages("ggplot2")
[1] 113
library("emmeans", quietly = T)
[1] 114
 [1] "emmeans"   "car"       "psych"     "carData"   "readr"     "stats"     "graphics"  "grDevices" "utils"    
[10] "datasets"  "methods"   "base"     
dm5plot <- emmeans(datamodel5, ~training * feedback)
[1] 115
contrast(dm5plot, "revpairwise", by = "feedback", adjust = "none")
[1] 116
feedback = no feedback:
 contrast               estimate     SE  df t.ratio p.value
 training - no training   -0.203 0.0299 194 -6.812  <.0001 

feedback = feedback:
 contrast               estimate     SE  df t.ratio p.value
 training - no training   -0.106 0.0293 194 -3.628  0.0004 

emmip(dm5plot, training ~ feedback, CIs = TRUE)
[1] 117
q5plot <- emmip(dm5plot, training ~ feedback, CIs = TRUE, plotit = FALSE)
[1] 118
summary(datamodel5)
[1] 119

Call:
lm(formula = incidents ~ scale(years) + scale(empathy) + training + 
    feedback + training * feedback, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30367 -0.10365 -0.00795  0.08806  0.44359 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.50964    0.02081  24.494  < 2e-16 ***
scale(years)                      -0.04840    0.01083  -4.468 1.34e-05 ***
scale(empathy)                     0.07280    0.01098   6.630 3.25e-10 ***
trainingtraining                  -0.20350    0.02987  -6.812 1.18e-10 ***
feedbackfeedback                   0.08556    0.02930   2.920  0.00391 ** 
trainingtraining:feedbackfeedback  0.09733    0.04184   2.326  0.02103 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1462 on 194 degrees of freedom
Multiple R-squared:  0.4015,	Adjusted R-squared:  0.386 
F-statistic: 26.02 on 5 and 194 DF,  p-value: < 2.2e-16

