

	####### B123510 R script #######

install.packages("psych")
[1] 1
install.packages("car")
[1] 2
install.packages("ggplots2")
[1] 3
library(psych, quietly = T)
[1] 4
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
library(car, quietly = T)
[1] 5
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[10] "methods"   "base"     
library(ggplot2, quietly = T)
[1] 6
 [1] "ggplot2"   "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"    
[10] "datasets"  "methods"   "base"     
data <- read.csv("../RMS2_report_1920.csv", header = T)
[1] 7
data

  subject incidents training feedback empathy years
1       1 0.6666667        2        2      11    10
2       2 0.3333333        2        1      10     7
3       3 0.3333333        2        2      10     5
4       4 0.4166667        2        1       9     7
5       5 0.5000000        2        2      12    12
6       6 0.2500000        2        1      11    10
7       7 0.8333333        2        2      13     8
8       8 0.4166667        2        1      14    10
 [ reached 'max' / getOption("max.print") -- omitted 192 rows ]


describe(data)
[1] 9
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training     3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
str(data)
[1] 10
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : int  2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : int  2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
sum(is.na(data))
[1] 11
[1] 0
sum(is.nan(as.matrix(data)))
[1] 12
[1] 0
data$training <- factor(data$training, levels = c("1", "2"), 
    labels = c("no training", "training"))
[1] 13
data$feedback <- factor(data$feedback, levels = c("1", "2"), 
    labels = c("no feedback", "feedback"))
[1] 14
data$training
[1] 15
 [1] training training training training training training training training training training training training
[13] training training training training training training training training training training training training
[25] training training training training training training training training training training training training
[37] training training training training training training training training training training training training
[49] training training
 [ reached getOption("max.print") -- omitted 150 entries ]
Levels: no training training
data$feedback
[1] 16
 [1] feedback    no feedback feedback    no feedback feedback    no feedback feedback    no feedback feedback   
[10] no feedback feedback    no feedback feedback    no feedback feedback    no feedback feedback    no feedback
[19] feedback    no feedback feedback    no feedback feedback    no feedback feedback    no feedback feedback   
[28] no feedback feedback    no feedback feedback    no feedback feedback    no feedback feedback    no feedback
[37] feedback    no feedback feedback    no feedback feedback    no feedback feedback    no feedback feedback   
[46] no feedback feedback    no feedback feedback    no feedback
 [ reached getOption("max.print") -- omitted 150 entries ]
Levels: no feedback feedback
table(data$training, data$feedback)
[1] 17
             
              no feedback feedback
  no training          50       50
  training             50       50
M1 <- lm(scale(empathy) ~ years, data = data)
[1] 18
summary(M1)
[1] 19

Call:
lm(formula = scale(empathy) ~ years, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.97192 -0.59754  0.02792  0.64474  2.30713 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.69717    0.19347  -3.603 0.000397 ***
years        0.08575    0.02226   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.9669 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

plot(data$years, data$empathy, xlab = "Empathy (1=Low, 20=High)", 
    ylab = "Experience (years)", main = "Relationship between Empathy and Experience", 
    pch = 16)
[1] 20
abline(coefficients(M1))
[1] 21
plot(M1, which = 1)
[1] 22
M1a <- lm(scale(empathy) ~ years + I(years^2), data = data)
[1] 23
summary(M1a)
[1] 24

Call:
lm(formula = scale(empathy) ~ years + I(years^2), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.77645 -0.55225 -0.03293  0.46143  2.38949 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -3.542969   0.312115  -11.35   <2e-16 ***
years        0.815324   0.071676   11.38   <2e-16 ***
I(years^2)  -0.040853   0.003887  -10.51   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.776 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

cooks <- cooks.distance
[1] 25
which(cooks > (4/(200 - 1 - 1)))
[1] 26
[1] "Error in cooks > (4/(200 - 1 - 1)) : \n  comparison (6) is possible only for atomic and list types\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in cooks > (4/(200 - 1 - 1)): comparison (6) is possible only for atomic and list types>


####################################################################################################

[1] "Error in cooks > (4/(200 - 1 - 1)) : \n  comparison (6) is possible only for atomic and list types\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in cooks > (4/(200 - 1 - 1)): comparison (6) is possible only for atomic and list types>
####################################################################################################


plot(M1, which = 4)
[1] 27
abline(h = cooks.distance(M1) > (4/(200 - 1 - 1)))
[1] 28
plot(M1, which = 5)
[1] 29
qqPlot(M1a)
[1] 30
shapiro.test(M1a$residuals)
[1] 31

	Shapiro-Wilk normality test

data:  M1a$residuals
W = 0.9941, p-value = 0.6155

hist(M1a$residuals, col = "pink")
[1] 32
residualPlots(M1a)
[1] 33
ncvTest(M1a)
[1] 34
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
crPlots(M1a)
[1] 35
durbinWatsonTest(M1a)
[1] 36
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855   0.874
 Alternative hypothesis: rho != 0
M2 <- lm(incidents ~ training + scale(empathy) + years, data = data)
[1] 37
summary(M2)
[1] 38

Call:
lm(formula = incidents ~ training + scale(empathy) + years, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.665721   0.036551  18.214  < 2e-16 ***
trainingtraining -0.152427   0.023166  -6.580 4.20e-10 ***
scale(empathy)    0.064430   0.012014   5.363 2.29e-07 ***
years            -0.014085   0.003889  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

plot(M2, which = 1)
[1] 39
cooks <- cooks.distance(M2)
[1] 40
which(cooks > (4/(200 - 3 - 1)))
[1] 41
 13  35  74 137 165 
 13  35  74 137 165 
plot(M2, which = 4)
[1] 42
abline(h = cooks.distance(M2) > (4/(200 - 3 - 1)), col = "pink")
[1] 43
plot(M2, which = 5)
[1] 44
qqPlot(M2)
[1] 45
hist(M2$residuals, col = "pink")
[1] 46
shapiro.test(M2$residuals)
[1] 47

	Shapiro-Wilk normality test

data:  M2$residuals
W = 0.97994, p-value = 0.005838

residualPlots(M2)
[1] 48
ncvTest(M2)
[1] 49
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
crPlots(M2)
[1] 50
durbinWatsonTest(M2)
[1] 51
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.196
 Alternative hypothesis: rho != 0
vif(M2)
[1] 52
      training scale(empathy)          years 
      1.020633       1.092467       1.085089 
M3 <- lm(incidents ~ feedback + scale(empathy) + years, data = data)
[1] 53
M3


Call:
lm(formula = incidents ~ feedback + scale(empathy) + years, data = data)

Coefficients:
     (Intercept)  feedbackfeedback    scale(empathy)             years  
         0.51209           0.13213           0.05883          -0.01269  



summary(M3)
[1] 55

Call:
lm(formula = incidents ~ feedback + scale(empathy) + years, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35580 -0.11148 -0.01097  0.11092  0.53552 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.512093   0.035846  14.286  < 2e-16 ***
feedbackfeedback  0.132132   0.023585   5.602 7.09e-08 ***
scale(empathy)    0.058827   0.012251   4.802 3.11e-06 ***
years            -0.012689   0.003975  -3.192  0.00165 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1663 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

plot(M3, which = 1)
[1] 56
cooks <- cooks.distance(M3)
[1] 57
which(cooks > (4/(200 - 3 - 1)))
[1] 58
 73  74  89 137 156 160 165 176 
 73  74  89 137 156 160 165 176 
plot(M3, which = 4)
[1] 59
abline(h = cooks.distance(M3) > (4/(200 - 3 - 1)), col = "pink")
[1] 60
plot(M3, which = 5)
[1] 61
qqPlot(M3)
[1] 62
hist(M3$residuals, col = "pink")
[1] 63
shapiro.test(M3$residuals)
[1] 64

	Shapiro-Wilk normality test

data:  M3$residuals
W = 0.98772, p-value = 0.08174

residualPlots(M3)
[1] 65
ncvTest(M3)
[1] 66
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.01731234, Df = 1, p = 0.89532
crPlots(M3)
[1] 67
durbinWatsonTest(M3)
[1] 68
 lag Autocorrelation D-W Statistic p-value
   1       0.2738264      1.444081       0
 Alternative hypothesis: rho != 0
vif(M3)
[1] 69
      feedback scale(empathy)          years 
      1.005211       1.079397       1.077467 
BIC(M2, M3)
[1] 70
   df       BIC
M2  5 -137.6738
M3  5 -127.4675
M4 <- lm(incidents ~ training * feedback, data = data)
[1] 71
summary(M4)
[1] 72

Call:
lm(formula = incidents ~ training * feedback, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40000 -0.10500  0.00000  0.08333  0.43333 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.49333    0.02314  21.323  < 2e-16 ***
trainingtraining                  -0.16000    0.03272  -4.890 2.09e-06 ***
feedbackfeedback                   0.09667    0.03272   2.954  0.00352 ** 
trainingtraining:feedbackfeedback  0.05333    0.04627   1.153  0.25047    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1636 on 196 degrees of freedom
Multiple R-squared:  0.2431,	Adjusted R-squared:  0.2315 
F-statistic: 20.99 on 3 and 196 DF,  p-value: 7.827e-12

ggplot(data, aes(x = feedback, y = incidents, fill = training)) + 
    geom_boxplot()
[1] 73
plot(M4, which = 1)
[1] 74
cooks <- cooks.distance(M4)
[1] 75
which(cooks > (4/(200 - 3 - 1)))
[1] 76
  7  13  89 131 150 159 160 165 176 181 
  7  13  89 131 150 159 160 165 176 181 
plot(M4, which = 4)
[1] 77
abline(h = cooks.distance(M4) > (4/(200 - 3 - 1)), col = "pink")
[1] 78
plot(M4, which = 5)
[1] 79
qqPlot(M4)
[1] 80
hist(M4$residuals, col = "pink")
[1] 81
shapiro.test(M4$residuals)
[1] 82

	Shapiro-Wilk normality test

data:  M4$residuals
W = 0.9767, p-value = 0.002072

residualPlots(M4)
[1] 83
ncvTest(M4)
[1] 84
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 1.881673, Df = 1, p = 0.17014
durbinWatsonTest(M4)
[1] 85
 lag Autocorrelation D-W Statistic p-value
   1       0.1114255      1.770733   0.138
 Alternative hypothesis: rho != 0
vif(M4)
[1] 86
         training          feedback training:feedback 
                2                 2                 3 
