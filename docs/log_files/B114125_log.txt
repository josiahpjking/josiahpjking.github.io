

	####### B114125 R script #######

install.packages("psych")
[1] 1
library("psych", quietly = T)
[1] 2
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
install.packages("MASS")
[1] 3
install.packages("car")
[1] 4
library("car", quietly = T)
[1] 5
 [1] "car"       "carData"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[11] "base"     
install.packages("interactions")
[1] 6
library("interactions", quietly = T)
[1] 7
 [1] "interactions" "car"          "carData"      "psych"        "readr"        "stats"        "graphics"     "grDevices"   
 [9] "utils"        "datasets"     "methods"      "base"        
install.packages("lm.beta")
[1] 8
library("lm.beta", quietly = T)
[1] 9
 [1] "lm.beta"      "interactions" "car"          "carData"      "psych"        "readr"        "stats"        "graphics"    
 [9] "grDevices"    "utils"        "datasets"     "methods"      "base"        
setwd("~/Desktop")
[1] 10
data <- read.csv("~/Desktop/rms2/code_check/RMS2_report_1920.csv")
[1] 11
str(data)
[1] 12
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : int  2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : int  2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
describe(data)
[1] 13
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training     3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 3 rows ]
data$training <- as.factor(data$training)
[1] 14
levels(data$training)
[1] 15
[1] "1" "2"
levels(data$training) <- c("No Training", "Training")
[1] 16
levels(data$training)
[1] 17
[1] "No Training" "Training"   
data$feedback <- as.factor(data$feedback)
[1] 18
levels(data$feedback)
[1] 19
[1] "1" "2"
levels(data$feedback) <- c("No Feedback", "Feedback")
[1] 20
levels(data$feedback)
[1] 21
[1] "No Feedback" "Feedback"   
data$years_s <- scale(data$years)
[1] 22
data$empathy_s <- scale(data$empathy)
[1] 23
str(data)
[1] 24
'data.frame':	200 obs. of  8 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : Factor w/ 2 levels "No Training",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : Factor w/ 2 levels "No Feedback",..: 2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
 $ years_s  : num [1:200, 1] 0.607 -0.367 -1.017 -0.367 1.257 ...
  ..- attr(*, "scaled:center")= num 8.13
  ..- attr(*, "scaled:scale")= num 3.08
 $ empathy_s: num [1:200, 1] 0.325 -0.069 -0.069 -0.463 0.719 ...
  ..- attr(*, "scaled:center")= num 10.2
  ..- attr(*, "scaled:scale")= num 2.54
NULL
describe(data)
[1] 25
          vars   n   mean    sd median trimmed   mad  min    max  range skew kurtosis   se
subject      1 200 100.50 57.88 100.50  100.50 74.13 1.00 200.00 199.00 0.00    -1.22 4.09
incidents    2 200   0.48  0.19   0.42    0.47  0.12 0.08   0.92   0.83 0.32    -0.31 0.01
training*    3 200   1.50  0.50   1.50    1.50  0.74 1.00   2.00   1.00 0.00    -2.01 0.04
 [ reached 'max' / getOption("max.print") -- omitted 5 rows ]
describeBy(data$incidents, data$training)
[1] 26

 Descriptive statistics by group 
group: No Training
   vars   n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X1    1 100 0.54 0.18    0.5    0.53 0.25 0.25 0.92  0.67 0.39    -0.73 0.02
--------------------------------------------------------------------------------------------- 
group: Training
   vars   n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X1    1 100 0.41 0.17   0.42     0.4 0.12 0.08 0.92  0.83 0.38    -0.05 0.02
describeBy(data$incidents, data$feedback)
[1] 27

 Descriptive statistics by group 
group: No Feedback
   vars   n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X1    1 100 0.41 0.17   0.42     0.4 0.12 0.08 0.92  0.83  0.6     0.36 0.02
--------------------------------------------------------------------------------------------- 
group: Feedback
   vars   n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X1    1 100 0.54 0.18    0.5    0.53 0.25 0.08 0.92  0.83 0.11    -0.44 0.02
describeBy(data$incidents, list(data$training, data$feedback))
[1] 28

 Descriptive statistics by group 
: No Training
: No Feedback
   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X1    1 50 0.49 0.17   0.42    0.48 0.12 0.25 0.92  0.67 0.82    -0.07 0.02
--------------------------------------------------------------------------------------------- 
: Training
: No Feedback
   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X1    1 50 0.33 0.14   0.33    0.33 0.12 0.08 0.58   0.5 0.15    -0.96 0.02
--------------------------------------------------------------------------------------------- 
: No Training
: Feedback
   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X1    1 50 0.59 0.17   0.58    0.59 0.12 0.25 0.92  0.67 0.05    -0.81 0.02
--------------------------------------------------------------------------------------------- 
: Training
: Feedback
   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se
X1    1 50 0.48 0.17    0.5    0.48 0.12 0.08 0.92  0.83 0.23    -0.07 0.02
plot(data$years_s, data$empathy_s, main = "Relationship Between Empathy and Years", 
    ylab = "Empathy", xlab = "Years")
[1] 29
lm <- lm(empathy_s ~ years_s, data = data)
[1] 30
summary(lm)
[1] 31

Call:
lm(formula = empathy_s ~ years_s, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.97192 -0.59754  0.02792  0.64474  2.30713 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -3.419e-16  6.837e-02   0.000 1.000000    
years_s      2.640e-01  6.854e-02   3.852 0.000158 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.9669 on 198 degrees of freedom
Multiple R-squared:  0.06972,	Adjusted R-squared:  0.06502 
F-statistic: 14.84 on 1 and 198 DF,  p-value: 0.0001582

crPlots(lm)
[1] 32
M1 <- lm(empathy_s ~ years_s + I(years_s^2), data = data)
[1] 33
summary(M1)
[1] 34

Call:
lm(formula = empathy_s ~ years_s + I(years_s^2), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.77645 -0.55225 -0.03293  0.46143  2.38949 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)   0.38537    0.06599   5.840 2.13e-08 ***
years_s       0.46511    0.05824   7.986 1.13e-13 ***
I(years_s^2) -0.38731    0.03685 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.776 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

MASS::studres1 <- studres(M1)
[1] 35


####################################################################################################

[1] "Error in studres(M1) : could not find function \"studres\"\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in studres(M1): could not find function "studres">
####################################################################################################


which(abs(MASS::studres1) > 2)
[1] 36
[1] "Error : 'studres1' is not an exported object from 'namespace:MASS'\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError: 'studres1' is not an exported object from 'namespace:MASS'>


####################################################################################################

[1] "Error : 'studres1' is not an exported object from 'namespace:MASS'\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError: 'studres1' is not an exported object from 'namespace:MASS'>
####################################################################################################


hats1 <- hatvalues(M1)
[1] 37
which(hats1 > 2 * mean(hats1))
[1] 38
 16  61  88  89  92 116 137 141 154 155 165 171 190 200 
 16  61  88  89  92 116 137 141 154 155 165 171 190 200 
cooks1 <- cooks.distance(M1)
[1] 39
names(which(cooks1 > 4/(200 - 2 - 1)))
[1] 40
[1] "61"  "89"  "137" "141" "154" "163" "165" "175"
plot(M1, which = 4)
[1] 41
M1a <- lm(empathy_s ~ years_s + I(years_s^2), data = data[-c(89, 
    154, 175), ])
[1] 42
summary(M1a)
[1] 43

Call:
lm(formula = empathy_s ~ years_s + I(years_s^2), data = data[-c(89, 
    154, 175), ])

Residuals:
     Min       1Q   Median       3Q      Max 
-1.78535 -0.50858  0.05337  0.51825  1.92746 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)   0.38960    0.06611   5.894 1.65e-08 ***
years_s       0.48209    0.05762   8.366 1.15e-14 ***
I(years_s^2) -0.40261    0.04146  -9.710  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.7561 on 194 degrees of freedom
Multiple R-squared:  0.4117,	Adjusted R-squared:  0.4056 
F-statistic: 67.87 on 2 and 194 DF,  p-value: < 2.2e-16

summary(M1)
[1] 44

Call:
lm(formula = empathy_s ~ years_s + I(years_s^2), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.77645 -0.55225 -0.03293  0.46143  2.38949 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)   0.38537    0.06599   5.840 2.13e-08 ***
years_s       0.46511    0.05824   7.986 1.13e-13 ***
I(years_s^2) -0.38731    0.03685 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.776 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

qqPlot(M1)
[1] 45
hist(M1$residuals)
[1] 46
shapiro.test(M1$residuals)
[1] 47

	Shapiro-Wilk normality test

data:  M1$residuals
W = 0.9941, p-value = 0.6155

plot(M1, which = 3)
[1] 48
ncvTest(M1)
[1] 49
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
crPlots(M1)
[1] 50
durbinWatsonTest(M1)
[1] 51
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855    0.88
 Alternative hypothesis: rho != 0
vif(M1)
[1] 52
     years_s I(years_s^2) 
    1.120963     1.120963 
summary(M1)
[1] 53

Call:
lm(formula = empathy_s ~ years_s + I(years_s^2), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.77645 -0.55225 -0.03293  0.46143  2.38949 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)   0.38537    0.06599   5.840 2.13e-08 ***
years_s       0.46511    0.05824   7.986 1.13e-13 ***
I(years_s^2) -0.38731    0.03685 -10.510  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.776 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

confint(M1)
[1] 54
                  2.5 %     97.5 %
(Intercept)   0.2552290  0.5155094
years_s       0.3502606  0.5799588
I(years_s^2) -0.4599764 -0.3146350
plot(data$training, data$incidents, xlab = "Training Group", 
    ylab = "Incidents (%)", main = "Boxplot of Effect of Training on Interventions (%)", 
    col = c("Blue", "Green"))
[1] 55
M2 <- lm(incidents ~ training + years_s + empathy_s, data = data)
[1] 56
summary(M2)
[1] 57

Call:
lm(formula = incidents ~ training + years_s + empathy_s, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.55121    0.01630  33.821  < 2e-16 ***
trainingTraining -0.15243    0.02317  -6.580 4.20e-10 ***
years_s          -0.04337    0.01197  -3.622 0.000372 ***
empathy_s         0.06443    0.01201   5.363 2.29e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

confint(M2)
[1] 58
                       2.5 %      97.5 %
(Intercept)       0.51907118  0.58335566
trainingTraining -0.19811433 -0.10673936
years_s          -0.06698004 -0.01975377
empathy_s         0.04073642  0.08812297
MASS::studres2 <- studres(M2)
[1] 59


####################################################################################################

[1] "Error in studres(M2) : could not find function \"studres\"\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in studres(M2): could not find function "studres">
####################################################################################################


which(abs(MASS::studres2) > 2)
[1] 60
[1] "Error : 'studres2' is not an exported object from 'namespace:MASS'\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError: 'studres2' is not an exported object from 'namespace:MASS'>


####################################################################################################

[1] "Error : 'studres2' is not an exported object from 'namespace:MASS'\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError: 'studres2' is not an exported object from 'namespace:MASS'>
####################################################################################################


hats2 <- hatvalues(M2)
[1] 61
which(hats2 > 2 * mean(hats2))
[1] 62
 61  88  89  92  94 137 141 154 163 175 200 
 61  88  89  92  94 137 141 154 163 175 200 
cooks2 <- cooks.distance(M2)
[1] 63
names(which(cooks2 > 4/(200 - 3 - 1)))
[1] 64
[1] "13"  "35"  "74"  "137" "165"
plot(M2, which = 4)
[1] 65
M2a <- lm(incidents ~ training + years_s + empathy_s, data = data[-c(13, 
    74, 137), ])
[1] 66
summary(M2a)
[1] 67

Call:
lm(formula = incidents ~ training + years_s + empathy_s, data = data[-c(13, 
    74, 137), ])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.29350 -0.11585 -0.00812  0.10001  0.40267 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.54946    0.01565  35.119  < 2e-16 ***
trainingTraining -0.15372    0.02228  -6.900 7.28e-11 ***
years_s          -0.05002    0.01190  -4.204 4.01e-05 ***
empathy_s         0.07366    0.01181   6.236 2.77e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1552 on 193 degrees of freedom
Multiple R-squared:  0.2988,	Adjusted R-squared:  0.2879 
F-statistic: 27.41 on 3 and 193 DF,  p-value: 8.212e-15

summary(M2)
[1] 68

Call:
lm(formula = incidents ~ training + years_s + empathy_s, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35648 -0.12270 -0.01474  0.10105  0.53180 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.55121    0.01630  33.821  < 2e-16 ***
trainingTraining -0.15243    0.02317  -6.580 4.20e-10 ***
years_s          -0.04337    0.01197  -3.622 0.000372 ***
empathy_s         0.06443    0.01201   5.363 2.29e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1621 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

qqPlot(M2)
[1] 69
hist(M2$residuals, xlab = "Residuals", main = "Histogram of Residuals")
[1] 70
shapiro.test(M2$residuals)
[1] 71

	Shapiro-Wilk normality test

data:  M2$residuals
W = 0.97994, p-value = 0.005838

plot(M2, which = 3)
[1] 72
ncvTest(M2)
[1] 73
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
crPlots(M2)
[1] 74
durbinWatsonTest(M2)
[1] 75
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.164
 Alternative hypothesis: rho != 0
vif(M2)
[1] 76
 training   years_s empathy_s 
 1.020633  1.085089  1.092467 
plot(data$feedback, data$incidents, xlab = "Feedback Group", 
    ylab = "Incidents (%)", main = "Boxplot of Effect of Feedback on Interventions (%)", 
    col = c("Red", "Purple"))
[1] 77
M3 <- lm(incidents ~ feedback + years_s + empathy_s, data = data)
[1] 78
summary(M3)
[1] 79

Call:
lm(formula = incidents ~ feedback + years_s + empathy_s, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35580 -0.11148 -0.01097  0.11092  0.53552 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.40893    0.01666  24.553  < 2e-16 ***
feedbackFeedback  0.13213    0.02358   5.602 7.09e-08 ***
years_s          -0.03907    0.01224  -3.192  0.00165 ** 
empathy_s         0.05883    0.01225   4.802 3.11e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1663 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

confint(M3)
[1] 80
                       2.5 %      97.5 %
(Intercept)       0.37608731  0.44178104
feedbackFeedback  0.08561885  0.17864446
years_s          -0.06320693 -0.01493060
empathy_s         0.03466751  0.08298704
MASS::studres3 <- studres(M3)
[1] 81


####################################################################################################

[1] "Error in studres(M3) : could not find function \"studres\"\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in studres(M3): could not find function "studres">
####################################################################################################


which(abs(MASS::studres3) > 2)
[1] 82
[1] "Error : 'studres3' is not an exported object from 'namespace:MASS'\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError: 'studres3' is not an exported object from 'namespace:MASS'>


####################################################################################################

[1] "Error : 'studres3' is not an exported object from 'namespace:MASS'\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError: 'studres3' is not an exported object from 'namespace:MASS'>
####################################################################################################


hats3 <- hatvalues(M3)
[1] 83
which(hats3 > 2 * mean(hats3))
[1] 84
 61  88  89  92 137 141 154 163 175 200 
 61  88  89  92 137 141 154 163 175 200 
cooks3 <- cooks.distance(M3)
[1] 85
names(which(cooks3 > 4/(200 - 3 - 1)))
[1] 86
[1] "73"  "74"  "89"  "137" "156" "160" "165" "176"
plot(M3, which = 4)
[1] 87
M3a <- lm(incidents ~ feedback + years_s + empathy_s, data = data[-c(74, 
    89, 137), ])
[1] 88
summary(M3a)
[1] 89

Call:
lm(formula = incidents ~ feedback + years_s + empathy_s, data = data[-c(74, 
    89, 137), ])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.34740 -0.11326 -0.01073  0.11335  0.53413 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.41229    0.01643  25.098  < 2e-16 ***
feedbackFeedback  0.12890    0.02328   5.537 9.99e-08 ***
years_s          -0.04182    0.01277  -3.276  0.00125 ** 
empathy_s         0.06314    0.01256   5.028 1.13e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1633 on 193 degrees of freedom
Multiple R-squared:  0.2282,	Adjusted R-squared:  0.2162 
F-statistic: 19.02 on 3 and 193 DF,  p-value: 7.541e-11

summary(M3)
[1] 90

Call:
lm(formula = incidents ~ feedback + years_s + empathy_s, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35580 -0.11148 -0.01097  0.11092  0.53552 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.40893    0.01666  24.553  < 2e-16 ***
feedbackFeedback  0.13213    0.02358   5.602 7.09e-08 ***
years_s          -0.03907    0.01224  -3.192  0.00165 ** 
empathy_s         0.05883    0.01225   4.802 3.11e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1663 on 196 degrees of freedom
Multiple R-squared:  0.2175,	Adjusted R-squared:  0.2055 
F-statistic: 18.16 on 3 and 196 DF,  p-value: 1.933e-10

qqPlot(M3)
[1] 91
hist(M3$residuals, xlab = "Residuals", main = "Histogram of Residuals")
[1] 92
shapiro.test(M3$residuals)
[1] 93

	Shapiro-Wilk normality test

data:  M3$residuals
W = 0.98772, p-value = 0.08174

plot(M3, which = 3)
[1] 94
ncvTest(M3)
[1] 95
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.01731234, Df = 1, p = 0.89532
crPlots(M3)
[1] 96
durbinWatsonTest(M3)
[1] 97
 lag Autocorrelation D-W Statistic p-value
   1       0.2738264      1.444081       0
 Alternative hypothesis: rho != 0
vif(M3)
[1] 98
 feedback   years_s empathy_s 
 1.005211  1.077467  1.079397 
AIC(M2, M3)
[1] 99
   df       AIC
M2  5 -154.1654
M3  5 -143.9591
BIC(M2, M3)
[1] 100
   df       BIC
M2  5 -137.6738
M3  5 -127.4675
M4 <- lm(incidents ~ training + feedback + scale(years) + scale(empathy) + 
    training * feedback, data = data)
[1] 101
summary(M4)
[1] 102

Call:
lm(formula = incidents ~ training + feedback + scale(years) + 
    scale(empathy) + training * feedback, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30367 -0.10365 -0.00795  0.08806  0.44359 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.50964    0.02081  24.494  < 2e-16 ***
trainingTraining                  -0.20350    0.02987  -6.812 1.18e-10 ***
feedbackFeedback                   0.08556    0.02930   2.920  0.00391 ** 
scale(years)                      -0.04840    0.01083  -4.468 1.34e-05 ***
scale(empathy)                     0.07280    0.01098   6.630 3.25e-10 ***
trainingTraining:feedbackFeedback  0.09733    0.04184   2.326  0.02103 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1462 on 194 degrees of freedom
Multiple R-squared:  0.4015,	Adjusted R-squared:  0.386 
F-statistic: 26.02 on 5 and 194 DF,  p-value: < 2.2e-16

confint(M4)
[1] 103
                                        2.5 %      97.5 %
(Intercept)                        0.46859906  0.55067248
trainingTraining                  -0.26241518 -0.14457775
feedbackFeedback                   0.02777246  0.14334354
scale(years)                      -0.06976733 -0.02703665
scale(empathy)                     0.05114323  0.09445675
trainingTraining:feedbackFeedback  0.01481491  0.17985274
cat_plot(M4, training, modx = feedback, geom = "line", point.shape = TRUE, 
    vary.lty = TRUE, y.label = "Incidents (%)", x.label = "Training", 
    legend.main = "Feedback", data = data)
[1] 104
MASS::studres4 <- studres(M4)
[1] 105


####################################################################################################

[1] "Error in studres(M4) : could not find function \"studres\"\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError in studres(M4): could not find function "studres">
####################################################################################################


which(abs(MASS::studres4) > 2)
[1] 106
[1] "Error : 'studres4' is not an exported object from 'namespace:MASS'\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError: 'studres4' is not an exported object from 'namespace:MASS'>


####################################################################################################

[1] "Error : 'studres4' is not an exported object from 'namespace:MASS'\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
<simpleError: 'studres4' is not an exported object from 'namespace:MASS'>
####################################################################################################


hats4 <- hatvalues(M4)
[1] 107
which(hats4 > 2 * mean(hats4))
[1] 108
 61  88  89 137 141 154 200 
 61  88  89 137 141 154 200 
cooks4 <- cooks.distance(M4)
[1] 109
names(which(cooks4 > 4/(200 - 5 - 1)))
[1] 110
[1] "13"  "73"  "74"  "89"  "117" "137" "160" "165" "176"
plot(M4, which = 4)
[1] 111
M4a <- lm(incidents ~ training + feedback + scale(years) + scale(empathy) + 
    training * feedback, data = data[-c(13, 137, 176), ])
[1] 112
summary(M4a)
[1] 113

Call:
lm(formula = incidents ~ training + feedback + scale(years) + 
    scale(empathy) + training * feedback, data = data[-c(13, 
    137, 176), ])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.29641 -0.09999 -0.00384  0.09683  0.33452 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.50368    0.01995  25.251  < 2e-16 ***
trainingTraining                  -0.19797    0.02848  -6.950 5.60e-11 ***
feedbackFeedback                   0.08798    0.02809   3.132  0.00201 ** 
scale(years)                      -0.05383    0.01046  -5.146 6.58e-07 ***
scale(empathy)                     0.07779    0.01061   7.330 6.34e-12 ***
trainingTraining:feedbackFeedback  0.08848    0.04007   2.208  0.02845 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1386 on 191 degrees of freedom
Multiple R-squared:  0.4376,	Adjusted R-squared:  0.4228 
F-statistic: 29.72 on 5 and 191 DF,  p-value: < 2.2e-16

summary(M4)
[1] 114

Call:
lm(formula = incidents ~ training + feedback + scale(years) + 
    scale(empathy) + training * feedback, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30367 -0.10365 -0.00795  0.08806  0.44359 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                        0.50964    0.02081  24.494  < 2e-16 ***
trainingTraining                  -0.20350    0.02987  -6.812 1.18e-10 ***
feedbackFeedback                   0.08556    0.02930   2.920  0.00391 ** 
scale(years)                      -0.04840    0.01083  -4.468 1.34e-05 ***
scale(empathy)                     0.07280    0.01098   6.630 3.25e-10 ***
trainingTraining:feedbackFeedback  0.09733    0.04184   2.326  0.02103 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1462 on 194 degrees of freedom
Multiple R-squared:  0.4015,	Adjusted R-squared:  0.386 
F-statistic: 26.02 on 5 and 194 DF,  p-value: < 2.2e-16

qqPlot(M4)
[1] 115
hist(M4$residuals, xlab = "Residuals", main = "Histogram of Residuals")
[1] 116
shapiro.test(M4$residuals)
[1] 117

	Shapiro-Wilk normality test

data:  M4$residuals
W = 0.98794, p-value = 0.08817

plot(M4, which = 3)
[1] 118
ncvTest(M4)
[1] 119
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 4.398557, Df = 1, p = 0.035969
plot(M4, which = 1)
[1] 120
durbinWatsonTest(M4)
[1] 121
 lag Autocorrelation D-W Statistic p-value
   1      0.09456969      1.801048   0.188
 Alternative hypothesis: rho != 0
vif(M4)
[1] 122
         training          feedback      scale(years)    scale(empathy) training:feedback 
         2.086826          2.007327          1.092150          1.122147          3.070068 
