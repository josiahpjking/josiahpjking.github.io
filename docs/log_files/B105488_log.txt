

	####### B105488 R script #######

df <- read.csv(file = "../RMS2_report_1920.csv", header = T)
[1] 1
install.packages("psych")
[1] 2
library("psych", quietly = T)
[1] 3
[1] "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"     
install.packages("ggplot2")
[1] 4
library(ggplot2, quietly = T)
[1] 5
 [1] "ggplot2"   "psych"     "readr"     "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[10] "base"     
install.packages("data.table")
[1] 6
library(data.table, quietly = T)
[1] 7
 [1] "data.table" "ggplot2"    "psych"      "readr"      "stats"      "graphics"   "grDevices"  "utils"     
 [9] "datasets"   "methods"    "base"      
install.packages("car")
[1] 8
library(car, quietly = T)
[1] 9
 [1] "car"        "carData"    "data.table" "ggplot2"    "psych"      "readr"      "stats"      "graphics"  
 [9] "grDevices"  "utils"      "datasets"   "methods"    "base"      
install.packages("interactions")
[1] 10
library(interactions, quietly = T)
[1] 11
 [1] "interactions" "car"          "carData"      "data.table"   "ggplot2"      "psych"        "readr"       
 [8] "stats"        "graphics"     "grDevices"    "utils"        "datasets"     "methods"      "base"        
class(df$training)
[1] 12
[1] "integer"
df$training <- as.factor(df$training)
[1] 13
levels(df$training)
[1] 14
[1] "1" "2"
df$training <- factor(df$training, labels = c("No training", 
    "training"))
[1] 15
str(df$training)
[1] 16
 Factor w/ 2 levels "No training",..: 2 2 2 2 2 2 2 2 2 2 ...
NULL
class(df$feedback)
[1] 17
[1] "integer"
df$feedback <- as.factor(df$feedback)
[1] 18
levels(df$feedback)
[1] 19
[1] "1" "2"
df$feedback <- factor(df$feedback, labels = c("No feedback", 
    "feedback"))
[1] 20
str(df$feedback)
[1] 21
 Factor w/ 2 levels "No feedback",..: 2 1 2 1 2 1 2 1 2 1 ...
NULL
describe(df)[, c(2:4, 8, 9, 11, 12)]
[1] 22
            n   mean    sd  min    max  skew kurtosis
subject   200 100.50 57.88 1.00 200.00  0.00    -1.22
incidents 200   0.48  0.19 0.08   0.92  0.32    -0.31
training* 200   1.50  0.50 1.00   2.00  0.00    -2.01
feedback* 200   1.50  0.50 1.00   2.00  0.00    -2.01
empathy   200  10.18  2.54 3.00  16.00 -0.37     0.23
years     200   8.13  3.08 1.00  19.00  0.52     0.48
summary(df)[, c(3, 4)]
[1] 23
        training          feedback  
 No training:100   No feedback:100  
 training   :100   feedback   :100  
                                    
                                    
                                    
                                    
str(df)
[1] 24
'data.frame':	200 obs. of  6 variables:
 $ subject  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ incidents: num  0.667 0.333 0.333 0.417 0.5 ...
 $ training : Factor w/ 2 levels "No training",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ feedback : Factor w/ 2 levels "No feedback",..: 2 1 2 1 2 1 2 1 2 1 ...
 $ empathy  : int  11 10 10 9 12 11 13 14 12 12 ...
 $ years    : int  10 7 5 7 12 10 8 10 7 9 ...
NULL
plot(df$years, df$empathy, xlab = "Experience", ylab = "Empathy")
[1] 25
plot(df$years, df$incidents, xlab = "Experience", ylab = "Intervention")
[1] 26
plot(df$empathy, df$incidents, xlab = "Empathy", ylab = "Intervention")
[1] 27
boxplot(df$training, df$incidents, ylab = "Intervention", names = c("training", 
    "No training"), pch = 16)
[1] 28
boxplot(df$feedback, df$incidents, ylab = "Intervention", names = c("feedback", 
    "No feedback"), pch = 16)
[1] 29
M1 <- lm(scale(empathy) ~ scale(years), data = df)
[1] 30
crPlots(M1)
[1] 31
df$empathy_1 <- df$empathy - min(df$empathy)
[1] 32
df$years_1 <- df$years - min(df$years)
[1] 33
M1a <- lm(scale(df$empathy_1) ~ scale(df$years_1) + scale(I(df$years_1^2)))
[1] 34
summary(M1a)
[1] 35

Call:
lm(formula = scale(df$empathy_1) ~ scale(df$years_1) + scale(I(df$years_1^2)))

Residuals:
     Min       1Q   Median       3Q      Max 
-1.77645 -0.55225 -0.03293  0.46143  2.38949 

Coefficients:
                         Estimate Std. Error t value Pr(>|t|)    
(Intercept)             3.346e-16  5.487e-02    0.00        1    
scale(df$years_1)       2.259e+00  1.976e-01   11.43   <2e-16 ***
scale(I(df$years_1^2)) -2.077e+00  1.976e-01  -10.51   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.776 on 197 degrees of freedom
Multiple R-squared:  0.404,	Adjusted R-squared:  0.3979 
F-statistic: 66.75 on 2 and 197 DF,  p-value: < 2.2e-16

plot(cooks.distance(M1a))
[1] 36
abline(h = cooks.distance(M1a) > (4/(200 - 2 - 1)), col = "red")
[1] 37
plot(M1a, which = 4)
[1] 38
crPlots(M1a)
[1] 39
plot(df$years_1, df$empathy_1, main = "Relationship between empathy and experience", 
    xlab = "Years of Experience", ylab = "Empathy")
[1] 40
lines(lowess(df$years_1, df$empathy_1))
[1] 41
hist(M1a$residuals)
[1] 42
shapiro.test(M1a$residuals)
[1] 43

	Shapiro-Wilk normality test

data:  M1a$residuals
W = 0.9941, p-value = 0.6155

residualPlots(M1a)
[1] 44
ncvTest(M1a)
[1] 45
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.5107658, Df = 1, p = 0.47481
durbinWatsonTest(M1a)
[1] 46
 lag Autocorrelation D-W Statistic p-value
   1      0.01288905      1.973855   0.874
 Alternative hypothesis: rho != 0
contrasts(df$training) <- contr.sum(2)
[1] 47
contrasts(df$training) <- contr.treatment
[1] 48
M2 <- lm(scale(incidents) ~ training + scale(empathy) + scale(years), 
    data = df)
[1] 49
summary(M2)
[1] 50

Call:
lm(formula = scale(incidents) ~ training + scale(empathy) + scale(years), 
    data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.91022 -0.65749 -0.07901  0.54149  2.84967 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.40839    0.08733   4.676 5.43e-06 ***
training2      -0.81678    0.12414  -6.580 4.20e-10 ***
scale(empathy)  0.34525    0.06438   5.363 2.29e-07 ***
scale(years)   -0.23238    0.06416  -3.622 0.000372 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.8689 on 196 degrees of freedom
Multiple R-squared:  0.2565,	Adjusted R-squared:  0.2451 
F-statistic: 22.53 on 3 and 196 DF,  p-value: 1.408e-12

plot(cooks.distance(M2))
[1] 51
abline(h = cooks.distance(M2) > (4/(200 - 2 - 1)), col = "red")
[1] 52
which(cooks.distance(M2) > (4/(200 - 2 - 1)))
[1] 53
 13  35  74 137 165 
 13  35  74 137 165 
plot(M2, which = 4)
[1] 54
M2a <- lm(incidents ~ training + scale(empathy) + scale(years), 
    data = df[-c(13, 35, 74, 137, 165), ])
[1] 55
summary(M2a)
[1] 56

Call:
lm(formula = incidents ~ training + scale(empathy) + scale(years), 
    data = df[-c(13, 35, 74, 137, 165), ])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.28818 -0.11426 -0.00621  0.09990  0.40582 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.54661    0.01543  35.423  < 2e-16 ***
training2      -0.15385    0.02197  -7.003 4.16e-11 ***
scale(empathy)  0.07176    0.01150   6.240 2.76e-09 ***
scale(years)   -0.04841    0.01148  -4.215 3.84e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1521 on 191 degrees of freedom
Multiple R-squared:  0.3042,	Adjusted R-squared:  0.2932 
F-statistic: 27.83 on 3 and 191 DF,  p-value: 5.621e-15

hist(M2$residuals)
[1] 57
shapiro.test(M2$residuals)
[1] 58

	Shapiro-Wilk normality test

data:  M2$residuals
W = 0.97994, p-value = 0.005838

residualPlots(M2)
[1] 59
ncvTest(M2)
[1] 60
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.0293323, Df = 1, p = 0.86401
durbinWatsonTest(M2)
[1] 61
 lag Autocorrelation D-W Statistic p-value
   1      -0.1072849      2.199897   0.204
 Alternative hypothesis: rho != 0
par(mfrow = c(1, 3))
[1] 62
$mfrow
[1] 1 1

plot(M2)
[1] 63
crPlots(M2)
[1] 64
vif(M2)
[1] 65
      training scale(empathy)   scale(years) 
      1.020633       1.092467       1.085089 
log.incidents <- log(df$incidents + (-1 * min(df$incidents) + 
    1))
[1] 66
M2b <- lm(log.incidents ~ df$training + scale(df$empathy) + scale(df$years))
[1] 67
summary(M2b)
[1] 68

Call:
lm(formula = log.incidents ~ df$training + scale(df$empathy) + 
    scale(df$years))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.27676 -0.08678 -0.00271  0.07438  0.35035 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)        0.377249   0.011606  32.504  < 2e-16 ***
df$training2      -0.111288   0.016497  -6.746 1.67e-10 ***
scale(df$empathy)  0.047061   0.008555   5.501 1.17e-07 ***
scale(df$years)   -0.031662   0.008526  -3.713 0.000266 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1155 on 196 degrees of freedom
Multiple R-squared:  0.2661,	Adjusted R-squared:  0.2549 
F-statistic: 23.69 on 3 and 196 DF,  p-value: 3.963e-13

plot(cooks.distance(M2b))
[1] 69
abline(h = cooks.distance(M2b) > (4/(200 - 2 - 1)), col = "red")
[1] 70
plot(M2b, which = 4)
[1] 71
which(cooks.distance(M2b) > (4/(200 - 2 - 1)))
[1] 72
 13  30  35  60  74  88  94 137 
 13  30  35  60  74  88  94 137 
hist(M2b$residuals)
[1] 73
shapiro.test(M2b$residuals)
[1] 74

	Shapiro-Wilk normality test

data:  M2b$residuals
W = 0.9905, p-value = 0.2109

residualPlots(M2b)
[1] 75
ncvTest(M2b)
[1] 76
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 1.036059, Df = 1, p = 0.30874
durbinWatsonTest(M2b)
[1] 77
 lag Autocorrelation D-W Statistic p-value
   1       -0.116516      2.217742    0.14
 Alternative hypothesis: rho != 0
par(mfrow = c(1, 3))
[1] 78
$mfrow
[1] 1 3

plot(M2b)
[1] 79
crPlots(M2b)
[1] 80
vif(M2b)
[1] 81
      df$training scale(df$empathy)   scale(df$years) 
         1.020633          1.092467          1.085089 
contrasts(df$feedback) <- contr.sum(2)
[1] 82
contrasts(df$feedback) <- contr.treatment
[1] 83
M3 <- lm(log.incidents ~ feedback + scale(empathy) + scale(years), 
    data = df)
[1] 84
summary(M3)
[1] 85

Call:
lm(formula = log.incidents ~ feedback + scale(empathy) + scale(years), 
    data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.27663 -0.07576 -0.00237  0.08302  0.35261 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.273797   0.011893  23.021  < 2e-16 ***
feedback2       0.095615   0.016841   5.677 4.87e-08 ***
scale(empathy)  0.042942   0.008748   4.909 1.92e-06 ***
scale(years)   -0.028503   0.008740  -3.261  0.00131 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1188 on 196 degrees of freedom
Multiple R-squared:  0.2235,	Adjusted R-squared:  0.2116 
F-statistic:  18.8 on 3 and 196 DF,  p-value: 9.283e-11

plot(cooks.distance(M3))
[1] 86
abline(h = cooks.distance(M3) > (4/(200 - 2 - 1)), col = "red")
[1] 87
plot(M3, which = 4)
[1] 88
which(cooks.distance(M3) > (4/(200 - 2 - 1)))
[1] 89
 17  30  60  73  74  88  89  94 137 156 160 176 200 
 17  30  60  73  74  88  89  94 137 156 160 176 200 
M3a <- lm(log.incidents ~ feedback + scale(empathy) + scale(years), 
    data = df[-c(17, 30, 60, 73, 74, 88, 89, 94, 137, 156, 160, 
        176, 200)])
[1] 90
summary(M3a)
[1] 91

Call:
lm(formula = log.incidents ~ feedback + scale(empathy) + scale(years), 
    data = df[-c(17, 30, 60, 73, 74, 88, 89, 94, 137, 156, 160, 
        176, 200)])

Residuals:
     Min       1Q   Median       3Q      Max 
-0.27663 -0.07576 -0.00237  0.08302  0.35261 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.273797   0.011893  23.021  < 2e-16 ***
feedback2       0.095615   0.016841   5.677 4.87e-08 ***
scale(empathy)  0.042942   0.008748   4.909 1.92e-06 ***
scale(years)   -0.028503   0.008740  -3.261  0.00131 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1188 on 196 degrees of freedom
Multiple R-squared:  0.2235,	Adjusted R-squared:  0.2116 
F-statistic:  18.8 on 3 and 196 DF,  p-value: 9.283e-11

hist(M3$residuals)
[1] 92
shapiro.test(M3$residuals)
[1] 93

	Shapiro-Wilk normality test

data:  M3$residuals
W = 0.99572, p-value = 0.8487

residualPlots(M3)
[1] 94
ncvTest(M3)
[1] 95
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.6746094, Df = 1, p = 0.41145
durbinWatsonTest(M3)
[1] 96
 lag Autocorrelation D-W Statistic p-value
   1       0.2794347      1.431864       0
 Alternative hypothesis: rho != 0
par(mfrow = c(1, 3))
[1] 97
$mfrow
[1] 1 3

plot(M3)
[1] 98
crPlots(M3)
[1] 99
vif(M3)
[1] 100
      feedback scale(empathy)   scale(years) 
      1.005211       1.079397       1.077467 
AIC(M2b, M3)
[1] 101
    df       AIC
M2b  5 -289.9740
M3   5 -278.6673
BIC(M2b, M3)
[1] 102
    df       BIC
M2b  5 -273.4824
M3   5 -262.1757
M4 <- lm(log.incidents ~ training + feedback + scale(empathy_1) + 
    scale(years_1) + training * feedback, data = df)
[1] 103
summary(M4)
[1] 104

Call:
lm(formula = log.incidents ~ training + feedback + scale(empathy_1) + 
    scale(years_1) + training * feedback, data = df)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.226383 -0.071808 -0.000811  0.069089  0.284822 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)          0.348968   0.014712  23.720  < 2e-16 ***
training2           -0.151903   0.021123  -7.191 1.36e-11 ***
feedback2            0.058376   0.020716   2.818  0.00533 ** 
scale(empathy_1)     0.053400   0.007764   6.878 8.14e-11 ***
scale(years_1)      -0.035425   0.007660  -4.625 6.84e-06 ***
training2:feedback2  0.077604   0.029583   2.623  0.00940 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1034 on 194 degrees of freedom
Multiple R-squared:  0.4176,	Adjusted R-squared:  0.4026 
F-statistic: 27.82 on 5 and 194 DF,  p-value: < 2.2e-16

plot(cooks.distance(M4))
[1] 105
abline(h = cooks.distance(M4) > (4/(200 - 2 - 1)), col = "red")
[1] 106
plot(M4, which = 4)
[1] 107
which(cooks.distance(M4) > (4/(200 - 2 - 1)))
[1] 108
 13  73  74  89 117 137 176 
 13  73  74  89 117 137 176 
M4a <- lm(log.incidents ~ training + feedback + scale(empathy_1) + 
    scale(years_1) + training * feedback, data = df[-c(13, 73, 
    74, 89, 117, 137, 176)], )
[1] 109
summary(M4a)
[1] 110

Call:
lm(formula = log.incidents ~ training + feedback + scale(empathy_1) + 
    scale(years_1) + training * feedback, data = df[-c(13, 73, 
    74, 89, 117, 137, 176)])

Residuals:
      Min        1Q    Median        3Q       Max 
-0.226383 -0.071808 -0.000811  0.069089  0.284822 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)          0.348968   0.014712  23.720  < 2e-16 ***
training2           -0.151903   0.021123  -7.191 1.36e-11 ***
feedback2            0.058376   0.020716   2.818  0.00533 ** 
scale(empathy_1)     0.053400   0.007764   6.878 8.14e-11 ***
scale(years_1)      -0.035425   0.007660  -4.625 6.84e-06 ***
training2:feedback2  0.077604   0.029583   2.623  0.00940 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1034 on 194 degrees of freedom
Multiple R-squared:  0.4176,	Adjusted R-squared:  0.4026 
F-statistic: 27.82 on 5 and 194 DF,  p-value: < 2.2e-16

hist(M4a$residuals)
[1] 111
shapiro.test(M4$residuals)
[1] 112

	Shapiro-Wilk normality test

data:  M4$residuals
W = 0.99434, p-value = 0.6515

residualPlots(M4)
[1] 113
ncvTest(M4)
[1] 114
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.577295, Df = 1, p = 0.44737
durbinWatsonTest(M4)
[1] 115
 lag Autocorrelation D-W Statistic p-value
   1      0.09726586      1.794988   0.192
 Alternative hypothesis: rho != 0
par(mfrow = c(1, 3))
[1] 116
$mfrow
[1] 1 3

plot(M4)
[1] 117
vif(M4)
[1] 118
         training          feedback  scale(empathy_1)    scale(years_1) training:feedback 
         2.086826          2.007327          1.122147          1.092150          3.070068 
cat_plot(M4, pred = training, modx = feedback, geom = "line", 
    interval = TRUE, y.label = "Intervention")
[1] 119
