---
title: "dg_paper"
bibliography: references.bib
author: "JK, BM"
editor_options: 
  chunk_output_type: console
format: 
  html:
    output-file: "dg_paper.html"
---


Population rates are frequently in the news, and comparisons of rates --- either across time or between populations --- are often used to support introduction of policies or as evidence of policy successes (or failures). The comparison of two or more rates, however, do not always reflect actual differences in, e.g., prevalence or incidence of the event in question. Instead, rate differences may be directly or indirectly influenced by underlying differences in the populations' characteristics.  

For a simple motivating example, suppose that we have two populations --- or rather, a population at two different timepoints --- Scottish offenders in 2006, and Scottish offenders in 2016, and we are concerned with the prevalance of reconvictions. In 2006, 32.4% of offenders had a reconviction, and in 2016 that number was 27.2%:  

```{r}
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: "Two populations and their crude rates"
#| label: tbl-egc
library(tidyverse)
df <- DasGuptR::reconv |> 
  filter(year %in% c(2006,2016)) |>
  mutate(Age = ifelse(Age %in% c("under 21","21 to 25","26 to 30"),"under 30","over 30")) |>
  group_by(year,Age) |>
  summarise(
    offenders = sum(offenders),
    reconvicted = sum(reconvicted),
    prev = reconvicted/offenders
  )
#dgnpop(df,"year","prev","Age","offenders") |> dg_table()
df |> group_by(year) |>
  summarise(
    offenders = sum(offenders),
    reconvicted = sum(reconvicted),
    crude_rate = round(sum(reconvicted)/sum(offenders),3)
  ) |> mutate_all(as.character) |>
  bind_rows(
    tibble(year = "",reconvicted = "",offenders="difference",crude_rate="0.052")
  ) |> gt::gt()
```

However, these two populations may differ in their age distributions. To keep things simple, splitting the populations into those above/below 30 years old, we can see that the 2006 population is made up of proportionally a lot more younger offenders (for which reconvictions are more common):  

```{r}
#| echo: false
#| tbl-cap: "Two populations and their age specific rates"
#| label: tbl-eg1
gt::gt(df |> mutate(age_prop = round(offenders/sum(offenders),3),.after=offenders) |>
         mutate(prev=round(prev,3)) |> arrange(desc(Age)))
```

For social scientists, demographers, economists and those working in public health, two related methods --- standardisation and decomposition --- are frequently employed in order to separate out differences in rates from differences in the compositional factors that make up the populations (or that make up the calculation of the rate itself).  

Standardisation and decomposition are broad terms to capture methods that attempt to answer two questions: What would the rate be if these populations were the same with respect to X? (standardisation), and how much of the difference in rates between two populations is due to the populations differing in X? (decomposition).  

In the context of our example, we might ask what the rate of reconvictions would be in 2016 if the population had the same age distribution as 2006, or vice versa, or even what the rate of reconvictions would be in both years if the populations were the same (at the average) in age distributions. Similarly, we could ask how much of the crude difference in rates of $0.324-0.272 = `r 0.324-0.272`$ is due to differences in the age distributions between the two populations. Note that these two tasks of standardisation and decomposition are implicitly linked --- for example, if *all* of the difference is due to age-distribution differences (@tbl-stddec), then any "age-standardised rates" would be identical in both populations.  


```{r}
#| echo: false
#| label: tbl-stddec
#| tbl-cap: "If the rates of reconvictions for each age-group were the same for both populations, differences in age-distributions can still lead to differences in crude rates"
df2 <- df
df2$prev <- rep(df2$prev[1:2],2)
df2$reconvicted <- round(df2$prev*df2$offenders)
df2 <- df2 |> mutate(age_prop = round(offenders/sum(offenders),3),.after=offenders)
# dgnpop(df2,"year","prev","Age","size") |> dg_table()
df2c <- df2 |> group_by(year) |> summarise(
  Age = "Total (Crude)",offenders=sum(offenders),age_prop=1,reconvicted=sum(reconvicted),
  prev = round(reconvicted/offenders,3)
) 
gt::gt(df2 |> mutate(prev=round(prev,3)) |> arrange(desc(Age)) |>
         bind_rows(df2c))


```

```{r}
#| echo: false
#| label: tbl-stddec2
#| tbl-cap: "In the case that the two age distributions are equal, then none of the difference in crude rates can be attributable to age distribution differences"
df3 <- df
df3$offenders[3] <- round((df3$offenders[1]/df3$offenders[2]) * df3$offenders[4])
df3$reconvicted = round(df3$offenders*df3$prev)
df3 <- df3 |> mutate(age_prop = round(offenders/sum(offenders),3),.after=offenders)
df3c <- df3 |> group_by(year) |> summarise(
  Age = "Total (Crude)",offenders=sum(offenders),age_prop=1,reconvicted=sum(reconvicted),
  prev = round(reconvicted/offenders,3)
) 
gt::gt(df3 |> mutate(prev=round(prev,3)) |> arrange(desc(Age)) |>
         bind_rows(df3c))
```


There are two broad approaches to the task of decomposition, and these are largely determined by the type of data available. Regression based methods of decomposition use unit level data, regressing rates (or binary events) onto a set of explanatory factors, and then estimating the counterfactual had each unit been in the alternative population. One such regression based decomposition method is widely referred to as Oaxaca-Blinder decomposition [@Oaxaca1973-ef; @Blinder1973-ig], but as a whole these techniques can more generally be achieved by following the logic of the excellent **{{marginaleffects}}** package workflow, in that aggregating unit-level observed and counterfactual outcomes up to the level of the two populations in different ways can provide the relevant average predictions (for standardised rates) and comparisons (for decomposition of the crude difference in the two rates). 

The second approach to decomposition can be traced back to Kitagawa [-@Kitagawa1955-sa], who used population level data to separate differences between two rates into differences between population characteristics vs differences between rates of occurrence. The terms "Oaxaca-Blinder decomposition" and "Kitagawa decomposition" are often used interchangeably, and the confusion is perhaps understandable given that the two are equivalent in the special case of a binary outcome and a linear probability model. The difference here is that Kitagawa's approach needs only population proportions, and not the individual unit level data. The equivalence in this special case is demonstrated in detail in Oaxaca [@Oaxaca2023-gt], and the idea is not dissimilar to how regression models can be estimated from the sample moments (i.e. how path tracing rules allow us to estimate regression models from covariance matrices alone). 

```{r}
#| include: false
#| echo: false
df |> group_by(year) |> mutate(age_prop = offenders/sum(offenders),.after=offenders) |>
  mutate(age_prop=round(age_prop,3),prev=round(prev,3)) |>
  arrange(desc(Age)) |>
  gt::gt()
```
In practice Kitagawa's starting point is a population level table such as that in @tbl-eg1. The decomposition of the crude rates here is relatively straightforward in that it can be split into two parts: the proportions in each age-group, and the age-group-specific rates. In our example, this is just that the 2016 rate of $0.272 = (0.301 \cdot 0.447) + (0.248 \cdot 0.553)$. We can write this more generally for an arbitrary number of sub-groups as:  
  
$$
\begin{align}
r &= \sum\limits_{i}a_ib_i \\
\text{where:}& \\
a &= \text{group specific rate} \\
b &= \text{proportion of population in group} \\
\end{align}
$$ {#eq-rab}


In this formulation, it is straightforward to compute "standardised rates" by calculating the rates $\sum\limits_{i}a_ib_i$ under different counterfactuals such as e.g., if the 2016 population had the same age distribution as 2006, giving $(0.301 \cdot 0.599) + (0.248 \cdot 0.401) = 0.280$, or where the age distribution was held at the average of the two populations (@eq-manage).   
$$
\begin{align}
\text{age-standardised-}r^{2006} = \left( 0.373 \cdot \frac{0.599 + 0.447}{2} \right) + \left( 0.252 \cdot \frac{0.401 + 0.553}{2} \right) &= 0.315 \\
\quad \\
\end{align}
$$ {#eq-manage1}
$$
\begin{align}
\text{age-standardised-}r^{2016}  = \left( 0.301 \cdot \frac{0.599 + 0.447}{2} \right) + \left( 0.248 \cdot \frac{0.401 + 0.553}{2} \right) &= 0.276 \\
\quad \\
\text{difference} &= 0.039 \\
\end{align}
$$ {#eq-manage}

Standardisation holding the compositional factors at their average allows us to consider the decomposition question: If the two populations had the same age distribution, how much would they differ in their rates? The difference between standardised rates (`r 0.039`), taken as a proportion of the difference between crude rates (`r 0.324-0.272`), this allows us to attribute 75% of the difference in crude rates as being due to differences in the group-specific rates in the populations (and their other stray causes), with 25% being explained by differences in the age distribution. We can similarly see this by substituting in the averages of the group-specific rates instead of the averages of the group proportions, for which we get $0.302$ and $0.289$, and a difference of $0.013$ --- 25% of the crude rate difference (@eq-manrate).

$$
\begin{align}
\text{rate-standardised-}r_{2006} = \left( \frac{0.373 + 0.301}{2} \cdot 0.599 \right) + \left( \frac{0.252 + 0.248}{2} \cdot 0.401 \right) &= 0.302 \\
\quad \\
\end{align}
$$ {#eq-manrate1}
$$
\begin{align}
\text{rate-standardised-}r_{2016}  = \left( \frac{0.373 + 0.301}{2} \cdot 0.447 \right) + \left( \frac{0.252 + 0.248}{2} \cdot 0.553 \right) &= 0.289 \\
\quad \\
\text{difference} &= 0.013 \\
\end{align}
$$ {#eq-manrate}

We can express this method of standardisation and decomposition using letters to differentiate the compositional factors that make up the rate (age-group proportions, age-group specific rates) across each sub-population $i$, with superscripts to distinguish these factors from each of two populations $p$ and $p'$:  

$$
\begin{align}
b\text{-standardised-}r^p &= \sum\limits_{i}\frac{b^p_i+b^{p'}_i}{2} a^p_i \quad\quad\quad a\text{-standardised-}r^p = \sum\limits_{i}\frac{a^p_i+a^{p'}_i}{2} b^p_i \\
\end{align}
$$ {#eq-kgwstd1}
$$
\begin{align}
b\text{-standardised-}r^{p'} &= \sum\limits_{i}\frac{b^p_i+b^{p'}_i}{2} a^{p'}_i \quad\quad\quad a\text{-standardised-}r^{p'} = \sum\limits_{i}\frac{a^p_i+a^{p'}_i}{2} b^{p'}_i \\
\end{align}
$$ {#eq-kgwstd}

$$
\Delta\text{crude-}r = \sum\limits_{i}\frac{b^p_i+b^{p'}_i}{2} (a^p_i-a^{p'}_i) + \sum\limits_{i}\frac{a^p_i+a^{p'}_i}{2} (b^p_i-b^{p'}_i) 
$$ {#eq-kgw}


<!-- $$ -->
<!-- \begin{align} -->
<!-- ab - AB &= \frac{b+B}{2} (a-A) + \frac{a+A}{2} (b-B) \\ -->
<!-- 2(ab - AB) &= (b+B) (a-A) + (a+A)(b-B) \\ -->
<!-- 0 &= (b+B) (a-A) + (a+A)(b-B) - 2(ab - AB) \\ -->
<!-- 0 &= ba-bA+Ba-BA + ba-Ba+bA-BA - 2(ab - AB) \\ -->
<!-- 0 &= 2ba-2BA - 2(ab - AB) \\ -->
<!-- 0 &= 0 \\ -->
<!-- \end{align} -->
<!-- $$ -->


Note that the neatness of decomposing differences in crude rates into the sum of differences in standardised rates cannot be achieved when using one population as the standard because of the interaction between the factors. In @eq-p1s, the first two terms represent the differences in $b-$ and $a-$standardised rates using population $p'$ as the standard, but to equate this to the difference in crude rates requires the addition of the interaction term. In contrast, Kitagawa's formulation in @eq-kgw divides the interaction term equally between the two sets of standardised rates. 

$$
\Delta\text{crude-}r = \sum\limits_{i}b^{p'}_i(a^p_i-a^{p'}_i) \quad + \quad \sum\limits_{i}a^{p'}_i(b^p_i-b^{p'}_i) \quad + \quad \sum\limits_{i}(a^p_i-a^{p'}_i)(b^p_i-b^{p'}_i)
$$ {#eq-p1s}

When extended to greater numbers of compositional factors, algebraically manipulating differences in weighted averages becomes cumbersome as interaction terms proliferate. In a series of articles [-@Das-Gupta1978-er; -@Das-Gupta1989-no; -@Das-Gupta1991-qu; -@Das-Gupta1993-ie; -@Das-Gupta1994-fl], Das Gupta extended Kitagawa's work to a generalised form for any number of factors, and then followed this with standardisation across more than two populations. Das Gupta's approach is essentially a combinatorics problem --- to compute the rate were only one factor to differ between the two populations, the calculation of the rate is averaged over all possible counterfactuals (different combinations of other factors changing vs being held equal). These are weighted relative to the number of factors varied/held equal, meaning that their importance is lessened the farther the counterfactual is from an actual population.^[does that make any sense at all?? "importance of the specific factor gradually fades when further changes to other factors are introduced in the counterfactual."]

Das Gupta's general solution for the decomposition of two rates can be written as:  

$$
\Delta\text{crude-}r = \sum\limits_{\vec{\alpha} \in K}Q(\vec{\alpha}^p) - Q(\vec{\alpha}^{p'})
$$ {#eq-dg1}

Where $K$ is the set of factors $\alpha, \beta, ..., \kappa$, which may take the form of vectors over sub-populations. $Q(\vec{\alpha}^p)$ denotes the rate in population $p$ holding $K \setminus \{\alpha\}$ (all factors other than $\alpha$), standardised across populations $p$ and $p'$. The total crude rate difference is the sum of all standardised rate differences, and the standardisation $Q$ is expressed as:  


<!-- $$ -->
<!-- Q(\vec{\alpha}) = \sum\limits^{|K|-1}_{j=1} \frac{ f({K \setminus \{\alpha\} \choose j-1}, \vec{\alpha}) } { |K| {|K|-1\choose j-1} } -->
<!-- $$ -->
<!-- $f({K \setminus \{\alpha\} \choose j-1}, \alpha)$ is the sum of, for all possible sets of $j-1$ factors from $K\setminus\{\alpha\}$, the rate function calculated with $j$ factors from population $0$ and the rest from population $1$.  -->

<!-- $$ -->
<!-- Q(\vec{\alpha}^p) = \sum\limits_{j=1}^{\begin{cases}  -->
<!-- |K|\,/\,2 & \text{if }|K|\text{ is even} \\ -->
<!-- (|K|-1)\,/\,2 & \text{if }|K|\text{ is odd} \ -->
<!-- \end{cases}} \frac{ \sum\limits_{L \in {K \setminus \{\alpha\} \choose j-1}}f(\{L^p,(K\setminus L)^{p'},\vec{\alpha}^p\}) + f(\{L^{p'},(K\setminus L)^p,\vec{\alpha}^p\})} { |K| {|K|-1\choose j-1} } -->
<!-- $$ -->


<!-- $$ -->
<!-- Q(\vec{\alpha}^p) = \sum\limits^{|K|-1}_{j=1} \frac{ \sum\limits_{L \in {K \setminus \{\alpha\} \choose j-1}}f(\{L^p,(K\setminus L)^{p'},\vec{\alpha}^p\}) + f(\{L^{p'},(K\setminus L)^p,\vec{\alpha}^p\})} { |K| {|K|-1\choose j-1} } -->
<!-- $$ -->


$$
Q(\vec{\alpha}^p) = \sum\limits_{j=1}^{\lceil \frac{|K|}{2} \rceil} \frac{ \sum\limits_{L \in {K \setminus \{\alpha\} \choose j-1}}f(\{L^p,(K\setminus L)^{p'},\vec{\alpha}^p\}) + f(\{L^{p'},(K\setminus L)^p,\vec{\alpha}^p\})} { |K| {|K|-1\choose j-1} }
$$ {#eq-dg2}

Where $f(K)$ is the function that defines the calculation of the rate (this could be the simple sum of products such as the reconvictions example above, or something arbitrarily complex that returns a single real value).  

For example, when there are 5 factors, $K = {a, b, c, d, e}$, and the rate is calculated as the simple product, then:  

$$
\begin{align}
Q(a) =& \frac{abcde + ab'c'd'e'}{5} + \\
& \frac{abcde' + abcd'e + abc'de +ab'cde + ab'c'd'e + ab'c'de' + ab'cd'e' +abc'd'e'}{20} + \\
& \frac{abcd'e' + abc'de' + abc'd'e + ab'c'de + ab'cd'e + ab'cde'}{30}\\
\end{align}
$$



<!-- For example, when there are 3 factors, $K = {a, b, c}$, and the rate is calculated as the simple product:   -->
<!-- $$ -->
<!-- \begin{align} -->
<!-- \Delta\text{crude-}r =&  \left( \frac{bc+b'c'}{3} + \frac{bc'+b'c}{6} \right) (a-a') \quad + \\ -->
<!-- & \quad \left( \frac{ac+a'c'}{3} + \frac{ac'+a'c}{6} \right) (b-b') \quad + \\ -->
<!-- & \quad \left( \frac{ab+a'b'}{3} + \frac{ab'+a'b}{6} \right) (c-c') -->
<!-- \end{align} -->
<!-- $$ -->

As with how Kitagawa approached the scenario with two compositional factors, Das Gupta's method distributed interactions equally across all factors, meaning that standardised rates and decompositions equate to average treatment effects at the level of populations. Where $Y$ is a binary outcome, and $P$ the binary population indicator (note we switch from $p$ and $p'$ to using $0$ and $1$ in line with the conventional counterfactual notation), using $Y^0$ to denote the random variable of outcomes under population $0$, and $Y^1$ as the outcomes under population $1$, we can express crude rates as the expected value of observed outcomes for each population, and the crude rate difference can be written as the straightforward contrast :  

$$
\Delta\text{crude-}r = \mathbb{E}[Y^1|P=1] - \mathbb{E}[Y^0|P=0]
$$ {#eq-cf1}

Das Gupta's methodology estimates, for instance, what the population level rates would be if every factor included in the decomposition was held equal (thereby isolating the differences due to rates rather than population characteristics). The term "held equal" here is, crucially, averaged across the two populations, meaning it is invariant to differences in size between the two populations, and equates to the counterfactual statement in @eq-cf_pmrate. Similarly, we could consider how the rates would differ if the probability of the rate event were equal in the two populations but they differed in their compositional structure (@eq-cf_pmall). In the case of the two factor decomposition, the statements in @eq-cf_pmrate and @eq-cf_pmall, when estimated from individual-level data correspond to the standardised rates as would be calculated from @eq-manrate and @eq-manage on population level proportions (see supplementary for demonstration in R). When more factors are included in the decomposition, this second is broken down further to counterfactual statements of the expected rates were the populations to differ in each factor separately, while holding all others equal.^[started to write "It is important to note that rate decomposition methods are focused at the level of the population, not the individuals within them, so estimands are more easily written at this level rather than in terms of expectations of population averaged individual-level effects)". and thought about $\Delta \text{crude-}r = \mathbb{E} [R^1|P=1] -  \mathbb{E} [R^0|P=0]$ and $\Delta \text{P-rate-standardised-}r = \mathbb{E} [R^1 - R^0]]$, but it doesn't quite work i don't think] 


$$
\mathbb{E} [ \mathbb{E}[Y^1|P=0], \mathbb{E}[Y^1|P=1] ] - \mathbb{E} [ \mathbb{E}[Y^0|P=0], \mathbb{E}[Y^0|P=1] ]
$$ {#eq-cf_pmrate}
$$
\mathbb{E} [ \mathbb{E}[Y^0|P=1], \mathbb{E}[Y^1|P=1] ] - \mathbb{E} [ \mathbb{E}[Y^0|P=0], \mathbb{E}[Y^1|P=0] ] 
$$ {#eq-cf_pmall}

<!-- $$ -->
<!-- \Delta \text{crude-}r = \mathbb{E} [R^1|P=1] -  \mathbb{E} [R^0|P=0] \\ -->
<!-- \Delta \text{P-rate-standardised-}r = \mathbb{E} [R^1 - R^0] \\ -->
<!-- \Delta \text{P-a-standardised-}r = \mathbb{E} [R^1 - R^0] \\ -->
<!-- $$ -->

While the regression based approach to decomposition brings with it a lot of the flexibility of regression modelling in general, the drawback of the Kitagawa/Das Gupta approach to decomposition is also its clear benefit --- while the decomposition (being algebraic) is restricted to fully cross-classified data (i.e., specifying sub-population rates for the full joint distribution of all variables, with no missing values), all that is needed is the aggregated data (which is often all that researchers can get from various official statistics). Furthermore, the arbitrary complexity of the rate function in Das Gupta's approach extends its utility to scenarios where we may want to decompose something other than prevalence/incidence..

:::{.column-margin}
TODO (not sure about this. there's a bit of a question mark over all of DG for me in that if we're working with population level data, surely we can just 'uncount' it to get back individual level data, and then use regression methods of decomposition (i.e. the bit shown in the supplementary)).  

so what's the advantage of DG, now that computers make uncounting so easy?
:::

  
While Kitagawa's decomposition technique is well used (citations increasing), references to Das Gupta have not seen the same popularity. This is quite possibly due to the computational complexity involved with calculating all permutations of counterfactual population states. Das Gupta himself provided code in Fortran for examples of his method [-@Das-Gupta1993-ie], and, somewhat more recently, standalone Windows executables [@Wang2000-hc] and packages in Stata [@Li2017-kj] have been produced. 

Here we present the **{{DasGuptR}}** package for R that implements the full functionality described in Das Gupta's 1993 manual for standardising and decomposing rates as a function of a set of $K$ vectors across $N$ populations. Associated package vignettes provide examples of how this can be adapted in order to bootstrap standard errors for decomposition effects [as in @Wang2000-hc], and how to use standardised cell-specific rates in order to decompose rate differences in to effects for each category of a compositional variable (e.g., allowing us to ask if the difference is driven by change in one specific age-group more than others), as described by @Chevan2009-dt. 


# The DasGuptR package

For a full explanation of Das Gupta's methodology of standardisation and decomposition, see @Das-Gupta1993-ie, from which examples below are taken. We follow the same exposition: building up the number of factors, factors as vectors, different rate functions, cross-classified population structures, and finally extensions to more than just 2 populations.  

## package workflow

The DasGuptR workflow requires data to be in long format, with variables denoting the population and each compositional factor. 

```{r}
#| message: false
library(DasGuptR)
eg.dg <- data.frame(
  pop = c("pop1","pop2"),
  alpha = c(.6,.3),
  beta = c(.5,.45)
)
eg.dg
```

The workhorse of the DasGuptR package is `dgnpop()`, which computes the crude rates and the standardised rates for $K$ factors across $N$ populations.  

It's crucial to be aware that the factors referred to in the outputs of DasGuptR are those that are **not** being held constant in the standardisation. Very often when standardisation is done over a smaller number of factors --- say $x,y,z$ --- researchers use "$xy$-standardised rates" to refer to rates were both $x$ and $y$ held constant across the two populations (similar to the names used in @eq-manage1 to @eq-manrate2). In DasGuptR, these would be presented as the "$K-z$-standardised rates". 

```{r}
dgnpop(eg.dg, pop = "pop", factors = c("alpha","beta"))
```

These can be quickly turned into a wide table of 'decomposition effects' in the style of Das Gupta using `dg_table()`, which --- when working with just two populations --- calculates and displays differences in standardised rate and also expresses these as a percentage of the crude rate difference:  

```{r}
dgnpop(eg.dg, pop = "pop", factors = c("alpha","beta")) |>
  dg_table()
```

## more factors

The addition of more factors into the composition of a population rate is handled in `dgnpop()` by simply adding to the `factors` argument. The default behaviour will take the rate to be the product of all factors specified.  

**Example 2.3:** Percentage having non-marital live births as the product of four factors for white women aged 15 to 19 in the United States, in the years 1971 and 1979.  
```{r}
eg2.3 <- data.frame(
  pop = c(1971, 1979),
  # non-marital live births x 100 / non-marital pregnancies
  birth_preg = c(25.3, 32.7),
  # non-marital pregnancies / sexually active single women
  preg_actw = c(.214, .290),
  # sexually active single women / total single women
  actw_prop = c(.279, .473),
  # total single women / total women
  w_prop = c(.949, .986)
)

dgnpop(eg2.3,
  pop = "pop",
  factors = c("birth_preg", "preg_actw", "actw_prop", "w_prop")
) |>
  dg_table()
```


## factors as vectors  

It is often the case that we have data for each compositional factor on a set of sub-populations, and the crude rates for the population are the aggregated cell-specific rates.  

In these cases, `dgnpop()` requires the user to provide an appropriate rate function that aggregates up to a summary value for each population.  For instance, in the example below, the cell-specific rates are calculated as the product of 3 factors, and the population rate is the sum of the cell-specific rates, so the user would specify `ratefunction = "sum(a*b*c)"`. 

**Example 4.3:** Crude birth rate per 1000 as a function of three vector factors in Taiwan, in the years 1960 and 1970:  
```{r}
eg4.3 <- data.frame(
  agegroup = rep(1:7, 2),
  pop = rep(c(1970, 1960), e = 7),
  # number of births in age-group x 1000 / number of married women in age-group
  bm = c(488, 452, 338, 156, 63, 22, 3,
         393, 407, 369, 274, 184, 90, 16),
  # number of married women in-age group / total women in age-group
  mw = c(.082, .527, .866, .941, .942, .923, .876,
         .122, .622, .903, .930, .916, .873, .800),
  # total women in age-group / total population
  wp = c(.058, .038, .032, .030, .026, .023, .019,
         .043, .041, .036, .032, .026, .020, .018)
)

dgnpop(eg4.3,
  pop = "pop", factors = c("bm", "mw", "wp"),
  ratefunction = "sum(bm*mw*wp)"
) |>
  dg_table()
```

For most purposes when working with vector factors it is the population-level rates that are of interest, and providing an appropriate rate function will suffice for the decomposition. If the rate function provided does not aggregate up to a summary value, then `dgnpop()` will return an array of standardised cell-specific rates of the same length as the number of sub-populations. In order for this to work, the user is also required to specify the variable(s) indicating the sub-population in `id_vars` argument.[In the simple case where there is only one variable indicating a single set of sub-populations (e.g., different age-groups), then these could also be calculated by running `dgnpop()` on the data for each sub-population separately.]

Aggregating these post-hoc will simply retrieve the population-level standardised rates, but cell-specific standardisation may be of use for those looking to calculate *category effects* as detailed in [Chevan & Sutherland 2009](https://doi.org/10.1353/dem.0.0060){target="_blank"} to examine the amount to which a difference in rates is attributable to differences in _specific_ sub-populations (see associated vignette with `vignette("category_effects", package="DasGuptR")`). 

```{r}
#| eval: false
dgnpop(eg4.3,
  pop = "pop", factors = c("bm", "mw", "wp"),
  id_vars = c("agegroup"),
  ratefunction = "bm*mw*wp"
)
#>          rate  pop std.set factor agegroup
#> 1   2.4892880 1970    1960     bm        1
#> 2  10.2678580 1970    1960     bm        2
#> 3  10.1688427 1970    1960     bm        3
#> 4   4.5237920 1970    1960     bm        4
#> 5   1.5217020 1970    1960     bm        5
#> 6   0.4250290 1970    1960     bm        6
#> 7   0.0465280 1970    1960     bm        7
#> 8   2.0046930 1960    1970     bm        1
#> 9   9.2456155 1960    1970     bm        2
#> ..  ...       ...     ...      ...     ...
#> ..  ...       ...     ...      ...     ...
```

## rate functions

Das Gupta's methodology generalises to rates that are not simply products of the set of factors. The `ratefunction` argument of `dgnpop()` allows the user to define a custom rate function (function $f()$ in @eq-dg2). This may be as simple as a subtraction `a-b` (Example 3.1 below), or something more complicated that aggregates over different combinations of factors `sum(a*b)/sum(a*b*c)` (Example 4.4 below).  

**Example 3.1:** Crude rate of natural increase, US in years 1940 and 1960
```{r}
eg3.1 <- data.frame(
  pop = c(1940, 1960),
  # births x 1000 / total population
  crude_birth = c(19.4, 23.7),
  # deaths x 1000 / total population
  crude_death = c(10.8, 9.5)
)

dgnpop(eg3.1,
  pop = "pop", factors = c("crude_birth", "crude_death"),
  ratefunction = "crude_birth-crude_death"
) |>
  dg_table()
```

**Example 4.4:** Illegitimacy Ratio as a function of four vector factors: United States, 1963 and 1983
```{r}
eg4.4 <- data.frame(
  pop = rep(c(1963, 1983), e = 6),
  agegroup = c("15-19", "20-24", "25-29", "30-34", "35-39", "40-44"),
  # number of women in age-group / total women
  A = c(.200, .163, .146, .154, .168, .169, 
        .169, .195, .190, .174, .150, .122),
  # number of unmarried women in age-group / number of women in age-group
  B = c(.866, .325, .119, .099, .099, .121,
        .931, .563, .311, .216, .199, .191),
  # births to unmarried women in age-group / number of unmarried women in age-group
  C = c(.007, .021, .023, .015, .008, .002,
        .018, .026, .023, .016, .008, .002),
  # births to married women in age-group / married women in age-group
  D = c(.454, .326, .195, .107, .051, .015,
        .380, .201, .149, .079, .025, .006)
)

dgnpop(eg4.4,
  pop = "pop", factors = c("A", "B", "C", "D"),
  id_vars = "agegroup", ratefunction = "sum(A*B*C) / (sum(A*B*C) + sum(A*(1-B)*D))"
) |>
  dg_table()
```

The `ratefunction` argument can be given any string that when parsed and evaluated will return a summary value for a rate. At the point at which the string is evaluated, each factor (or vector-factor) is stored in a named list, meaning the function must simply refer to those factors by name. It is possible, for instance, to define a custom function in the user's environment, and provide a call to that function to the `ratefunction` argument of `dgnpop()`. Example 4.4 above can also be achieved using the code below. There is no real limit to the complexity of the rate function the user wishes to specify, and Das Gupta provides one such example in which the rate is obtained iteratively via Newton-Raphson (See Example 4.1 in the supplementary of additional examples).  

```{r}
#| eval: false
myratef <- function(a, b, c, d) {
  return(sum(a * b * c) / (sum(a * b * c) + sum(a * (1 - b) * d)))
}

dgnpop(eg4.4,
  pop = "pop", factors = c("A", "B", "C", "D"),
  id_vars = "agegroup", ratefunction = "myratef(A,B,C,D)"
) |>
  dg_table()
```


## population structures and cross-classified data  

Very often, we have data on factors across a set of sub-populations because we are interested specifically in effects of the sub-population structure differences between two populations --- i.e. separating out how much the crude rate differences are due to differences in the structure of the populations vs differences in the cell-specific rates.  

To do this, we require data on the sizes (or relative sizes) of each sub-population. The simplest case here would be if we had data on a single set of sub-populations (e.g., age-groups), and had the group-specific rates and group sizes. The crude rates for the population would be simply the sum of all the group-specific rates weighted by the relative size of the group.  

**Example 5.1:** Household Headship Rates per 100: United States, years 1970 and 1985
```{r}
eg5.1 <- data.frame(
  age_group = rep(c("15-19", "20-24", "25-29", "30-34", 
                    "35-39", "40-44", "45-49", "50-54", 
                    "55-59", "60-64", "65-69", "70-74", 
                    "75+"), 2),
  pop = rep(c(1970, 1985), e = 13),
  # number in age-group / total population * 100
  size = c(
    12.9, 10.9, 9.5, 8.0, 7.8, 8.4, 8.6, 7.8, 7.0, 5.9, 4.7, 3.6, 4.9, 
    10.1, 11.2, 11.6, 10.9, 9.4, 7.7, 6.3, 6.0, 6.3, 5.9, 5.1, 4.0, 5.5
  ),
  # age-group specific rate
  rate = c(
    1.9, 25.8, 45.7, 49.6, 51.2, 51.6, 51.8, 54.9, 58.7, 60.4, 62.8, 66.6, 66.8, 
    2.2, 24.3, 45.8, 52.5, 56.1, 55.6, 56.0, 57.4, 57.2, 61.2, 63.9, 68.6, 72.2
  )
)
```

In this case, we can decompose this into the rate-standardised and age-standardised rates in various ways. 

1. Given that the rate is defined as a weighted sum $\sum\limits_iw_ir_i$, by creating a new column of weights, we can simply include this in the set of compositional factors as previously. As the weights are simply the proportions of the population in each age-group, and we have percentages, we can simply divide by 100:  
```{r}
eg5.1$age_str <- eg5.1$size / 100

dgnpop(eg5.1,
  pop = "pop", factors = c("age_str", "rate"),
  id_vars = "age_group", ratefunction = "sum(age_str*rate)"
) |>
  dg_table()
```

2. Alternatively, we could include the conversion to proportions *inside* the rate function, including the original cell sizes variable in the factors. Because the inputs to the rate function here are the set of vector factors for each population, internal calls to `sum(size)` will give us the total population size, and `size/sum(size)` will give us our proportions:  
```{r}
dgnpop(eg5.1,
  pop = "pop", factors = c("size", "rate"),
  id_vars = "age_group", ratefunction = "sum( (size/sum(size))*rate )"
) |>
  dg_table()
```

3. Finally, we can instead provide the variable indicating the size of each sub-population into the `crossclassified` argument of `dgnpop()`. 
```{r}
dgnpop(eg5.1,
  pop = "pop", factors = c("rate"),
  id_vars = "age_group", 
  crossclassified = "size"
) |>
  dg_table()
```

This latter approach can be extended to situations in which we have cross-classified data - i.e. individual sub-populations are defined by the combination of multiple variables such as age and race, as Example 5.3 below. 

In these cases, defining weights as `size/sum(size)` will collapse the two cross-classified factors that make up the structure of the population (age and race), thereby getting us only part of the way. This allows us to decompose rate differences into "rate" vs "age-and-race" differences, but leaves us unable to separate out differences in age distributions from differences in race distributions. Instead, providing the cell-specific sizes to the `crossclassified` argument will re-express the proportion of the population in a given cell as the product of symmetrical expressions corresponding to each of the cross-classified variables.  

Using $i$ to donote a specific sub-population that is uniquely identified through the set of cross-classified variables $K: \{\alpha,\beta,...,\kappa\}$, and $i_\kappa$ to denote the specific level of $\kappa$ for sub-population $i$, we can indicate the size of sub-population $i$ with $N_{\forall \kappa \in K, \kappa = i_\kappa}$, and the expression $N_{\forall \kappa \in K, \kappa = .}$ denotes the total population size, summed across all levels of all variables. The Das Gupta methodology expresses the proportion of the population in sub-population $i$ as a product of $|K|$ variables (@eq-popstr1). 

$$
\frac{N_{\forall \kappa \in K, \kappa = i_\kappa}}
{N_{\forall \kappa \in K, \kappa = .}} = 
\hat\alpha_{\forall \kappa \in K, \kappa = i_\kappa} \hat\beta_{\forall \kappa \in K, \kappa = i_\kappa} ... \hat\kappa_{\forall \kappa \in K, \kappa = i_\kappa}  
$$ {#eq-popstr1}
  
These are defined as the product of ratios that aggregate over different combinations of the cross-classified variables (@eq-popstr2), which are then standardised via the method described earlier (@eq-dg1, @eq-dg2), and multiplied by the average cell-specific rates (i.e., $\frac{r_i + r_i'}{2}$) and aggregated up to the population level to obtain $\alpha\ldots\kappa$-standardised rates.  

$$
\hat\alpha_{\forall \kappa \in K, \kappa = i_\kappa} = \prod\limits_{j=0}^{|K|-1} \left( \prod\limits_{L\in {K \setminus \{\alpha\} \choose j}} \frac{N_{\forall \kappa \in K,\ \kappa = \begin{cases} i_\kappa & \text{if } \kappa \in \{\alpha, L\} \\ . & \text{otherwise} \end{cases}} }{N_{\forall \kappa \in K,\ \kappa = \begin{cases} i_\kappa & \text{if } \kappa \in L \\ . & \text{otherwise} \end{cases}} }\right)^{\frac{1}{|K|{|K|-1 \choose j}}}
$$ {#eq-popstr2}

<br>

<!-- $$ -->
<!-- \begin{equation} -->
<!-- \hat\alpha_{\forall \kappa \in K, \kappa = i_\kappa} = \prod\limits_{j=0}^{|K|-1}  -->

<!-- \left( \prod\limits_{L\in {K \setminus \{\alpha\} \choose j}} -->

<!-- \frac{N_{ -->
<!-- \forall \kappa \in K,\ \kappa = -->
<!-- \begin{cases} i_\kappa & \text{if } \kappa \in \{\alpha, L\} \\ . & \text{otherwise} \end{cases}} -->
<!-- } -->
<!-- {N_{\forall \kappa \in K,\ \kappa = \begin{cases} i_\kappa & \text{if } \kappa \in L \\ . & \text{otherwise} \end{cases}} -->
<!-- } -->

<!-- \right)^{\frac{1}{|K|{|K|-1 \choose j}}} -->
<!-- \end{equation} -->
<!-- $$ -->

**Example 5.3:** Death rates per 1000: United States, years 1970 and 1985
```{r}
eg5.3 <- data.frame(
  race = rep(rep(1:2, e = 11), 2),
  age = rep(rep(1:11, 2), 2),
  pop = rep(c(1985, 1970), e = 22),
  # number of people in age-race-group
  size = c(
    3041, 11577, 27450, 32711, 35480, 27411, 19555, 19795, 15254, 8022, 
    2472, 707, 2692, 6473, 6841, 6547, 4352, 3034, 2540, 1749, 804, 236,
    2968, 11484, 34614, 30992, 21983, 20314, 20928, 16897, 11339, 5720, 
    1315, 535, 2162, 6120, 4781, 3096, 2718, 2363, 1767, 1149, 448, 117
  ),
  # death rate in age-race-group
  rate = c(
    9.163, 0.462, 0.248, 0.929, 1.084, 1.810, 4.715, 12.187, 27.728, 64.068, 157.570, 
    17.208, 0.738, 0.328, 1.103, 2.045, 3.724, 8.052, 17.812, 34.128, 68.276, 125.161, 
    18.469, 0.751, 0.391, 1.146, 1.287, 2.672, 6.636, 15.691, 34.723, 79.763, 176.837, 
    36.993, 1.352,0.541, 2.040, 3.523, 6.746, 12.967, 24.471, 45.091, 74.902, 123.205
  )
)

dgnpop(eg5.3,
  pop = "pop", factors = c("rate"),
  id_vars = c("race", "age"), crossclassified = "size"
) |>
  dg_table()
```

## N populations

When standardising across more than two populations, computing the standardisation across all pairs of populations returns $N-1$ sets of standardised rates for each population, and the decomposition between populations are internally inconsistent --- i.e. differences in standardised rates between populations 1 and 2, and between 2 and 3, should (but do not) sum to the difference between 1 and 3.  

Das Gupta [-@Das-Gupta1993-ie] provided a secondary procedure that takes sets of pairwise standardised rates and resolves these problems. This is presented in @eq-npop, with $\bar r_{x|y}$ denoting a standardised rate in population $x$ that is standardised across populations $x$ and $y$. When given more than two populations, `dgnpop()` will automatically undertake this procedure. The resulting standardised across all N populations and can be used as previously with `dg_table()` and `dg_plot()`.  

$$
\bar r_{1|23\ldots N} = \frac{\sum\limits_{i=2}^{N}\bar r_{1|i}}{N-1} + 
\frac{\sum\limits_{i=2}^{N} 
    \sum\limits_{j\neq1,i}^{N} 
        \bar r_{i|j} - (N-2) \bar r_{i|1} }{N(N-1)}
$$ {#eq-npop}


**Example 6.5:** Illegitimacy Ratio as a function of four vector factors: United States, years 1963, 1968, 1973, 1978, and 1983
```{r}
eg6.5 <- data.frame(
  pop = rep(c(1963, 1968, 1973, 1978, 1983), e = 6),
  agegroup = c("15-19", "20-24", "25-29", "30-34", "35-39", "40-44"),
  # number of women in age-group / total women
  A = c(
    .200, .163, .146, .154, .168, .169,
    .215, .191, .156, .137, .144, .157,
    .218, .203, .175, .144, .127, .133,
    .205, .200, .181, .162, .134, .118,
    .169, .195, .190, .174, .150, .122
  ),
  # number of unmarried women in age-group / number of women in age-group
  B = c(
    .866, .325, .119, .099, .099, .121,
    .891, .373, .124, .100, .107, .127,
    .870, .396, .158, .125, .113, .129,
    .900, .484, .243, .176, .155, .168,
    .931, .563, .311, .216, .199, .191
  ),
  # births to unmarried women in age-group / number of unmarried women in age-group
  C = c(
    .007, .021, .023, .015, .008, .002,
    .010, .023, .023, .015, .008, .002,
    .011, .016, .017, .011, .006, .002,
    .014, .019, .015, .010, .005, .001,
    .018, .026, .023, .016, .008, .002
  ),
  # births to married women in age-group / married women in age-group
  D = c(
    .454, .326, .195, .107, .051, .015,
    .433, .249, .159, .079, .037, .011,
    .314, .181, .133, .063, .023, .006,
    .313, .191, .143, .069, .021, .004,
    .380, .201, .149, .079, .025, .006
  )
)
```

<!-- As an illustration, standardisation independently across pairs of populations above returns decomposition effects below.  -->

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| message: false -->
<!-- p12 <- dgnpop(eg6.5[eg6.5$pop %in% c(1963, 1968), ], -->
<!--   pop = "pop", factors = c("A", "B", "C", "D"), -->
<!--   id_vars = "agegroup", -->
<!--   ratefunction = "1000*sum(A*B*C) / (sum(A*B*C) + sum(A*(1-B)*D))" -->
<!-- ) |> -->
<!--   dg_table() -->
<!-- p23 <- dgnpop(eg6.5[eg6.5$pop %in% c(1968, 1973), ], -->
<!--   pop = "pop", factors = c("A", "B", "C", "D"), -->
<!--   id_vars = "agegroup", -->
<!--   ratefunction = "1000*sum(A*B*C) / (sum(A*B*C) + sum(A*(1-B)*D))" -->
<!-- ) |> -->
<!--   dg_table() -->
<!-- p13 <- dgnpop(eg6.5[eg6.5$pop %in% c(1963, 1973), ], -->
<!--   pop = "pop", factors = c("A", "B", "C", "D"), -->
<!--   id_vars = "agegroup", -->
<!--   ratefunction = "1000*sum(A*B*C) / (sum(A*B*C) + sum(A*(1-B)*D))" -->
<!-- ) |> -->
<!--   dg_table() -->

<!-- tibble( -->
<!--   ` ` = c(LETTERS[1:4], "Crude"), -->
<!--   `1963 v 1968` = p12$diff, -->
<!--   `1968 v 1973` = p23$diff, -->
<!--   `1963 v 1973` = p13$diff -->
<!-- ) |> knitr::kable() -->
<!-- ``` -->


```{r}
dgnpop(eg6.5,
  pop = "pop", factors = c("A", "B", "C", "D"),
  id_vars = "agegroup",
  ratefunction = "1000*sum(A*B*C) / (sum(A*B*C) + sum(A*(1-B)*D))"
) |>
  dg_table()
```

Because using `dg_table()` with multiple populations will return standardised rates for each population, it will not return decomposition effects unless only two populations are specified:  
```{r}
dgnpop(eg6.5,
  pop = "pop", factors = c("A", "B", "C", "D"),
  id_vars = "agegroup",
  ratefunction = "1000*sum(A*B*C) / (sum(A*B*C) + sum(A*(1-B)*D))"
) |>
  dg_table(pop1 = 1963, pop2 = 1968)
```

<!-- Alternatively, the decomposition effects are returned by `dgnpop()` in the `diffs` entry:  -->

<!-- ```{r} -->
<!-- #| eval: false -->
<!-- dgnpop(eg6.5, -->
<!--   pop = "pop", factors = c("A", "B", "C", "D"), -->
<!--   id_vars = "agegroup", -->
<!--   ratefunction = "1000*sum(A*B*C) / (sum(A*B*C) + sum(A*(1-B)*D))", -->
<!--   diffs = TRUE -->
<!-- )$diffs -->
<!-- #>           diff  pop diff.calc        std.set factor -->
<!-- #> 1    1.8822361 1963 1963-1968 1973.1978.1983      A -->
<!-- #> 2    1.0659408 1963 1963-1973 1968.1978.1983      A -->
<!-- #> 3   -1.4102979 1963 1963-1978 1968.1973.1983      A -->
<!-- #> 4   -8.1697336 1963 1963-1983 1968.1973.1978      A -->
<!-- #> 5   -0.8162953 1968 1968-1973 1963.1978.1983      A -->
<!-- #> 6   -3.2925340 1968 1968-1978 1963.1973.1983      A -->
<!-- #> ...  ...       ...  ...       ...                 .. -->
<!-- #> ...  ...       ...  ...       ...                 .. -->
<!-- ``` -->

When working with multiple populations in a time series, we can get quick rough and ready plots of the standardised rates using `dg_plot()`:  

```{r}
dgnpop(eg6.5,
  pop = "pop", factors = c("A", "B", "C", "D"),
  id_vars = "agegroup",
  ratefunction = "1000*sum(A*B*C) / (sum(A*B*C) + sum(A*(1-B)*D))"
) |>
  dg_plot()
```

**Example 6.6:** Birthrates by Nine Age-Sex groups: United States, 1940 to 1990
```{r}
#| message: false
data(uspop)
# birthrate = births per 1000 for age-sex group
# thous = population in thousands of age-sex group
head(uspop)

dgo_us <- dgnpop(uspop,
  pop = "year", factors = c("birthrate"),
  id_vars = "agebin", crossclassified = "thous"
)
dg_plot(dgo_us)
```

**Example S.R:** Prevalence of reconvictions by Age and Sex: Scotland, 2004 to 2016
    
```{r}
#| message: false
data(reconv)
# prev_rate = number of reconvicted individuals for age-sex group / number of offenders in age-sex group
# offenders = number of offenders in age-sex group
head(reconv)

dg_srec <- dgnpop(reconv,
  pop = "year", factors = c("prev_rate"),
  id_vars = c("Sex", "Age"), crossclassified = "offenders"
)
dg_plot(dg_srec)
dg_table(dg_srec, 2006, 2016)
```



# Supplementary - Standardisation and decomposition with marginaleffects

```{r}
### population level data ---
pop_data <- DasGuptR::reconv |> 
  filter(year %in% c(2006,2016)) |>
  mutate(Age = ifelse(Age %in% c("under 21","21 to 25","26 to 30"),"under 30","over 30")) |>
  group_by(year,Age) |>
  reframe(
    offenders = sum(offenders),
    reconvicted = sum(reconvicted),
    prev = reconvicted/offenders
  )
```

```{r}
#| results: "hold"
## Das Gupta method on population level data:  
dgnpop(pop_data, pop = "year", factors = c("prev"), id_vars = "Age",
       crossclassified = "offenders") |>
  dg_table()
```

```{r}
#| results: "hold"
### individual level data ---
indiv_data <- 
  pop_data |> 
  select(-prev) |>
  mutate(not_reconvicted = offenders - reconvicted) |>
  pivot_longer(4:5) |>
  uncount(value) |>
  mutate(
    outcome = (name == "reconvicted")*1
  )

## linear probability model:  
modl <- lm(outcome ~ Age*year, data = indiv_data)

library(marginaleffects)
## E[Y^?|P=0]
atu <- avg_predictions(modl, variables = "year", 
                       newdata = subset(indiv_data, year == 2006))$estimate
## E[Y^?|P=1]
att <- avg_predictions(modl, variables = "year", 
                       newdata = subset(indiv_data, year == 2016))$estimate

## Crude rates ---
with(indiv_data, mean(outcome[year == 2006]))
with(indiv_data, mean(outcome[year == 2016]))
```

```{r}
#| results: "hold"
### rate-standardised rates: ---
# 2006
mean(c(atu[1], att[1]))
# 2016
mean(c(atu[2], att[2]))
# decomp
mean(c(atu[2], att[2])) - mean(c(atu[1], att[1]))
```

```{r}
#| results: "hold"
### age-standardised rates: ---
# 2006
mean(atu)
# 2016
mean(att)
# decomp
mean(att) - mean(atu)
```



# Supplementary - Additional examples from Das Gupta 1991

**Example 2.1:** Mean earnings as product of two factors for black males and white males 18 years and over, US 1980

    - Crude Rate = total earnings / total population
    - `avg_earnings` = total earnings / persons who earned
    - `earner_prop` = persons who earned / total population
  
```{r}
eg2.1 <- data.frame(
  pop = c("black", "white"),
  avg_earnings = c(10930, 16591),
  earner_prop = c(.717892, .825974)
)

dgnpop(eg2.1, pop = "pop", factors = c("avg_earnings", "earner_prop")) |>
  dg_table()
```

**Example 2.2:** Birth rate as the product of three factors: Austria and Chile, 1981.  

    - Crude Rate = births x 1000 / total population
    - `birthsw1549` = births x 1000 / women aged 15-49
    - `propw1549` = women aged 15-49 / total women
    - `propw` = total women / total population
    
```{r}
eg2.2 <- data.frame(
  pop = c("austria", "chile"),
  birthsw1549 = c(51.78746, 84.90502),
  propw1549 = c(.45919, .75756),
  propw = c(.52638, .51065)
)

dgnpop(eg2.2, pop = "pop", factors = c("birthsw1549", "propw1549", "propw")) |>
  dg_table()
```

**Example 2.4:** Total fertility rate as product of five factors: South Korea, 1960 and 1970  

    - `prop_m` = index of proportion married
    - `noncontr` = index of noncontraception
    - `abort` = index of induced abortion
    - `lact` = index of lactational infecundability
    - `fecund` = total fecundity rate

```{r}
eg2.4 <- data.frame(
  pop = c(1970, 1980),
  prop_m = c(.58, .72),
  noncontr = c(.76, .97),
  abort = c(.84, .97),
  lact = c(.66, .56),
  fecund = c(16.573, 16.158)
)

dgnpop(eg2.4,
  pop = "pop",
  factors = c("prop_m", "noncontr", "abort", "lact", "fecund")
) |>
  dg_table()
```

**Example 4.1:** Female intrinsic growth rate per person as a function of two vector factors: United States, 1980 and 1985  

    - `Lx` = stationary female population
    - `Mx` = fertility rate

```{r}
eg4.1 <- data.frame(
  age_group = c("10-15", "15-20", "20-25", "25-30", "30-35", "35-40", "40-45", "45-50", "50-55"),
  pop = rep(c(1965, 1960), e = 9),
  Lx = c(486446, 485454, 483929, 482046, 479522, 475844, 470419, 462351, 450468,
         485434, 484410, 492905, 481001, 478485, 474911, 469528, 461368, 449349),
  mx = c(.00041, .03416, .09584, .07915, .04651, .02283, .00631, .00038, .00000,
         .00040, .04335, .12581, .09641, .05504, .02760, .00756, .00045, .00000)
)

# rate function:
RF4.1 <- function(A, B) {
  idx <- seq_len(length(A))
  mu0 <- sum(A * B / 100000)
  mu1 <- sum((5 * idx + 7.5) * A * B / 100000)
  r1 <- log(mu0) * (mu0 / mu1)
  while (TRUE) {
    Nr1 <- 0
    Dr1 <- 0
    Nr1 <- Nr1 + sum(exp(-r1 * (5 * idx + 7.5)) * A * (B / 100000))
    Dr1 <- Dr1 - sum((5 * idx + 7.5) * exp(-r1 * (5 * idx + 7.5)) * A * (B / 100000))
    r2 <- r1 - ((Nr1 - 1) / Dr1)
    if (abs(r2 - r1) <= .0000001) {
      break
    }
    r1 <- r2
  }
  return(r2)
}

# crude rates:
RF4.1(A = eg4.1$Lx[1:9], B = eg4.1$mx[1:9])
RF4.1(A = eg4.1$Lx[10:18], B = eg4.1$mx[10:18])

# decomposition:
dgnpop(eg4.1,
  pop = "pop", factors = c("Lx", "mx"),
  id_vars = "age_group",
  ratefunction = "RF4.1(Lx,mx)"
) |>
  dg_table()
```

<!-- # Supplementary - Re-expression of population structures -->

<!-- When working with cross-classified data, Das Gupta developed a method of specifying group proportions as set of symmetric proportions indicating the contribution of each structural variable.   -->

<!-- Returning to the example above, we can compute these manually for the case of 2 cross-classified variables as so:   -->

<!-- ```{r} -->
<!-- eg5.3a <- -->
<!--   eg5.3 |> -->
<!--   group_by(pop) |> -->
<!--   mutate(n_tot = sum(size)) |> -->
<!--   group_by(pop, age) |> -->
<!--   mutate(n_age = sum(size)) |> -->
<!--   group_by(pop, race) |> -->
<!--   mutate(n_race = sum(size)) |> -->
<!--   ungroup() |> -->
<!--   mutate( -->
<!--     A = ((size / n_race) * (n_age / n_tot))^(1 / 2), -->
<!--     B = ((size / n_age) * (n_race / n_tot))^(1 / 2), -->
<!--   ) -->
<!-- ``` -->

<!-- The product of variables `A` and `B` above will return the individual group proportions:  -->

<!-- ```{r} -->
<!-- eg5.3a |> -->
<!--   mutate( -->
<!--     AB = A * B, -->
<!--     prop = size / n_tot -->
<!--   ) |> -->
<!--   head() -->
<!-- ``` -->

<!-- Internally, `dgnpop()` will do this provided `id_vars` and `crossclassified` are specified as detailed above. However, should users wish, this intermediary step in the decomposition can be done using `dgcc()`:   -->

<!-- ```{r} -->
<!-- dgcc(eg5.3, -->
<!--   pop = "pop", id_vars = c("age", "race"), -->
<!--   crossclassified = "size" -->
<!-- ) |> -->
<!--   head() -->
<!-- ``` -->
